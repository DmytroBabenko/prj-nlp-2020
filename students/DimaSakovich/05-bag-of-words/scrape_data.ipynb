{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langdetect import detect, lang_detect_exception\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import stanfordnlp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu_tokenizer.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu_tagger.pt', 'pretrain_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu.pretrain.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu_lemmatizer.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu_parser.pt', 'pretrain_path': '/home/dima/stanfordnlp_resources/uk_iu_models/uk_iu.pretrain.pt', 'lang': 'uk', 'shorthand': 'uk_iu', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp_uk = stanfordnlp.Pipeline(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_format = 'https://xl-catalog-api.rozetka.com.ua/v2/goods/get?front-type=xl&category_id={cat_id}&page={page_num}&sort=rank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_url_format = 'https://product-api.rozetka.com.ua/v3/comments/get?front-type=xl&goods={product_id}&page={page_num}&sort=date&limit=10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_cat = 80037\n",
    "notebook_cat = 80004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(x):\n",
    "    try:\n",
    "        return detect(x)\n",
    "    except lang_detect_exception.LangDetectException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_data(cat_id, limit=10):\n",
    "\n",
    "    product_url = product_url_format.format(cat_id=cat_id, page_num=1)\n",
    "    resp = requests.get(product_url)\n",
    "    body = json.loads(resp.text)\n",
    "    pages_num_p = body['data']['total_pages']\n",
    "\n",
    "    lst = []\n",
    "    counter = 0\n",
    "    for i in range(1, pages_num_p + 1):\n",
    "        product_url = product_url_format.format(cat_id=cat_id, page_num=i)\n",
    "        time.sleep(0.5)\n",
    "        resp = requests.get(product_url)\n",
    "        body = json.loads(resp.text)\n",
    "        ids = body['data']['ids']\n",
    "\n",
    "        for id_ in ids:\n",
    "            comment_url = comment_url_format.format(product_id=id_, page_num=1)\n",
    "            time.sleep(0.5)\n",
    "            resp = requests.get(comment_url)\n",
    "            body = json.loads(resp.text)\n",
    "            pages_num_c = body['data']['pages']['count']\n",
    "\n",
    "            for j in range(1, pages_num_c + 1):\n",
    "                comment_url = comment_url_format.format(product_id=id_, page_num=j)\n",
    "                time.sleep(0.5)\n",
    "                resp = requests.get(comment_url)\n",
    "                body = json.loads(resp.text)\n",
    "                data = body['data']['comments']\n",
    "\n",
    "                for item in data:\n",
    "                    res = {'user': item['usertitle'], 'mark': item['mark'], 'text': item['text'], \n",
    "                           'pros': item['dignity'], 'cons': item['shortcomings']}\n",
    "                    lang = lang_detect(res['text'].strip())\n",
    "                    if lang == 'uk' and res not in lst:\n",
    "                        lst.append(res)\n",
    "                        counter += 1\n",
    "                    \n",
    "                    if counter >= limit:\n",
    "                        return pd.DataFrame(lst)\n",
    "#                     elif counter % 100 == 0:\n",
    "#                         print(counter)\n",
    "    return pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 ms, sys: 774 µs, total: 23 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    df_tv = pd.read_csv('reviews_tv.csv')\n",
    "except FileNotFoundError:\n",
    "    df_tv = get_data(tv_cat, limit=10000)\n",
    "    df_tv.to_csv('reviews_tv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 ms, sys: 855 µs, total: 23.4 ms\n",
      "Wall time: 22.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    df_laptop = pd.read_csv('reviews_laptop.csv')\n",
    "except FileNotFoundError:\n",
    "    df_laptop = get_data(tv_cat, limit=10000)\n",
    "    df_laptop.to_csv('reviews_laptop.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_tv, df_laptop])\n",
    "\n",
    "df = df.loc[(df.mark.notna()) & (df.mark > 0)]\n",
    "df.fillna(\" \", inplace=True)\n",
    "df['target'] = np.where(df.mark < 3, 'neg', np.where(df.mark == 3, 'neu', 'pos'))\n",
    "df['text'] = df.apply(lambda row: \" \".join([row.text, row.pros, row.cons]).strip(), 1)\n",
    "df['text'] = df.text.map(lambda x: BeautifulSoup(x).get_text())\n",
    "df.drop(['user', 'mark', 'pros', 'cons'], 1, inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4414, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>В цьому телевізорі все чудово і різнокольорово...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Відмінний телевізор. Купив саме те, що хотів. ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Замовив і був задоволений. Простий телевізор, ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Чудовий телевізор, все працює. Чудова ціна. Бе...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>Чудовий телевізор</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text target\n",
       "0      2  В цьому телевізорі все чудово і різнокольорово...    pos\n",
       "1      7  Відмінний телевізор. Купив саме те, що хотів. ...    pos\n",
       "2      8  Замовив і був задоволений. Простий телевізор, ...    pos\n",
       "3      9  Чудовий телевізор, все працює. Чудова ціна. Бе...    pos\n",
       "4     12                                  Чудовий телевізор    pos"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
    "df.target = df.target.map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3708\n",
       "0     417\n",
       "1     289\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    filter_pos = ('PUNCT', 'ADP', 'SYM', 'CCONJ', 'SCONJ', 'PROPN')\n",
    "    filter_words = [\"і\", \"та\", \"або\", \"й\", \"то\", \"б\", \"але\"]\n",
    "    sentences = nlp_uk(x).sentences\n",
    "    res = []\n",
    "    for sent in sentences:\n",
    "        if '?' in list(sent.words)[-1].text:\n",
    "            continue\n",
    "        res.append([token.lemma for token in sent.words if token.upos not in filter_pos])\n",
    "    return \" \".join(list(filter(lambda x: x not in filter_words, sum(res, [])))).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df = df.loc[~df.text.isna()]\n",
    "except FileNotFoundError:\n",
    "    lst = []\n",
    "\n",
    "    for item in tqdm(df.text.values):\n",
    "        lst.append(tokenize(item))\n",
    "\n",
    "    df['text'] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0,\n",
    "                                                    stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(min_df=5, max_df=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.fit(X_train)\n",
    "\n",
    "X_train_vec = tf_idf.transform(X_train)\n",
    "X_test_vec = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(clf):\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    print(\"f1 macro:\", f1_score(y_test, y_pred, average='macro'))\n",
    "    print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_interval = [0.01, 0.1, 0.2, 0.5, 1, 2, 5, 10, 100]\n",
    "\n",
    "# for i in reg_interval:\n",
    "#     train_eval(LinearSVC(C=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=1)\n",
    "\n",
    "model.fit(X_train_vec, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88        82\n",
      "           1       0.95      0.68      0.79        56\n",
      "           2       0.97      0.99      0.98       715\n",
      "\n",
      "    accuracy                           0.96       853\n",
      "   macro avg       0.94      0.85      0.88       853\n",
      "weighted avg       0.96      0.96      0.96       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = FeatureUnion([\n",
    "    ('TfIdf_Unigram', TfidfVectorizer(min_df=5, max_df=0.75, ngram_range=(1, 1), strip_accents='unicode')),\n",
    "    ('TfIdf_Bigram',  TfidfVectorizer(min_df=2, max_df=0.75, ngram_range=(2, 2), strip_accents='unicode'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (1 / y_train.value_counts(normalize=True)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:   29.6s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:   30.4s remaining:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:   32.5s remaining:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   38.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LinearSVC__C': 1} 0.8597509332905282\n"
     ]
    }
   ],
   "source": [
    "N_COMP = 400\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"main_union\", FeatureUnion([\n",
    "        (\"pipe1\", Pipeline([\n",
    "            ('tf_idf', tf_idf),\n",
    "        ])),\n",
    "        (\"pipe2\", Pipeline([\n",
    "            ('tf_idf', tf_idf),\n",
    "            (\"SVD\", TruncatedSVD(n_components=N_COMP))\n",
    "        ])),\n",
    "    ])),\n",
    "    ('LinearSVC', LinearSVC(class_weight=class_weights))\n",
    "#     (\"LogReg\", LogisticRegression(max_iter=300))\n",
    "])\n",
    "\n",
    "distributions = {\n",
    "    \"LinearSVC__C\": [0.5, 1, 5],\n",
    "#     \"LogReg__C\": [0.5, 1, 5, 10],\n",
    "#     \"LogReg__penalty\": [\"l2\"],\n",
    "}\n",
    "clf = RandomizedSearchCV(pipeline,\n",
    "                         distributions,\n",
    "                         random_state=0,\n",
    "                         scoring=make_scorer(f1_macro),\n",
    "                         n_iter=10,\n",
    "                         cv=5,\n",
    "                         verbose=5,\n",
    "                         n_jobs=-1)\n",
    "search = clf.fit(X_train, y_train)\n",
    "print(search.best_params_, search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90        82\n",
      "           1       1.00      0.86      0.92        56\n",
      "           2       0.98      0.99      0.99       715\n",
      "\n",
      "    accuracy                           0.98       853\n",
      "   macro avg       0.96      0.91      0.94       853\n",
      "weighted avg       0.98      0.98      0.98       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = search.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### out-of-fold LGB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import plot_importance, LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#     'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'num_rounds': 1000,\n",
    "    'max_depth': -1, #  8\n",
    "    'learning_rate': 0.01,  #  0.007\n",
    "    'num_leaves': 31, # was 127\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 300,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l2': 0.7,\n",
    "    'feature_fraction': 0.2, #  0.8\n",
    "    'metric': 'custom',\n",
    "}\n",
    "\n",
    "# class_weigths = (\n",
    "#     np.log1p(1/y_train.value_counts(normalize=True)\n",
    "#     )\n",
    "# ).to_dict()\n",
    "\n",
    "classifier = LGBMClassifier(**params)\n",
    "\n",
    "# train_ind = df_features.train_dev == 'train'\n",
    "# val_ind = df_features.train_dev == 'dev'\n",
    "# X_tr = df_features[train_ind].drop(columns=non_features)\n",
    "# X_val = df_features[val_ind].drop(columns=non_features)\n",
    "# y_tr = df_features.loc[train_ind, TARGET].astype(np.int32)\n",
    "# y_val = df_features.loc[val_ind, TARGET].astype(np.int32)\n",
    "\n",
    "# print(X_tr.shape, y_tr.shape)\n",
    "# print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_fscore(y_true, y_pred):\n",
    "    y_pred = y_pred.reshape(len(np.unique(y_true)), -1)\n",
    "    y_pred = y_pred.argmax(axis=0)\n",
    "    res = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'macro_f1', res, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweak it to see results\n",
    "use_sample_weight = True\n",
    "# perform validation strategy\n",
    "N_FOLDS = 4\n",
    "strategy = StratifiedKFold(n_splits=N_FOLDS, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[X_train.index]\n",
    "\n",
    "sample_weight = y_train.map(class_weights).values\n",
    "\n",
    "test = df.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oof = np.zeros(len(train), dtype=np.float32)\n",
    "pred_test = np.zeros(\n",
    "    (len(test), params['num_class'], N_FOLDS), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics = np.zeros(N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "\tTrain len: 2558\n",
      "\tVal len: 853\n",
      "\tFITTING MODEL...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's macro_f1: 0.830107\n",
      "[200]\tvalid_0's macro_f1: 0.840658\n",
      "[300]\tvalid_0's macro_f1: 0.843016\n",
      "[400]\tvalid_0's macro_f1: 0.842428\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid_0's macro_f1: 0.850816\n",
      "\tPREDICT OOF...\n",
      "\tPREDICTING TEST...\n",
      "\tFold score: 0.8508164885439063\n",
      "Fold: 2\n",
      "\tTrain len: 2558\n",
      "\tVal len: 853\n",
      "\tFITTING MODEL...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's macro_f1: 0.796662\n",
      "[200]\tvalid_0's macro_f1: 0.80872\n",
      "[300]\tvalid_0's macro_f1: 0.811773\n",
      "[400]\tvalid_0's macro_f1: 0.829004\n",
      "[500]\tvalid_0's macro_f1: 0.840366\n",
      "[600]\tvalid_0's macro_f1: 0.8347\n",
      "[700]\tvalid_0's macro_f1: 0.828892\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's macro_f1: 0.840366\n",
      "\tPREDICT OOF...\n",
      "\tPREDICTING TEST...\n",
      "\tFold score: 0.8403658228487697\n",
      "Fold: 3\n",
      "\tTrain len: 2558\n",
      "\tVal len: 853\n",
      "\tFITTING MODEL...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's macro_f1: 0.845719\n",
      "[200]\tvalid_0's macro_f1: 0.847298\n",
      "[300]\tvalid_0's macro_f1: 0.85038\n",
      "[400]\tvalid_0's macro_f1: 0.840705\n",
      "[500]\tvalid_0's macro_f1: 0.847849\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid_0's macro_f1: 0.855121\n",
      "\tPREDICT OOF...\n",
      "\tPREDICTING TEST...\n",
      "\tFold score: 0.8551205130797452\n",
      "Fold: 4\n",
      "\tTrain len: 2559\n",
      "\tVal len: 852\n",
      "\tFITTING MODEL...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's macro_f1: 0.820968\n",
      "[200]\tvalid_0's macro_f1: 0.844693\n",
      "[300]\tvalid_0's macro_f1: 0.84877\n",
      "[400]\tvalid_0's macro_f1: 0.868896\n",
      "[500]\tvalid_0's macro_f1: 0.857127\n",
      "[600]\tvalid_0's macro_f1: 0.862251\n",
      "[700]\tvalid_0's macro_f1: 0.853623\n",
      "Early stopping, best iteration is:\n",
      "[453]\tvalid_0's macro_f1: 0.871575\n",
      "\tPREDICT OOF...\n",
      "\tPREDICTING TEST...\n",
      "\tFold score: 0.8715748111698414\n"
     ]
    }
   ],
   "source": [
    "for i, (tr_ind, val_ind) in enumerate(strategy.split(X=np.ones(len(train)), y=train['target'])):\n",
    "    print(\n",
    "        f'Fold: {i + 1}\\n\\tTrain len: {len(tr_ind)}\\n\\tVal len: {len(val_ind)}')\n",
    "    # split tr/val\n",
    "    pipe = Pipeline([\n",
    "            ('TFIDF', TFIDF),\n",
    "            (\"SVD\", TruncatedSVD(n_components=N_COMP))\n",
    "        ])\n",
    "    pipe.fit(train.iloc[tr_ind]['text'])\n",
    "    \n",
    "    X = pipe.transform(train.iloc[tr_ind]['text'].copy())\n",
    "    y = train.iloc[tr_ind]['target'].copy()\n",
    "    X_val = pipe.transform(train.iloc[val_ind]['text'].copy())\n",
    "    y_val = train.iloc[val_ind]['target'].copy()\n",
    "    X_test = pipe.transform(test['text'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fit model\n",
    "    print('\\tFITTING MODEL...')\n",
    "    classifier.fit(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "#        early_stopping_rounds=params['early_stopping_rounds'],\n",
    "        verbose=params['verbose'],\n",
    "        eval_metric=lgb_fscore,\n",
    "        sample_weight=sample_weight[tr_ind] if use_sample_weight else None,\n",
    "    )\n",
    "    # predict OOF val\n",
    "    print('\\tPREDICT OOF...')\n",
    "    pred_oof[val_ind] = classifier.predict(X_val, num_threads=num_threads)\n",
    "    # predict test\n",
    "    print('\\tPREDICTING TEST...')\n",
    "    pred_test[..., i] = classifier.predict_proba(\n",
    "        X_test, num_threads=num_threads)\n",
    "    fold_metrics[i] = f1_macro(y_val, pred_oof[val_ind])\n",
    "    print(f'\\tFold score: {fold_metrics[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score:  0.854832206399943\n"
     ]
    }
   ],
   "source": [
    "print(f'Total score: ', f1_macro(train['target'], pred_oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91        82\n",
      "           1       1.00      0.79      0.88        56\n",
      "           2       0.97      0.99      0.98       715\n",
      "\n",
      "    accuracy                           0.97       853\n",
      "   macro avg       0.97      0.89      0.92       853\n",
      "weighted avg       0.97      0.97      0.97       853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_raw = pred_test.mean(axis=-1)\n",
    "y_pred = y_pred_raw.argmax(axis=1).astype(np.int32)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, in practice oof works better and avoids overfitting. But, I didn't use X_test, just to make it possible easy compare results with previous models.\n",
    "\n",
    "What can also be done:\n",
    "* add words tonal features from tone-dict-uk.tsv. and concat as sparse matrix\n",
    "* replace words with tones by tokens like `<positive>` or `<negative>`\n",
    "* classical features like `word_num`, `text_len`, `has_pros`, `has_cons`, `mean_words_tone` etc. But I'm not sure if it possilbe due to task limitations, as it should be only BoW\n",
    "* get BoW features w/o pros and cons concatenation\n",
    "\n",
    "Currently, pipeline with un and bi-grams extended by SVD features shows the best result, not suffering recall for neu class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p36] *",
   "language": "python",
   "name": "conda-env-p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
