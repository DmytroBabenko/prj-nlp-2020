{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "TASK_PATH = os.path.join(REPO_PATH, \"tasks\", \"08-syntactic-parsing.md\")\n",
    "DATA_PATH = '/home/dima/Projects/UD_Ukrainian-IU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_markdown(path):\n",
    "    with open(path, 'r') as fh:\n",
    "        content = fh.read()\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Синтаксичний аналіз\n",
       "\n",
       "## I. Покращення парсера залежностей\n",
       "\n",
       "Візьміть за основу парсер залежностей, побудований на практичному занятті, і зробіть мінімум дві ітерації для покращення якості.\n",
       "\n",
       "Варіанти покращення парсера:\n",
       "* підберіть кращий набір ознак;\n",
       "* зробіть класифікацію типів залежностей та поміряйте LAS (labelled attachment score);\n",
       "* додайте операцію swap для опрацювання непроективних дерев;\n",
       "* покращіть статичний оракул або замініть його недетермінованим чи динамічним оракулом;\n",
       "* спробуйте інший класифікатор та зробіть оптимізацію гіперпараметрів;\n",
       "* ваші ідеї.\n",
       "\n",
       "За основу можна використати або свій парсер, або [приклад із заняття](../lectures/08-dep-parser-uk.ipynb).\n",
       "\n",
       "Корисні посилання:\n",
       "* [UD-корпус для української](https://github.com/UniversalDependencies/UD_Ukrainian-IU/)\n",
       "* [Зручна бібліотека для роботи з форматом CoNLL](https://github.com/EmilStenstrom/conllu)\n",
       "* Стаття з блогу Matthew Honnibal - [Parsing English in 500 Lines of Python](https://explosion.ai/blog/parsing-english-in-python)\n",
       "* Книга про парсери залежностей - [Dependency Parsing by Kübler, McDonald, and Nivre](https://books.google.com.ua/books?id=k3iiup7HB9UC&pg=PA21&hl=uk&source=gbs_toc_r&cad=4#v=onepage&q&f=false)\n",
       "* Гарний огляд типів парсера залежностей та оракулів - [Improvements in Transition Based Systems for Dependency Parsing](http://paduaresearch.cab.unipd.it/8004/1/Tesi.pdf)\n",
       "\n",
       "## II. Використання парсера на нових даних\n",
       "\n",
       "Виберіть кілька випадкових речень українською мовою на побудуйте дерева залежностей для них, використовуючи свій парсер.\n",
       "\n",
       "Для токенізації можна використати https://github.com/lang-uk/tokenize-uk.\n",
       "\n",
       "Для частиномовного аналізу можна використати https://github.com/kmike/pymorphy2. Зважте, що частиномовні теги в UD та в pymorphy2 відрізняються, зокрема pymorphy2 не розрізняє типи сполучників. Нижче подано спосіб вирівняти ці дві нотації:\n",
       "\n",
       "```python\n",
       "import pymorphy2\n",
       "\n",
       "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
       "\n",
       "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
       "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
       "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
       "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
       "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
       "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
       "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
       "\n",
       "PREP = [\"до\", \"на\"]\n",
       "\n",
       "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
       "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
       "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
       "\n",
       "def normalize_pos(word):\n",
       "    if word.tag.POS == \"CONJ\":\n",
       "        if \"coord\" in word.tag:\n",
       "            return \"CCONJ\"\n",
       "        else:\n",
       "            return \"SCONJ\"\n",
       "    elif \"PNCT\" in word.tag:\n",
       "        return \"PUNCT\"\n",
       "    elif word.normal_form in PREP:\n",
       "        return \"PREP\"\n",
       "    elif word.normal_form in DET:\n",
       "        return \"DET\"\n",
       "    else:\n",
       "        return mapping.get(word.tag.POS, word.tag.POS)\n",
       "```\n",
       "\n",
       "Запишіть ваші спостереження та результати в окремий файл.\n",
       "\n",
       "### Оцінювання\n",
       "\n",
       "80% - I. Покращення парсера залежностей  \n",
       "20% - II. Використання парсера на нових даних\n",
       "\n",
       "### Крайній термін\n",
       "\n",
       "02.05.2020\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_markdown(TASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 s, sys: 82.5 ms, total: 2.94 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"uk_iu-ud-train.conllu\"), \"r\") as f:\n",
    "    train_trees = parse(f.read())\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"uk_iu-ud-dev.conllu\"), \"r\") as f:\n",
    "    test_trees = parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5496 672\n"
     ]
    }
   ],
   "source": [
    "print(len(train_trees), len(test_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree):\n",
    "    for node in tree:\n",
    "        head = node[\"head\"]\n",
    "        print(\"{} <-- {}\".format(node[\"form\"],\n",
    "                             tree[head - 1][\"form\"]\n",
    "                             if head > 0 else \"root\"))\n",
    "\n",
    "def check_tree(tree):\n",
    "    for n in tree:\n",
    "        if not isinstance(n[\"id\"], int):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У <-- домі\n",
      "домі <-- була\n",
      "римського <-- патриція\n",
      "патриція <-- домі\n",
      "Руфіна <-- патриція\n",
      "була <-- root\n",
      "прегарна <-- фреска\n",
      "фреска <-- була\n",
      ", <-- зображення\n",
      "зображення <-- фреска\n",
      "Венери <-- зображення\n",
      "та <-- Адоніса\n",
      "Адоніса <-- Венери\n",
      ". <-- була\n"
     ]
    }
   ],
   "source": [
    "tree = train_trees[0]\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad trees: \n",
      "Train: 197\n",
      "Test: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Bad trees: \" )\n",
    "print(\"Train:\", len(list(filter(check_tree, train_trees))))\n",
    "print(\"Test:\", len(list(filter(check_tree, test_trees))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5299 656\n"
     ]
    }
   ],
   "source": [
    "clean_train_trees = list(filter(lambda t: not check_tree(t), train_trees))\n",
    "clean_test_trees = list(filter(lambda t: not check_tree(t), test_trees))\n",
    "\n",
    "print(len(clean_train_trees), len(clean_test_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersects(n1, n2):\n",
    "    s1 = n1['id'] if n1['head'] > n1['id'] else n1['head']\n",
    "    e1 = n1['head'] if n1['head'] > n1['id'] else n1['id']\n",
    "    s2 = n2['id'] if n2['head'] > n2['id'] else n2['head']\n",
    "    e2 = n2['head'] if n2['head'] > n2['id'] else n2['id']\n",
    "    \n",
    "    return (s1 < s2 and e1 > s2 and e2 > e1) or (s2 < s1 and e2 > s1 and e1 > e2)\n",
    "\n",
    "def non_projective(tree):\n",
    "    for n1 in tree:\n",
    "        for n2 in tree:\n",
    "            if n1['id'] < n2['id'] and intersects(n1, n2):\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_projective_trees = list(filter(non_projective, clean_train_trees))\n",
    "\n",
    "len(non_projective_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4885"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projective_trees = list(filter(lambda t: not non_projective(t), clean_train_trees))\n",
    "\n",
    "len(projective_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design actions and the oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(str, Enum):\n",
    "    SHIFT = \"shift\"\n",
    "    REDUCE = \"reduce\"\n",
    "    RIGHT = \"right\"\n",
    "    LEFT = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle(stack, top_queue, relations):\n",
    "    \"\"\"\n",
    "    Make a decision on the right action to do.\n",
    "    \"\"\"\n",
    "    top_stack = stack[-1]\n",
    "    # check if both stack and queue are non-empty\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    # check if there are any clear dependencies\n",
    "    elif top_queue[\"head\"] == top_stack[\"id\"]:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack[\"head\"] == top_queue[\"id\"]:\n",
    "        return Actions.LEFT\n",
    "    # check if we can reduce the top of the stack\n",
    "    elif top_stack[\"id\"] in [i[0] for i in relations] and \\\n",
    "         (top_queue[\"head\"] < top_stack[\"id\"] or \\\n",
    "          [s for s in stack if s[\"head\"] == top_queue[\"id\"]]):\n",
    "        return Actions.REDUCE\n",
    "    # default option\n",
    "    else:\n",
    "        return Actions.SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "def trace_actions(tree, log=True):\n",
    "    \"\"\"\n",
    "    Try out the oracle to verify it's returning the right actions.\n",
    "    \"\"\"\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        queue[0] if len(queue) > 0 else None,\n",
    "                        relations)\n",
    "        if log:\n",
    "            print(\"Stack:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in stack])\n",
    "            print(\"Queue:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in queue])\n",
    "            print(\"Relations:\", relations)\n",
    "            print(action)\n",
    "            print(\"========================\")\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    if log:\n",
    "        print(\"Gold relations:\")\n",
    "        print([(node[\"id\"], node[\"head\"]) for node in tree])\n",
    "        print(\"Retrieved relations:\")\n",
    "        print(sorted(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace_actions(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(stack, queue):\n",
    "#     features = dict()\n",
    "#     if len(stack) > 0:\n",
    "#         stack_top = stack[-1]\n",
    "#         features[\"s0-word\"] = stack_top[\"form\"]\n",
    "#         features[\"s0-lemma\"] = stack_top[\"lemma\"]\n",
    "#         features[\"s0-tag\"] = stack_top[\"upostag\"]\n",
    "#         if stack_top[\"feats\"]:\n",
    "#             for k, v in stack_top[\"feats\"].items():\n",
    "#                 features[\"s0-\" + k] = v\n",
    "#     if len(stack) > 1:\n",
    "#         features[\"s1-tag\"] = stack_top[\"upostag\"]\n",
    "#     if queue:\n",
    "#         queue_top = queue[0]\n",
    "#         features[\"q0-word\"] = queue_top[\"form\"]\n",
    "#         features[\"q0-lemma\"] = queue_top[\"lemma\"]\n",
    "#         features[\"q0-tag\"] = queue_top[\"upostag\"]\n",
    "#         if queue_top[\"feats\"]:\n",
    "#             for k, v in queue_top[\"feats\"].items():\n",
    "#                 features[\"q0-\" + k] = v\n",
    "#     if len(queue) > 1:\n",
    "#         queue_next = queue[1]\n",
    "#         features[\"q1-word\"] = queue_next[\"form\"]\n",
    "#         features[\"q1-tag\"] = queue_next[\"upostag\"]\n",
    "#     if len(queue) > 2:\n",
    "#         features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "#     if len(queue) > 3:\n",
    "#         features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "#     if stack and queue:\n",
    "#         features[\"distance\"] = queue[0][\"id\"] - stack[-1][\"id\"]\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(stack, queue, relations=None):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    if len(stack) > 1:        \n",
    "        features[\"s0-word\"] = stack[-2][\"form\"]\n",
    "        features[\"s0-lemma\"] = stack[-2][\"lemma\"]\n",
    "        features[\"s0-tag\"] = stack[-2][\"upostag\"]\n",
    "#         features[\"s0-rchildren-num\"] = len([r for r in relations if r[1] == stack[-2]['id']])\n",
    "#         features[\"s0-lchildren-num\"] = len([r for r in relations if r[0] == stack[-2]['id']])\n",
    "        if stack[-2][\"feats\"]:\n",
    "            for k, v in stack[-2][\"feats\"].items():\n",
    "                features[\"s0-\" + k] = v\n",
    "    \n",
    "    if len(stack) > 2:\n",
    "        features[\"s1-word\"] = stack[-3][\"form\"]\n",
    "        features[\"s1-tag\"] = stack[-3][\"upostag\"]\n",
    "    \n",
    "    if len(stack) > 3:\n",
    "        features[\"s2-tag\"] = stack[-4][\"upostag\"]\n",
    "        \n",
    "    if len(stack) > 4:\n",
    "        features[\"s3-tag\"] = stack[-5][\"upostag\"]\n",
    "    \n",
    "    if len(stack) > 1:\n",
    "        queue_top = stack[-1]\n",
    "        features[\"q0-word\"] = stack[-1][\"form\"]\n",
    "        features[\"q0-lemma\"] = stack[-1][\"lemma\"]\n",
    "        features[\"q0-tag\"] = stack[-1][\"upostag\"]\n",
    "#         features[\"q0-rchildren-num\"] = len([r for r in relations if r[1] == stack[-1]['id']])\n",
    "#         features[\"q0-lchildren-num\"] = len([r for r in relations if r[0] == stack[-1]['id']])\n",
    "        if stack[-1][\"feats\"]:\n",
    "            for k, v in stack[-1][\"feats\"].items():\n",
    "                features[\"q0-\" + k] = v\n",
    "    \n",
    "    if len(queue) > 0:        \n",
    "        features[\"q1-word\"] = queue[0][\"form\"]\n",
    "        features[\"q1-tag\"] = queue[0][\"upostag\"]\n",
    "    \n",
    "    if len(queue) > 1:\n",
    "        features[\"q2-tag\"] = queue[1][\"upostag\"]\n",
    "    \n",
    "    if len(queue) > 2:\n",
    "        features[\"q3-tag\"] = queue[2][\"upostag\"]\n",
    "       \n",
    "    if len(stack) > 1:\n",
    "        features[\"distance\"] = stack[-1][\"id\"] - stack[-2][\"id\"]\n",
    "    \n",
    "    features['q-empty'] = not bool(queue)    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tree):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        queue[0] if len(queue) > 0 else None,\n",
    "                        relations)\n",
    "        features.append(extract_features(stack, queue))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 14\n",
      "Number of actions: 29\n",
      "List of actions taken: ['shift', 'left', 'shift', 'shift', 'left', 'right', 'right', 'reduce', 'reduce', 'left', 'right', 'shift', 'left', 'right', 'shift', 'left', 'right', 'right', 'shift', 'left', 'right', 'reduce', 'reduce', 'reduce', 'reduce', 'right', 'reduce', 'reduce', 'reduce']\n"
     ]
    }
   ],
   "source": [
    "features, labels = get_data(tree)\n",
    "print(\"Number of words:\", len(tree))\n",
    "print(\"Number of actions:\", len(labels))\n",
    "print(\"List of actions taken:\", labels)\n",
    "# print(\"Features:\")\n",
    "# for word in features:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  190298 190298\n",
      "test:  25820 25820\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = [], []\n",
    "for tree in train_trees:\n",
    "    tree_features, tree_labels = get_data([t for t in tree if type(t[\"id\"])==int])\n",
    "    train_features += tree_features\n",
    "    train_labels += tree_labels\n",
    "    \n",
    "test_features, test_labels = [], []\n",
    "for tree in test_trees:\n",
    "    tree_features, tree_labels = get_data([t for t in tree if type(t[\"id\"])==int])\n",
    "    test_features += tree_features\n",
    "    test_labels += tree_labels\n",
    "\n",
    "    \n",
    "print(\"train: \", len(train_features), len(train_labels))\n",
    "print(\"test: \", len(test_features), len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0\n",
    "N_COMP = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "truncated_svd = TruncatedSVD(n_components=N_COMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features:  115611\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.fit(train_features)\n",
    "\n",
    "train_features_vectorized = vec.transform(train_features)\n",
    "test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "print(\"\\nTotal number of features: \", len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 54s, sys: 1min 13s, total: 6min 7s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t_svd = truncated_svd.fit(train_features_vectorized)\n",
    "\n",
    "train_features_reduced = t_svd.transform(train_features_vectorized)\n",
    "test_features_reduced = t_svd.transform(test_features_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "max_iter reached after 151 seconds\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/p36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='sag',\n",
       "          tol=0.0001, verbose=100, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = LogisticRegression(random_state=RANDOM_STATE,\n",
    "                         C=1,\n",
    "                         solver=\"sag\", \n",
    "                         multi_class=\"multinomial\", # ovr\n",
    "                         max_iter=1000, \n",
    "                         verbose=100)\n",
    "lrc.fit(train_features_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.88      0.90      0.89      6371\n",
      "      reduce       0.86      0.81      0.83      6875\n",
      "       right       0.77      0.80      0.79      5996\n",
      "       shift       0.85      0.87      0.86      6578\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     25820\n",
      "   macro avg       0.84      0.84      0.84     25820\n",
      "weighted avg       0.84      0.84      0.84     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lrc.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(C=1, \n",
    "# #           kernel='rbf',\n",
    "#           gamma='auto',\n",
    "#           verbose=10, \n",
    "#           max_iter=100, \n",
    "# #           decision_function_shape='ovr',\n",
    "#           random_state=RANDOM_STATE)\n",
    "# # svc.fit(train_features_reduced, train_labels)\n",
    "# svc.fit(train_features_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = svc.predict(test_features_vectorized)\n",
    "# print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work at all. Had not too much time to play with hypoparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_fscore(y_true, y_pred):\n",
    "    y_pred = y_pred.reshape(len(np.unique(y_true)), -1)\n",
    "    y_pred = y_pred.argmax(axis=0)\n",
    "    res = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'macro_f1', res, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_class': 4,\n",
    "    'num_rounds': 5000,\n",
    "    'max_depth': -1, #  8\n",
    "    'learning_rate': 0.01,  #  0.007\n",
    "    'num_leaves': 31, # was 127\n",
    "    'verbose': 100,\n",
    "    'early_stopping_rounds': 300,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'lambda_l2': 0.7,\n",
    "    'feature_fraction': 0.2, #  0.8\n",
    "    'metric': 'custom',\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "\n",
    "lgb_clf = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/p36/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/dima/anaconda3/envs/p36/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/dima/anaconda3/envs/p36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's macro_f1: 0.746799\n",
      "[200]\tvalid_0's macro_f1: 0.775073\n",
      "[300]\tvalid_0's macro_f1: 0.793324\n",
      "[400]\tvalid_0's macro_f1: 0.802359\n",
      "[500]\tvalid_0's macro_f1: 0.811071\n",
      "[600]\tvalid_0's macro_f1: 0.818107\n",
      "[700]\tvalid_0's macro_f1: 0.821889\n",
      "[800]\tvalid_0's macro_f1: 0.826738\n",
      "[900]\tvalid_0's macro_f1: 0.831232\n",
      "[1000]\tvalid_0's macro_f1: 0.834389\n",
      "[1100]\tvalid_0's macro_f1: 0.837445\n",
      "[1200]\tvalid_0's macro_f1: 0.839946\n",
      "[1300]\tvalid_0's macro_f1: 0.841971\n",
      "[1400]\tvalid_0's macro_f1: 0.844382\n",
      "[1500]\tvalid_0's macro_f1: 0.847029\n",
      "[1600]\tvalid_0's macro_f1: 0.849394\n",
      "[1700]\tvalid_0's macro_f1: 0.851064\n",
      "[1800]\tvalid_0's macro_f1: 0.852692\n",
      "[1900]\tvalid_0's macro_f1: 0.854158\n",
      "[2000]\tvalid_0's macro_f1: 0.854906\n",
      "[2100]\tvalid_0's macro_f1: 0.856036\n",
      "[2200]\tvalid_0's macro_f1: 0.857217\n",
      "[2300]\tvalid_0's macro_f1: 0.858188\n",
      "[2400]\tvalid_0's macro_f1: 0.859826\n",
      "[2500]\tvalid_0's macro_f1: 0.860991\n",
      "[2600]\tvalid_0's macro_f1: 0.8615\n",
      "[2700]\tvalid_0's macro_f1: 0.862321\n",
      "[2800]\tvalid_0's macro_f1: 0.863064\n",
      "[2900]\tvalid_0's macro_f1: 0.863563\n",
      "[3000]\tvalid_0's macro_f1: 0.864217\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2998]\tvalid_0's macro_f1: 0.864254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        early_stopping_rounds=300, feature_fraction=0.2,\n",
       "        importance_type='split', lambda_l2=0.7, learning_rate=0.01,\n",
       "        max_depth=-1, metric='custom', min_child_samples=20,\n",
       "        min_child_weight=0.001, min_data_in_leaf=20, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_class=4, num_leaves=31,\n",
       "        num_rounds=3000, objective=None, random_state=0, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0, verbose=100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf.fit(\n",
    "    X=train_features_reduced,\n",
    "    y=train_labels,\n",
    "    eval_set=[(test_features_reduced, test_labels)],\n",
    "    verbose=params['verbose'],\n",
    "    eval_metric=lgb_fscore,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.89      0.93      0.91      6371\n",
      "      reduce       0.87      0.82      0.85      6875\n",
      "       right       0.81      0.82      0.81      5996\n",
      "       shift       0.88      0.89      0.88      6578\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     25820\n",
      "   macro avg       0.86      0.87      0.86     25820\n",
      "weighted avg       0.87      0.87      0.86     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lgb_clf.predict(test_features_reduced)\n",
    "\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the unlabeled attachment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parse(sentence, oracle, vectorizer, t_svd=None, log=False):\n",
    "    stack, queue, relations = [ROOT], sentence[:], []\n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = extract_features(stack, queue)\n",
    "            features = vectorizer.transform([features])\n",
    "            if t_svd:\n",
    "                features = t_svd.transform(features)\n",
    "            action = oracle.predict(features)[0]\n",
    "            if log:\n",
    "                print(\"Stack:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in stack])\n",
    "                print(\"Queue:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in queue])\n",
    "                print(\"Relations:\", relations)\n",
    "                print(action)\n",
    "                print(\"========================\")\n",
    "            # actual parsing\n",
    "            if action == Actions.SHIFT:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == Actions.REDUCE:\n",
    "                stack.pop()\n",
    "            elif action == Actions.LEFT:\n",
    "                relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "                stack.pop()\n",
    "            elif action == Actions.RIGHT:\n",
    "                relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "                stack.append(queue.pop(0))\n",
    "            else:\n",
    "                print(\"Unknown action.\")\n",
    "    return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 134\n",
      "Correctly defined: 93\n",
      "UAS: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total, tp = 0, 0\n",
    "for tree in tqdm(test_trees[:10]):\n",
    "    tree = [t for t in tree if type(t[\"id\"])==int]\n",
    "    golden = [(node[\"id\"], node[\"head\"]) for node in tree]\n",
    "    predicted = dep_parse(tree, lgb_clf, vec, t_svd)\n",
    "    total += len(tree)\n",
    "    tp += len(set(golden).intersection(set(predicted)))\n",
    "\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp/total, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
    "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
    "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
    "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
    "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
    "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
    "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
    "\n",
    "PREP = [\"до\", \"на\"]\n",
    "\n",
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if \"coord\" in word.tag:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    elif \"PNCT\" in word.tag:\n",
    "        return \"PUNCT\"\n",
    "    elif word.normal_form in PREP:\n",
    "        return \"PREP\"\n",
    "    elif word.normal_form in DET:\n",
    "        return \"DET\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_uk import tokenize_uk\n",
    "\n",
    "def convert_string_to_tree_format(text):\n",
    "    \"\"\"\n",
    "    перетрорення речення в формат корпусу\n",
    "    \"\"\"\n",
    "#     tokens= [t['form'] for t in trees_test[0]]\n",
    "\n",
    "    tokens = tokenize_uk.tokenize_words(text)\n",
    "    i = 0\n",
    "    tree = []\n",
    "    for token in tokens:\n",
    "        i = i + 1\n",
    "        # pos = str(morph.parse(token)[0].tag.POS)\n",
    "        word = morph.parse(token)[0]\n",
    "        token_ = OrderedDict([('id', i), \n",
    "                              ('form', str(token)), \n",
    "                              ('lemma', str(morph.parse(token)[0].normal_form)),\n",
    "                              ('upostag', normalize_pos(word)), \n",
    "                              ('xpostag', None), ('feats', None), \n",
    "                              ('head', None), ('deprel', None),\n",
    "                              ('deps', None), ('misc', None)])\n",
    "        tree.append(token_)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Це є звичайне речення, на якому ми тестуємо нашу гіпотезу.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('id', 1),\n",
       "              ('form', 'Це'),\n",
       "              ('lemma', 'це'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 2),\n",
       "              ('form', 'є'),\n",
       "              ('lemma', 'бути'),\n",
       "              ('upostag', 'VERB'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 3),\n",
       "              ('form', 'звичайне'),\n",
       "              ('lemma', 'звичайний'),\n",
       "              ('upostag', 'ADJ'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 4),\n",
       "              ('form', 'речення'),\n",
       "              ('lemma', 'речення'),\n",
       "              ('upostag', 'NOUN'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 5),\n",
       "              ('form', ','),\n",
       "              ('lemma', ','),\n",
       "              ('upostag', 'PUNCT'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 6),\n",
       "              ('form', 'на'),\n",
       "              ('lemma', 'на'),\n",
       "              ('upostag', 'PREP'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 7),\n",
       "              ('form', 'якому'),\n",
       "              ('lemma', 'який'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 8),\n",
       "              ('form', 'ми'),\n",
       "              ('lemma', 'ми'),\n",
       "              ('upostag', 'PRON'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 9),\n",
       "              ('form', 'тестуємо'),\n",
       "              ('lemma', 'тестувати'),\n",
       "              ('upostag', 'VERB'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 10),\n",
       "              ('form', 'нашу'),\n",
       "              ('lemma', 'наш'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 11),\n",
       "              ('form', 'гіпотезу'),\n",
       "              ('lemma', 'гіпотеза'),\n",
       "              ('upostag', 'NOUN'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 12),\n",
       "              ('form', '.'),\n",
       "              ('lemma', '.'),\n",
       "              ('upostag', 'PUNCT'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1 = convert_string_to_tree_format(sent1)\n",
    "tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2),\n",
       " (2, 0),\n",
       " (2, 9),\n",
       " (3, 4),\n",
       " (4, 2),\n",
       " (5, 9),\n",
       " (6, 9),\n",
       " (7, 9),\n",
       " (8, 9),\n",
       " (9, 0),\n",
       " (10, 11),\n",
       " (11, 9),\n",
       " (12, 9)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pred = dep_parse(convert_string_to_tree_format(sent1), lgb_clf, vec, t_svd)\n",
    "tree_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = 'Ти признайся мені, звідки в тебе ті чари'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('id', 1),\n",
       "              ('form', 'Це'),\n",
       "              ('lemma', 'це'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 2),\n",
       "              ('form', 'є'),\n",
       "              ('lemma', 'бути'),\n",
       "              ('upostag', 'VERB'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 3),\n",
       "              ('form', 'звичайне'),\n",
       "              ('lemma', 'звичайний'),\n",
       "              ('upostag', 'ADJ'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 4),\n",
       "              ('form', 'речення'),\n",
       "              ('lemma', 'речення'),\n",
       "              ('upostag', 'NOUN'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 5),\n",
       "              ('form', ','),\n",
       "              ('lemma', ','),\n",
       "              ('upostag', 'PUNCT'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 6),\n",
       "              ('form', 'на'),\n",
       "              ('lemma', 'на'),\n",
       "              ('upostag', 'PREP'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 7),\n",
       "              ('form', 'якому'),\n",
       "              ('lemma', 'який'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 8),\n",
       "              ('form', 'ми'),\n",
       "              ('lemma', 'ми'),\n",
       "              ('upostag', 'PRON'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 9),\n",
       "              ('form', 'тестуємо'),\n",
       "              ('lemma', 'тестувати'),\n",
       "              ('upostag', 'VERB'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 10),\n",
       "              ('form', 'нашу'),\n",
       "              ('lemma', 'наш'),\n",
       "              ('upostag', 'DET'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 11),\n",
       "              ('form', 'гіпотезу'),\n",
       "              ('lemma', 'гіпотеза'),\n",
       "              ('upostag', 'NOUN'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)]),\n",
       " OrderedDict([('id', 12),\n",
       "              ('form', '.'),\n",
       "              ('lemma', '.'),\n",
       "              ('upostag', 'PUNCT'),\n",
       "              ('xpostag', None),\n",
       "              ('feats', None),\n",
       "              ('head', None),\n",
       "              ('deprel', None),\n",
       "              ('deps', None),\n",
       "              ('misc', None)])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2 = convert_string_to_tree_format(sent1)\n",
    "tree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 0), (3, 2), (4, 9), (5, 9), (6, 7), (8, 9), (9, 2)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_pred = dep_parse(convert_string_to_tree_format(sent2), lgb_clf, vec, t_svd)\n",
    "tree_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
