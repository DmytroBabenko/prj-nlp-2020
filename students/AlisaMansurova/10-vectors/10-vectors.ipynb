{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tokenize_uk\n",
    "from langdetect import detect\n",
    "import pymorphy2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/news_cased_tokenized_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/fiction_lowercased_tokenized_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/news_lowercased_lemmatized_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_tokenized_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W019] Changing vectors name from uk_model.vectors to uk_model.vectors_325250, to avoid clash with previously loaded vectors. See Issue #3853.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# for some reason makes it worse, TODO: play with it and finf a way\n",
    "nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_lemmatized_word2vec') # used for grouping cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_cased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/news_lowercased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_cased_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_tokenized') # for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "        return j[0]['CallZText'], j[0]['CallZType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../1551.gov.ua/raw/'\n",
    "def read_files():\n",
    "    data = []\n",
    "    for d in os.listdir(PATH):\n",
    "        files = os.listdir(PATH + '/' + d)\n",
    "        for file in files:\n",
    "            data.append(open_file(os.path.join(PATH + '/' + d + f'/{file}')))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_categories(data):\n",
    "    return [y for _, y in data]\n",
    "\n",
    "\n",
    "def get_claims(data, cats):\n",
    "    return [x for x, y in data if y in cats]\n",
    "\n",
    "\n",
    "def lowerize(text):\n",
    "    words = tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "    return ' '.join([x.lower() for x in words])\n",
    "\n",
    "# STOP POINT!!!\n",
    "# TODO:\n",
    "# - figure out why lower/lemma don't work\n",
    "# - find better vectors (or compare with existing)\n",
    "def vectorize_sent(v_unk, text, preprocessor):\n",
    "#     text_lower = ' '.join([x.lower() for x in text.split()])\n",
    "#     text_lemma = ' '.join([morph.parse(x)[0].normal_form for x in text.split()])\n",
    "# tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "    \n",
    "    v = 0\n",
    "    text_processed = preprocessor(text)\n",
    "    \n",
    "#     doc = nlp_uk(text_processed)\n",
    "    doc = nlp_uk(text_processed)\n",
    "    total_len = len(doc)\n",
    "    for tok in doc:\n",
    "        # like_num\n",
    "#         if not tok.is_stop:\n",
    "        v += tok.vector\n",
    "#         v /= len(doc)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_claim_label(claim_pair, cat_groups):\n",
    "    label = 'Інше'\n",
    "    claim, current_label = claim_pair\n",
    "    for k, v in cat_groups.items():\n",
    "        if current_label in v:\n",
    "            label = k\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_data(data, cat_groups):\n",
    "    stop = ['вже', 'коли', 'ані', 'над', 'по', 'перед', 'після', 'або', 'вже']\n",
    "    claims = []\n",
    "    labels = []\n",
    "    for x, y in data:\n",
    "        try:\n",
    "            lang = detect(x)\n",
    "            if lang == 'uk': # or lang == 'ru':\n",
    "#                 words = tokenize_uk.tokenize_uk.tokenize_words(x)\n",
    "#                 words = [x for x in words]\n",
    "#                 words = [morph.parse(x)[0].normal_form for x in words if x not in stop]\n",
    "#                 t = ' '.join(words)\n",
    "#                 claims.append(t)\n",
    "                label = get_claim_label((x, y), cat_groups)\n",
    "                claims.append(x)\n",
    "#                 claims.append(t)\n",
    "                labels.append(label)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return claims, labels\n",
    "\n",
    "\n",
    "def get_all_cat_nlps(cats):\n",
    "    return [nlp_uk(x) for x in cats]\n",
    "\n",
    "def normalize_cat(words):\n",
    "        norm = []\n",
    "        for w in words:\n",
    "            m = morph.parse(w)[0]\n",
    "            if m.tag.POS and m.tag.POS != 'PREP' and m.tag.POS != 'PRCL' and m.tag.POS != 'NUMR':\n",
    "                norm.append(m.normal_form)\n",
    "\n",
    "        return nlp_uk(' '.join(norm))\n",
    "    \n",
    "\n",
    "def is_similar_cats(cat1, cat2):\n",
    "    \n",
    "    \n",
    "    def _is_water(w1, w2):\n",
    "        waters = ['ХВП', 'ГВП']\n",
    "        return any(x.upper() in waters for x in w1) and any(x.upper() in waters for x in w2)\n",
    "    \n",
    "    def _is_dism(w1, w2):\n",
    "        dism = 'відсутність опалення'\n",
    "        return set([x.lower() for x in w1]).intersection(dism) and set([x.lower() for x in w2]).intersection(dism)\n",
    "    \n",
    "    def _is_lift(w1, w2):\n",
    "        lift = 'ліфт'\n",
    "        return lift in [x.lower() for x in w1] and lift in [x.lower() for x in w2]\n",
    "    \n",
    "    def _is_inst(w1, w2):\n",
    "        keyword = 'встановлення'\n",
    "        return keyword in [x.lower() for x in w1] and keyword in [x.lower() for x in w2]\n",
    "    \n",
    "    if not cat1 or not cat2:\n",
    "        return False\n",
    "    if not cat1.vector_norm or not cat2.vector_norm:\n",
    "        min_len = min(len(c1), len(c2))\n",
    "        if min_len < 3:\n",
    "            return False\n",
    "        \n",
    "        skip_pos = ['CONJ']\n",
    "        morph1 = [morph.parse(x.text)[0] for x in c1]\n",
    "        morph2 = [morph.parse(x.text)[0] for x in c2]\n",
    "        inter = set([x.normal_form for x in morph1 if x.tag.POS not in skip_pos]).\\\n",
    "                intersection([x.normal_form for x in morph2 if x.tag.POS not in skip_pos])\n",
    "        return len(inter)/min_len > 0.6\n",
    "        \n",
    "    w1 = tokenize_uk.tokenize_uk.tokenize_words(cat1)\n",
    "    w2 = tokenize_uk.tokenize_uk.tokenize_words(cat2)\n",
    "    \n",
    "    if _is_water(w1, w2) or _is_dism(w1, w2) or _is_lift(w1, w2) or _is_inst(w1, w2):\n",
    "        return True\n",
    "    sim = normalize_cat(w1).similarity(normalize_cat(w2))\n",
    "#     sim = cat1.similarity(cat2)\n",
    "    return sim > 0.6\n",
    "\n",
    "\n",
    "def get_common_group_name(docs):\n",
    "    others = []\n",
    "    fst_sent = docs[0].text\n",
    "    words = fst_sent.split()\n",
    "    if 'ХВП' in words or 'ГВП' in words:\n",
    "        fst_sent = 'Проблеми з ХВП/ГВП'\n",
    "    return fst_sent\n",
    "\n",
    "\n",
    "def group_categories(cats, compare_fn):\n",
    "    grouped = []\n",
    "    cache = {}\n",
    "    \n",
    "    def _compare(a, b):\n",
    "        if not cache.get((a, b)):\n",
    "            is_sim = compare_fn(a, b)\n",
    "            cache[(a, b)] = is_sim\n",
    "            cache[(b, a)] = is_sim\n",
    "        else:t\n",
    "            is_sim = cache[(a, b)]\n",
    "        return is_sim\n",
    "\n",
    "    def _is_most_comp(lst, item):\n",
    "        similars = [x for x in lst if _compare(item, x)]\n",
    "        if len(lst) < 3:\n",
    "            return len(similars) == len(lst)\n",
    "        if len(lst) == 3:\n",
    "            return len(similars)/len(lst) > 0.6\n",
    "        return len(similars)/len(lst) >= 0.4\n",
    "    \n",
    "\n",
    "    def _inner(cts):\n",
    "        rst = []\n",
    "        processed = []\n",
    "\n",
    "        if len(cts) == 1:\n",
    "            a, b = cts[0], None\n",
    "        elif len(cts) > 2:\n",
    "            a, b, *rest = cts\n",
    "            rst = rest\n",
    "        elif len(cts) == 2:\n",
    "            a, b = cts\n",
    "        \n",
    "        if len(grouped):\n",
    "            for gr in grouped:\n",
    "                is_a = _is_most_comp(gr, a)\n",
    "                is_b = _is_most_comp(gr, b)\n",
    "                if is_a or is_b:\n",
    "                    idx = grouped.index(gr)\n",
    "                    \n",
    "                    if is_a:\n",
    "                        grouped[idx].append(a)\n",
    "                        processed.append(a)\n",
    "                    if is_b:\n",
    "                        grouped[idx].append(b)\n",
    "                        processed.append(b)\n",
    "        if a not in processed or b not in processed:\n",
    "            if _compare(a, b):\n",
    "                if a not in processed and b in processed:\n",
    "                    grouped.append([a])\n",
    "                elif a in processed and b not in processed:\n",
    "                    grouped.append([b])\n",
    "                else:\n",
    "                    grouped.append([a, b])\n",
    "            else:\n",
    "                if a and a not in processed:\n",
    "                    grouped.append([a])\n",
    "                if b and b not in processed:\n",
    "                    grouped.append([b])\n",
    "        if rst:\n",
    "            ll = len(rst)\n",
    "            if ll % 10 == 0:\n",
    "                print(ll)\n",
    "            _inner(rst)\n",
    "    _inner(cats)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "\n",
    "def get_calegory_groups(cats, compare_fn):\n",
    "    res = {}\n",
    "    cats_grouped = group_categories(cats, compare_fn)\n",
    "    for gr in cats_grouped:\n",
    "        group_name = get_common_group_name(gr)\n",
    "        res[group_name] = gr\n",
    "    return res\n",
    "\n",
    "def get_groups_text(cats_grouped):\n",
    "    res = {}\n",
    "    for k, v in cats_grouped.items():\n",
    "        res[k] = [x.text for x in v]\n",
    "    return res\n",
    "\n",
    "def reduce_cats(cats):\n",
    "    res = {'Інше': 0}\n",
    "#     res = {}\n",
    "    for x, y in cats:\n",
    "        if y > 200:\n",
    "            res[x] = y\n",
    "        else:\n",
    "            res['Інше'] += y\n",
    "    return res\n",
    "\n",
    "def get_all_claims(data):\n",
    "    return [x for x, _ in data]\n",
    "\n",
    "def get_all_claims_labels(data, categories_grouped):\n",
    "    return [get_claim_label(x, categories_grouped) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(data, cats, v_unk, preprocessor):\n",
    "    all_claims, all_labels = get_data(data, cats)\n",
    "    X = [vectorize_sent(v_unk, x, preprocessor) for x in all_claims]\n",
    "    y = all_labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claim_categories = sorted(Counter(get_categories(data)).items(), key=lambda x: x[1], reverse=True)\n",
    "all_categories = sorted(set(reduce_cats(all_claim_categories)))\n",
    "all_cat_nlps = get_all_cat_nlps(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n",
      ">> cache\n"
     ]
    }
   ],
   "source": [
    "categories_grouped_nlp = get_calegory_groups(all_cat_nlps, is_similar_cats)\n",
    "categories_grouped = get_groups_text(categories_grouped_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_unk = nlp_uk('unk')[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
    "                     metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n",
    "                     weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                precision    recall  f1-score   support\n",
      "\n",
      "                                                                          Інше       0.31      0.57      0.40        40\n",
      "                                                                   Інші Подяки       0.00      0.00      0.00         1\n",
      "                                            Інші технічні недоліки стану ліфту       0.14      0.25      0.18         4\n",
      "              Аварійний, травмонебезпечний стан утримання об’єктів благоустрою       0.14      0.17      0.15         6\n",
      "                                Будівництво/дооблаштування дитячого майданчику       0.00      0.00      0.00         2\n",
      "                              Видалення аварійних, пошкоджених хворобами дерев       0.00      0.00      0.00         0\n",
      "                                                    Вилов безпритульних тварин       0.67      1.00      0.80         2\n",
      "                                          Вирізування (кронування) гілля дерев       0.00      0.00      0.00         1\n",
      "                                                   Вологе прибирання приміщень       0.00      0.00      0.00         2\n",
      "                                         Встановлення лічильників на опалення.       0.20      0.20      0.20         5\n",
      "                                                    Відсутнє електропостачання       0.00      0.00      0.00         1\n",
      "             Відсутність каналізаційних люків та решіток на проїжджих частинах       1.00      1.00      1.00         2\n",
      "                                                          Відсутність опалення       0.57      0.67      0.62        18\n",
      "Відсутність освітлення на опорних стовпах за відсутності/несправності лампочок       0.20      0.33      0.25         3\n",
      "                                  Демонтаж рекламних вивісок з опорних стовпів       0.62      1.00      0.77         5\n",
      "      Демонтаж інших об’єктів, що входять до переліку малих архітектурних форм       0.00      0.00      0.00         1\n",
      "                    Евакуація безгосподарських, залишених транспортних засобів       0.00      0.00      0.00         1\n",
      "                                                             Коливання напруги       0.00      0.00      0.00         2\n",
      "                                          Контроль за станом рекламних засобів       0.67      0.67      0.67         3\n",
      "                                                               Не працює бювет       0.00      0.00      0.00         1\n",
      "                                                     Не працює вантажний  ліфт       0.40      0.40      0.40         5\n",
      "                                             Незадовільна температура опалення       0.00      0.00      0.00         3\n",
      "             Незадовільне обслуговування в амбулаторно-поліклінічних установах       0.00      0.00      0.00         8\n",
      "             Незадовільне розташування сміттєвих контейнерів та урн для сміття       1.00      0.25      0.40         4\n",
      "                                   Облаштування наземного пішохідного переходу       0.00      0.00      0.00         1\n",
      "                  Освітлення магістралей (проїжджих частин) та вулиць м. Києва       0.50      0.17      0.25         6\n",
      "   Перевірка дозвільної документації, демонтаж воріт, огорожі, паркану, забору       0.67      0.57      0.62         7\n",
      "                   Перевірка наявності дозволів на виконання будівельних робіт       0.00      0.00      0.00         1\n",
      "                                     Перерахунок плати за відсутність опалення       0.00      0.00      0.00         5\n",
      "                                         Питання освітлення на опорних стовпах       0.67      0.50      0.57         4\n",
      "                        Питання, що стосуються завершення опалювального сезону       0.00      0.00      0.00         3\n",
      "                                               Подяки загалом працівникам ЖЕКу       0.00      0.00      0.00         1\n",
      "                        Прибирання дерев, гілок, листя з закріпленої території       0.00      0.00      0.00         1\n",
      "                                       Прибирання та санітарний стан територій       0.33      0.09      0.14        11\n",
      "                                                 Про розгляд звернень громадян       0.00      0.00      0.00         2\n",
      "                                                            Проблеми з ХВП/ГВП       0.56      0.54      0.55        28\n",
      "                                          Проведення модернізації/заміни ліфту       0.00      0.00      0.00         1\n",
      "                                                                  Ремонт дахів       0.00      0.00      0.00         1\n",
      "                                                              Ремонт під’їзду;       0.00      0.00      0.00         1\n",
      "                                     Ремонт та обслуговування водостічних труб       0.00      0.00      0.00         1\n",
      "                                                         Ремонт фасаду будинку       0.00      0.00      0.00         1\n",
      "                                                             Робота світлофора       0.00      0.00      0.00         1\n",
      "                                 Скління та ремонт вікон на сходових клітинах;       0.00      0.00      0.00         1\n",
      "                                      Укладання та ремонт асфальтного покриття       0.33      0.43      0.38         7\n",
      "\n",
      "                                                                      accuracy                           0.39       204\n",
      "                                                                     macro avg       0.20      0.20      0.19       204\n",
      "                                                                  weighted avg       0.35      0.39      0.35       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_report(data[:1000], categories_grouped, v_unk, lowerize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: there are duplicates under diff cats!\n",
    "# categories_grouped\n",
    "\n",
    "with open('./cats_gr.json', 'w') as f:\n",
    "    json.dump(categories_grouped, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [\n",
    "        \"Відсутність каналізаційних люків та решіток на проїжджих частинах\",\n",
    "        \"Відсутність каналізаційних, водопровідних люків\",\n",
    "        \"Обслуговування та технічний стан каналізаційних колодязів\",\n",
    "        \"Технічний стан проїжджих частин вулиць та тротуарів\",\n",
    "        \"Технічний стан підземних пішохідних переходів\"\n",
    "    ]\n",
    "\n",
    "c1 = nlp_uk(sents[0])\n",
    "c2 = nlp_uk(sents[1])\n",
    "c3 = nlp_uk(sents[2])\n",
    "c4 = nlp_uk(sents[3])\n",
    "c5 = nlp_uk(sents[4])\n",
    "c1.similarity(c5)\n",
    "\n",
    "cache = {}\n",
    "cache[(c1, c2)] = True\n",
    "cache[(c1, c2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aa(c1, c2):\n",
    "    skip_pos = ['CONJ']\n",
    "    morph1 = [morph.parse(x.text)[0] for x in c1]\n",
    "    morph2 = [morph.parse(x.text)[0] for x in c2]\n",
    "    inter = set([x.normal_form for x in morph1 if x.tag.POS not in skip_pos]).\\\n",
    "            intersection([x.normal_form for x in morph2 if x.tag.POS not in skip_pos])\n",
    "    print(inter, len(inter)/min(len(c1), len(c2)))\n",
    "    return len(inter)/min(len(c1), len(c2)) >= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'технічний', 'стан'} 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa(c3, c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Технічний ADJF\n",
      "стан NOUN\n",
      "проїжджих ADJF\n",
      "частин NOUN\n",
      "вулиць NOUN\n",
      "та CONJ\n",
      "тротуарів NOUN\n"
     ]
    }
   ],
   "source": [
    "for tok in c4:\n",
    "    print(tok, morph.parse(tok.text)[0].tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _compare(a, b):\n",
    "        if not cache.get((a, b)):\n",
    "            is_sim = compare_fn(a, b)\n",
    "            cache[(a, b)] = is_sim\n",
    "            cache[(b, a)] = is_sim\n",
    "        else:\n",
    "            print('>> cache')\n",
    "            is_sim = cache[(a, b)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
