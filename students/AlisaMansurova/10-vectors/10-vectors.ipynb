{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tokenize_uk\n",
    "from langdetect import detect\n",
    "import pymorphy2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/news_cased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/fiction_lowercased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/news_lowercased_lemmatized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W019] Changing vectors name from uk_model.vectors to uk_model.vectors_538431, to avoid clash with previously loaded vectors. See Issue #3853.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason makes it worse, TODO: play with it and finf a way\n",
    "nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_lemmatized_word2vec') # used for grouping cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_cased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/news_lowercased_tokenized_word2vec') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W019] Changing vectors name from uk_model.vectors to uk_model.vectors_331941, to avoid clash with previously loaded vectors. See Issue #3853.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_cased_lemmatized') # no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/tmp/uk_vectors/ubercorpus_lowercased_tokenized') # for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "        return j[0]['CallZText'], j[0]['CallZType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../1551.gov.ua/raw/'\n",
    "def read_files():\n",
    "    data = []\n",
    "    for d in os.listdir(PATH):\n",
    "        files = os.listdir(PATH + '/' + d)\n",
    "        for file in files:\n",
    "            data.append(open_file(os.path.join(PATH + '/' + d + f'/{file}')))\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_categories(data):\n",
    "    return [y for _, y in data]\n",
    "\n",
    "\n",
    "def get_claims(data, cats):\n",
    "    return [x for x, y in data if y in cats]\n",
    "\n",
    "\n",
    "def lowerize(text):\n",
    "    words = tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "#     morphs = [morph.parse(x)[0] for x in words]\n",
    "#     lemmas = [x.normal_form for x in morphs]\n",
    "#     return ' '.join([x.lower() for x in lemmas])\n",
    "    return ' '.join([x.lower() for x in words])\n",
    "\n",
    "\n",
    "def vectorize_sent(v_unk, text, preprocessor):\n",
    "    v = 0\n",
    "    text_processed = preprocessor(text)\n",
    "    \n",
    "    doc = nlp_uk(text_processed)\n",
    "    total_len = len(doc)\n",
    "    for tok in doc:\n",
    "        # like_num\n",
    "#         if not tok.is_stop:\n",
    "        v += tok.vector\n",
    "#         v /= len(doc)\n",
    "    return v_unk if type(v) == int else v\n",
    "\n",
    "\n",
    "def get_claim_label(claim_pair, cat_groups):\n",
    "    label = 'Інше'\n",
    "    claim, current_label = claim_pair\n",
    "    for k, v in cat_groups.items():\n",
    "        if current_label in v:\n",
    "            label = k\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_data(data, cat_groups):\n",
    "    claims = []\n",
    "    labels = []\n",
    "    for x, y in data:\n",
    "        label = get_claim_label((x, y), cat_groups)\n",
    "        claims.append(x)\n",
    "        labels.append(label)\n",
    " \n",
    "    return claims, labels\n",
    "\n",
    "\n",
    "def get_all_cat_nlps(cats):\n",
    "    return [nlp_uk(x) for x in cats]\n",
    "\n",
    "def normalize_cat(words):\n",
    "        norm = []\n",
    "        for w in words:\n",
    "            m = morph.parse(w)[0]\n",
    "            if m.tag.POS and m.tag.POS != 'PREP' and m.tag.POS != 'PRCL' and m.tag.POS != 'NUMR':\n",
    "                norm.append(m.normal_form)\n",
    "\n",
    "        return nlp_uk(' '.join(norm))\n",
    "    \n",
    "\n",
    "def is_similar_cats(cat1, cat2):\n",
    "    \n",
    "    \n",
    "    def _is_water(w1, w2):\n",
    "        waters = ['ХВП', 'ГВП']\n",
    "        return any(x.upper() in waters for x in w1) and any(x.upper() in waters for x in w2)\n",
    "    \n",
    "    def _is_dism(w1, w2):\n",
    "        dism = 'відсутність опалення'\n",
    "        return set([x.lower() for x in w1]).intersection(dism) and set([x.lower() for x in w2]).intersection(dism)\n",
    "    \n",
    "    def _is_lift(w1, w2):\n",
    "        lift = 'ліфт'\n",
    "        return lift in [x.lower() for x in w1] and lift in [x.lower() for x in w2]\n",
    "    \n",
    "    def _is_inst(w1, w2):\n",
    "        keyword = 'встановлення'\n",
    "        return keyword in [x.lower() for x in w1] and keyword in [x.lower() for x in w2]\n",
    "    \n",
    "    if not cat1 or not cat2:\n",
    "        return False\n",
    "    if not cat1.vector_norm or not cat2.vector_norm:\n",
    "        min_len = min(len(cat1), len(cat2))\n",
    "        if min_len < 3:\n",
    "            return False\n",
    "        \n",
    "        skip_pos = ['CONJ']\n",
    "        morph1 = [morph.parse(x.text)[0] for x in cat1]\n",
    "        morph2 = [morph.parse(x.text)[0] for x in cat2]\n",
    "        inter = set([x.normal_form for x in morph1 if x.tag.POS not in skip_pos]).\\\n",
    "                intersection([x.normal_form for x in morph2 if x.tag.POS not in skip_pos])\n",
    "        return len(inter)/min_len > 0.6\n",
    "        \n",
    "    w1 = tokenize_uk.tokenize_uk.tokenize_words(cat1)\n",
    "    w2 = tokenize_uk.tokenize_uk.tokenize_words(cat2)\n",
    "    \n",
    "    if _is_water(w1, w2) or _is_dism(w1, w2) or _is_lift(w1, w2) or _is_inst(w1, w2):\n",
    "        return True\n",
    "    sim = normalize_cat(w1).similarity(normalize_cat(w2))\n",
    "    return sim > 0.6\n",
    "\n",
    "\n",
    "def get_common_group_name(docs):\n",
    "    others = []\n",
    "    fst_sent = docs[0].text\n",
    "    words = fst_sent.split()\n",
    "    if 'ХВП' in words or 'ГВП' in words:\n",
    "        fst_sent = 'Проблеми з ХВП/ГВП'\n",
    "    return fst_sent\n",
    "\n",
    "\n",
    "def group_categories(cats, compare_fn):\n",
    "    grouped = []\n",
    "    cache = {}\n",
    "    \n",
    "    def _compare(a, b):\n",
    "        if not cache.get((a, b)):\n",
    "            is_sim = compare_fn(a, b)\n",
    "            cache[(a, b)] = is_sim\n",
    "            cache[(b, a)] = is_sim\n",
    "        else:\n",
    "            is_sim = cache[(a, b)]\n",
    "        return is_sim\n",
    "\n",
    "    def _is_most_comp(lst, item):\n",
    "        similars = [x for x in lst if _compare(item, x)]\n",
    "        if len(lst) < 3:\n",
    "            return len(similars) == len(lst)\n",
    "        if len(lst) == 3:\n",
    "            return len(similars)/len(lst) > 0.6\n",
    "        return len(similars)/len(lst) >= 0.4\n",
    "    \n",
    "\n",
    "    def _inner(cts):\n",
    "        rst = []\n",
    "        processed = []\n",
    "\n",
    "        if len(cts) == 1:\n",
    "            a, b = cts[0], None\n",
    "        elif len(cts) > 2:\n",
    "            a, b, *rest = cts\n",
    "            rst = rest\n",
    "        elif len(cts) == 2:\n",
    "            a, b = cts\n",
    "        \n",
    "        if len(grouped):\n",
    "            for gr in grouped:\n",
    "                is_a = _is_most_comp(gr, a)\n",
    "                is_b = _is_most_comp(gr, b)\n",
    "                if is_a or is_b:\n",
    "                    idx = grouped.index(gr)\n",
    "                    \n",
    "                    if is_a:\n",
    "                        grouped[idx].append(a)\n",
    "                        processed.append(a)\n",
    "                    if is_b:\n",
    "                        grouped[idx].append(b)\n",
    "                        processed.append(b)\n",
    "        if a not in processed or b not in processed:\n",
    "            if _compare(a, b):\n",
    "                if a not in processed and b in processed:\n",
    "                    grouped.append([a])\n",
    "                elif a in processed and b not in processed:\n",
    "                    grouped.append([b])\n",
    "                else:\n",
    "                    grouped.append([a, b])\n",
    "            else:\n",
    "                if a and a not in processed:\n",
    "                    grouped.append([a])\n",
    "                if b and b not in processed:\n",
    "                    grouped.append([b])\n",
    "        if rst:\n",
    "            ll = len(rst)\n",
    "            if ll % 10 == 0:\n",
    "                print(ll)\n",
    "            _inner(rst)\n",
    "    _inner(cats)\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "\n",
    "def get_calegory_groups(cats, compare_fn):\n",
    "    res = {}\n",
    "    cats_grouped = group_categories(cats, compare_fn)\n",
    "    for gr in cats_grouped:\n",
    "        group_name = get_common_group_name(gr)\n",
    "        res[group_name] = gr\n",
    "    return res\n",
    "\n",
    "def get_groups_text(cats_grouped):\n",
    "    res = {}\n",
    "    for k, v in cats_grouped.items():\n",
    "        res[k] = [x.text for x in v]\n",
    "    return res\n",
    "\n",
    "def reduce_cats(cats):\n",
    "    res = {'Інше': 0}\n",
    "#     res = {}\n",
    "    for x, y in cats:\n",
    "        if y > 200:\n",
    "            res[x] = y\n",
    "        else:\n",
    "            res['Інше'] += y\n",
    "    return res\n",
    "\n",
    "def get_all_claims(data):\n",
    "    return [x for x, _ in data]\n",
    "\n",
    "def get_all_claims_labels(data, categories_grouped):\n",
    "    return [get_claim_label(x, categories_grouped) for x in data]\n",
    "\n",
    "def filter_uk_claims(data):\n",
    "    res = []\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        try:\n",
    "            lang = detect(x)\n",
    "            if lang == 'uk':\n",
    "                res.append((x, y))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#         if i % 1000 == 0:\n",
    "#             print('>>', i)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(data, cats, v_unk, preprocessor):\n",
    "    all_claims, all_labels = get_data(data, cats)\n",
    "    X = [vectorize_sent(v_unk, x, preprocessor) for x in all_claims]\n",
    "    y = all_labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_data = filter_uk_claims(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_claim_categories = sorted(Counter(get_categories(uk_data)).items(), key=lambda x: x[1], reverse=True)\n",
    "all_categories = sorted(set(reduce_cats(all_claim_categories)))\n",
    "all_cat_nlps = get_all_cat_nlps(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories_grouped_nlp = get_calegory_groups(all_cat_nlps, is_similar_cats)\n",
    "categories_grouped = get_groups_text(categories_grouped_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_unk = nlp_uk('unk')[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
    "                     metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n",
    "                     weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                                                            Інше       0.52      0.78      0.62      8277\n",
      "                                                                     Інші Подяки       0.32      0.17      0.22        76\n",
      "                                              Інші технічні недоліки стану ліфту       0.18      0.21      0.20       215\n",
      "                Аварійний, травмонебезпечний стан утримання об’єктів благоустрою       0.46      0.50      0.48       763\n",
      "                                  Будівництво/дооблаштування дитячого майданчику       0.34      0.38      0.36       221\n",
      "                                Видалення аварійних, пошкоджених хворобами дерев       0.26      0.17      0.21        82\n",
      "                                            Вирізування (кронування) гілля дерев       0.29      0.22      0.25        67\n",
      "                                                     Вологе прибирання приміщень       0.29      0.22      0.25       134\n",
      "                                           Встановлення лічильників на опалення.       0.28      0.25      0.26       507\n",
      "                                                         Встановлення світлофора       0.20      0.22      0.21        55\n",
      "               Встановлення сигнальних стовпчиків, бар’єрних огороджень,бордюрів       0.14      0.10      0.12        89\n",
      "Відновлення благоустрою після вик. планових,аварійних робіт на об’єктах благоуст       0.23      0.19      0.21       379\n",
      "                                                      Відсутнє електропостачання       0.33      0.19      0.24       149\n",
      "                                 Відсутність каналізаційних, водопровідних люків       0.07      0.01      0.02        68\n",
      "                                                            Відсутність опалення       0.55      0.59      0.57      1948\n",
      "                                                   ГЛ ”Несанкціонована торгівля”       0.40      0.23      0.29       241\n",
      "                                        Демонтаж рекламних конструкцій і вивісок       0.25      0.15      0.18       636\n",
      "        Демонтаж інших об’єктів, що входять до переліку малих архітектурних форм       0.00      0.00      0.00        62\n",
      "                                  Знищення омели, амброзії та рослин - паразитів       0.14      0.02      0.04        48\n",
      "                                                               Коливання напруги       0.58      0.18      0.28        60\n",
      "                                            Контроль за станом рекламних засобів       0.19      0.06      0.09       107\n",
      "                                                    Нанесення дорожньої розмітки       0.32      0.09      0.13        94\n",
      "                                                                 Не працює бювет       0.62      0.38      0.47       546\n",
      "               Незадовільне розташування сміттєвих контейнерів та урн для сміття       0.38      0.26      0.31       182\n",
      "     Перевірка дозвільної документації, демонтаж воріт, огорожі, паркану, забору       0.44      0.28      0.34       816\n",
      "                     Перевірка наявності дозволів на виконання будівельних робіт       0.40      0.15      0.22       175\n",
      "        Перерахунок та нарахування плати за інші види житлово-комунальних послуг       0.47      0.43      0.45       330\n",
      "                                           Питання освітлення на опорних стовпах       0.37      0.09      0.15       154\n",
      "                                                            Прибирання приміщень       0.42      0.23      0.30       220\n",
      "                                                   Про розгляд звернень громадян       0.28      0.10      0.14       301\n",
      "                                                              Проблеми з ХВП/ГВП       0.44      0.12      0.19       323\n",
      "                                                          Проведення дератизації       0.25      0.06      0.09        71\n",
      "                                    Ремонт або демонтаж огородження зеленої зони       0.49      0.28      0.35      1485\n",
      "                                                                    Ремонт дахів       0.43      0.13      0.20       196\n",
      "                                                                Ремонт під’їзду;       0.56      0.25      0.34       485\n",
      "                                                               Робота світлофора       0.68      0.22      0.33       147\n",
      "                                                    Робота циркуляційної системи       0.12      0.02      0.04       138\n",
      "                                                          Стихійне сміттєзвалище       0.39      0.07      0.11       196\n",
      "                             Технічний стан проїжджих частин вулиць та тротуарів       0.38      0.10      0.16       415\n",
      "                                       Удосконалення організації дорожнього руху       0.26      0.10      0.14        51\n",
      "                               Утримання підвалів, колясочних технічних поверхів       0.62      0.16      0.25       194\n",
      "\n",
      "                                                                        accuracy                           0.49     20703\n",
      "                                                                       macro avg       0.35      0.20      0.24     20703\n",
      "                                                                    weighted avg       0.46      0.49      0.45     20703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_report(uk_data, categories_grouped, v_unk, lowerize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./cats_gr.json', 'w') as f:\n",
    "    json.dump(categories_grouped, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_grouped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
