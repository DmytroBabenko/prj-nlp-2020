{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../UD_Ukrainian-IU'\n",
    "\n",
    "with open(PATH + '/uk_iu-ud-train.conllu') as f:\n",
    "    train_data = f.read()\n",
    "    \n",
    "with open(PATH + '/uk_iu-ud-dev.conllu') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "train_trees = parse(train_data)\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Utils \"\"\"\n",
    "\n",
    "\n",
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "\n",
    "def stringify_feats(token):\n",
    "    token_feats = token['feats']\n",
    "    return ';'.join([f'{k}={v}' for k, v in token_feats.items()]) if token_feats else 'NONE'\n",
    "\n",
    "\n",
    "\"\"\" Feature extractors \"\"\"\n",
    "\n",
    "\n",
    "def feature_extractor_base(stack, queue, _):\n",
    "    feat = {}\n",
    "\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-word'] = top_stack['form']\n",
    "        feat['s0-lemma'] = top_stack['lemma']\n",
    "        feat['s0-pos'] = top_stack['upostag']\n",
    "    if (len(stack)) > 1:\n",
    "        feat['s1-pos'] = stack[-2]['upostag']\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-word'] = top_queue['form']\n",
    "        feat['q0-lemma'] = top_queue['lemma']\n",
    "        feat['q0-pos'] = top_queue['upostag']\n",
    "    if (len(queue)) > 1:\n",
    "        q_next = queue[1]\n",
    "        feat['q1-word'] = q_next['form']\n",
    "        feat['q1-pos'] = q_next['upostag']\n",
    "    if (len(queue)) > 2:\n",
    "        feat['q2-pos'] = queue[2]['upostag']\n",
    "    if (len(queue)) > 3:\n",
    "        feat['q3-pos'] = queue[3]['upostag']\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_feats(stack, queue, _):\n",
    "    feat = {}\n",
    "\n",
    "    if stack:\n",
    "        feat['s0-feats'] = stringify_feats(stack[-1])\n",
    "    if (len(stack)) > 1:\n",
    "        feat['s1-feats'] = stringify_feats(stack[-2])\n",
    "    if queue:\n",
    "        feat['q0-feats'] = stringify_feats(queue[0])\n",
    "    if (len(queue)) > 1:\n",
    "        feat['q1-feats'] = stringify_feats(queue[1])\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_deprels(stack, queue, relations):\n",
    "    def _get_ldep_rdep(id, relations):\n",
    "        left = 100500\n",
    "        right = -1\n",
    "        ldep = 'NONE'\n",
    "        rdep = 'NONE'\n",
    "        for (ch, head, rel) in relations:\n",
    "            if head == id:\n",
    "                if (ch < head) and (ch < left):\n",
    "                    left = ch\n",
    "                    ldep = rel\n",
    "                if (ch > head) and (ch > right):\n",
    "                    right = ch\n",
    "                    rdep = rel\n",
    "        return ldep, rdep\n",
    "\n",
    "    feat = {}\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-deprel'] = top_stack['deprel'] or 'NONE'\n",
    "        ldep, rdep = _get_ldep_rdep(top_stack['id'], relations)\n",
    "        feat['s0-ldep'] = ldep\n",
    "        feat['s0-rdep'] = rdep\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-deprel'] = top_queue['deprel'] or 'NONE'\n",
    "        ldep, rdep = _get_ldep_rdep(top_queue['id'], relations)\n",
    "        feat['q0-ldep'] = ldep\n",
    "        feat['q0-rdep'] = rdep\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_path_to_root(stack, queue, relations):\n",
    "    def get_path_to_root(id, relations):\n",
    "        curr_ch = id\n",
    "        steps = 0\n",
    "        rels_sorted = sorted(relations, key=lambda x: x[0] == id, reverse=True)\n",
    "        for (ch, head, rel) in rels_sorted:\n",
    "            if curr_ch == ch:\n",
    "                steps = + 1\n",
    "                curr_ch = head\n",
    "        return steps\n",
    "\n",
    "    feat = {}\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-path-root'] = get_path_to_root(top_stack['id'], relations)\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-path-root'] = get_path_to_root(top_queue['id'], relations)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_dep_feats(stack, queue, relations):\n",
    "    def get_ldep_rdep_feats(id, relations, tokens):\n",
    "        left = 100500\n",
    "        right = -1\n",
    "        lfeat = 'NONE'\n",
    "        rfeat = 'NONE'\n",
    "        for (ch, head, _) in relations:\n",
    "            if head == id:\n",
    "                if (ch < head) and (ch < left):\n",
    "                    left = ch\n",
    "                if (ch > head) and (ch > right):\n",
    "                    right = ch\n",
    "\n",
    "        l_tok = next((x for x in tokens if x['id'] == left), None)\n",
    "        r_tok = next((x for x in tokens if x['id'] == right), None)\n",
    "        if l_tok:\n",
    "            lfeat = stringify_feats(l_tok)\n",
    "        if r_tok:\n",
    "            rfeat = stringify_feats(r_tok)\n",
    "\n",
    "        return lfeat, rfeat\n",
    "\n",
    "    feat = {}\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        ldep_f, rdep_f = get_ldep_rdep_feats(top_stack['id'], relations, stack)\n",
    "        feat['s0-ldep-feats'] = ldep_f\n",
    "        feat['s0-rdep-feats'] = rdep_f\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        ldep_f, rdep_f = get_ldep_rdep_feats(top_queue['id'], relations, stack)\n",
    "        feat['q0-ldep-feats'] = ldep_f\n",
    "        feat['q0-rdep-feats'] = rdep_f\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "\"\"\" Parser \"\"\"\n",
    "\n",
    "\n",
    "class Actions(str, Enum):\n",
    "    SHIFT = 'shift'\n",
    "    REDUCE = 'reduce'\n",
    "    RIGHT = 'right'\n",
    "    LEFT = 'left'\n",
    "\n",
    "\n",
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats',\n",
    "                                        None), ('head', None),  ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "\n",
    "# the very basic variant of deprels oracle\n",
    "# TODO: improve it!\n",
    "def predict_deprel(child, head, tokens):\n",
    "    ch_pos = child['upostag']\n",
    "    head_pos = head['upostag'] if not type(head) == str else head\n",
    "\n",
    "    def is_noun(tag):\n",
    "        return tag == 'NOUN'\n",
    "\n",
    "    def is_noun_or_pnoun(tag):\n",
    "        return tag == 'NOUN' or tag == 'PNOUN'\n",
    "\n",
    "    def get_ch_child(ch):\n",
    "        return next((x for x in tokens if x.get('head') and x['head'] == ch['id']), None)\n",
    "\n",
    "    if head_pos == 'root':\n",
    "        return 'root'\n",
    "    elif is_noun(head_pos) and is_noun(ch_pos) or head_pos == 'DET' and is_noun(ch_pos):\n",
    "        ch_child = get_ch_child(child)\n",
    "        if ch_child and ch_child['upostag'] == 'PUNCT':\n",
    "            return 'appos'\n",
    "        return 'nmod'\n",
    "    elif (is_noun_or_pnoun(head_pos) or head_pos == 'PRON') and ch_pos == 'ADP':\n",
    "        return 'case'\n",
    "    elif head_pos == 'ADJ' and ch_pos == 'ADV' or head_pos == 'VERB' and ch_pos == 'ADV':\n",
    "        return 'advmod'\n",
    "    elif is_noun_or_pnoun(head_pos) and ch_pos == 'ADJ':\n",
    "        return 'amod'\n",
    "    elif ch_pos == 'PUNCT':\n",
    "        return 'punct'\n",
    "    elif ch_pos == 'CCONJ':\n",
    "        return 'cc'\n",
    "    elif head_pos == 'PROPN' and ch_pos == 'PROPN':\n",
    "        ch_child = get_ch_child(child)\n",
    "        if not ch_child:\n",
    "            return 'flat:name'\n",
    "        if ch_child and ch_child['upostag'] == 'CCONJ':\n",
    "            return 'conj'\n",
    "        return 'flat:title'\n",
    "    elif head_pos == 'NOUN' and ch_pos == 'PROPN':\n",
    "        ch_child = get_ch_child(child)\n",
    "        if ch_child:\n",
    "            kid = get_ch_child(ch_child)\n",
    "            if ch_child['upostag'] == 'PROPN' and not kid or kid and kid['upostag'] == 'CCONJ':\n",
    "                return 'nmod'\n",
    "        return 'flat:title'\n",
    "    elif head_pos == 'VERB' and (ch_pos == 'NOUN' or ch_pos == 'PRON') \\\n",
    "            or head_pos == 'DET' and ch_pos == 'PRON':\n",
    "        ch_child = get_ch_child(child)\n",
    "        if ch_child:\n",
    "            if ch_child['upostag'] == 'ADP':\n",
    "                return 'obl'\n",
    "            return 'nsubj'\n",
    "        else:\n",
    "            if ch_pos == 'PRON':\n",
    "                return 'nsubj'\n",
    "        return 'obj'\n",
    "    elif head_pos == 'VERB' and ch_pos == 'VERB':\n",
    "        ch_child = get_ch_child(child)\n",
    "        if ch_child and ch_child['upostag'] == 'PUNCT':\n",
    "            return 'advcl'\n",
    "        return 'xcomp'\n",
    "    elif head_pos == 'ADV' and ch_pos == 'PART':\n",
    "        return 'discourse'\n",
    "    elif head_pos == 'NOUN' and ch_pos == 'DET':\n",
    "        return 'det'\n",
    "    elif head_pos == 'NOUN' and ch_pos == 'VERB':\n",
    "        return 'acl'\n",
    "    elif head_pos == 'NOUN' and ch_pos == 'SCONJ':\n",
    "        return 'mark'\n",
    "    elif head_pos == 'VERB' and ch_pos == 'ADJ':\n",
    "        return 'advcl:sp'\n",
    "    elif head_pos == 'ADJ' and ch_pos == 'NOUN':\n",
    "        ch_child = get_ch_child(child)\n",
    "        if ch_child and ch_child['upostag'] == 'PUNCT':\n",
    "            return 'advcl'\n",
    "        return 'obj'\n",
    "\n",
    "\n",
    "def oracle(stack, top_queue, relations):\n",
    "    \"\"\"\n",
    "    Make a decision on the right action to do.\n",
    "    \"\"\"\n",
    "    top_stack = stack[-1]\n",
    "    # check if both stack and queue are non-empty\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    # check if there are any clear dependencies\n",
    "    elif top_queue['head'] == top_stack['id']:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack['head'] == top_queue['id']:\n",
    "        return Actions.LEFT\n",
    "    # check if we can reduce the top of the stack\n",
    "    elif top_stack['id'] in [i[0] for i in relations] and \\\n",
    "        (top_queue['head'] < top_stack['id'] or\n",
    "         [s for s in stack if s['head'] == top_queue['id']]):\n",
    "        return Actions.REDUCE\n",
    "    # default option\n",
    "    else:\n",
    "        return Actions.SHIFT\n",
    "\n",
    "\n",
    "def dep_parse(tree, clf, vectorizer, feature_extractor):\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = feature_extractor(stack, queue, relations)\n",
    "            action = clf.predict(vectorizer.transform([features]))[0]\n",
    "\n",
    "            if action == Actions.SHIFT:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == Actions.REDUCE:\n",
    "                stack.pop()\n",
    "            elif action == Actions.LEFT:\n",
    "                deprel = stack[-1]['deprel'] or predict_deprel(\n",
    "                    stack[-1], queue[0], stack + queue)\n",
    "                rel = (stack[-1]['id'], queue[0]['id'], deprel)\n",
    "                relations.append(rel)\n",
    "                stack.pop()\n",
    "            elif action == Actions.RIGHT:\n",
    "                deprel = queue[0]['deprel'] or predict_deprel(\n",
    "                    queue[0], stack[-1], stack + queue)\n",
    "                rel = (queue[0]['id'], stack[-1]['id'], deprel)\n",
    "                relations.append(rel)\n",
    "                stack.append(queue.pop(0))\n",
    "\n",
    "    return sorted(relations)\n",
    "\n",
    "\n",
    "\"\"\" Data & reporting utils \"\"\"\n",
    "\n",
    "\n",
    "def get_data_for_tree(tree, feature_extractor):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        queue[0] if len(queue) > 0 else None,\n",
    "                        relations)\n",
    "        features.append(feature_extractor(stack, queue, relations))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            deprel = stack[-1]['deprel'] or predict_deprel(\n",
    "                stack[-1], queue[0], stack + queue)\n",
    "            rel = (stack[-1]['id'], queue[0]['id'], deprel)\n",
    "            relations.append(rel)\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            deprel = queue[0]['deprel'] or predict_deprel(\n",
    "                queue[0], stack[-1], stack + queue)\n",
    "            rel = (queue[0]['id'], stack[-1]['id'], deprel)\n",
    "            relations.append(rel)\n",
    "            stack.append(queue.pop(0))\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def get_data(trees, feature_extractor):\n",
    "    features, labels = [], []\n",
    "    for tree in trees:\n",
    "        t_f, t_l = get_data_for_tree(\n",
    "            [t for t in tree if type(t['id']) == int], feature_extractor)\n",
    "        features += t_f\n",
    "        labels += t_l\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def calculate_as(trees, clf, vect, feature_extractor):\n",
    "    total, tpu, tpl, full_match = 0, 0, 0, 0\n",
    "    golden_u, golden_l = None, None\n",
    "    for tree in trees:\n",
    "        tree = [t for t in tree if type(t['id']) == int]\n",
    "        golden_all = [(node['id'], node['head'], node['deprel'])\n",
    "                      for node in tree]\n",
    "        golden_u = [(x, y) for x, y, _ in golden_all]\n",
    "\n",
    "        predicted_all = dep_parse(tree, clf, vect, feature_extractor)\n",
    "        predicted_u = [(x, y) for x, y, _ in predicted_all]\n",
    "\n",
    "        total += len(tree)\n",
    "        tpu += len(set(golden_u).intersection(set(predicted_u)))\n",
    "        tpl += len(set(golden_all).intersection(set(predicted_all)))\n",
    "\n",
    "        if set(golden_all) == set(predicted_all):\n",
    "            full_match += 1\n",
    "\n",
    "    print('== Attachment score report ==')\n",
    "    print('Total: ', total)\n",
    "    print('Match unlabeled: ', tpu)\n",
    "#     print('Match labeled: ', tpl)\n",
    "    print('UAS: ', round(tpu/total, 2))\n",
    "#     print('LAS: ', round(tpl/total, 2))\n",
    "    print(\"Full match:\", round(full_match/len(trees), 2))\n",
    "\n",
    "\n",
    "def get_lrc_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def print_result(train_trees, test_trees, clf, feature_extractor):\n",
    "    train_feat, train_lab = get_data(train_trees, feature_extractor)\n",
    "    test_feat, test_lab = get_data(test_trees, feature_extractor)\n",
    "\n",
    "    clf.fit(train_feat, train_lab)\n",
    "    print(classification_report(test_lab, clf.predict(test_feat)))\n",
    "    calculate_as(test_trees, clf['lrc'], clf['dict_vect'], feature_extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_lrc_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.86      0.87      0.86      6371\n",
      "      reduce       0.85      0.78      0.81      6875\n",
      "       right       0.75      0.79      0.77      5996\n",
      "       shift       0.85      0.87      0.86      6578\n",
      "\n",
      "    accuracy                           0.83     25820\n",
      "   macro avg       0.83      0.83      0.83     25820\n",
      "weighted avg       0.83      0.83      0.83     25820\n",
      "\n",
      "== Attachment score report ==\n",
      "Total:  12574\n",
      "Match unlabeled:  8717\n",
      "UAS:  0.69\n",
      "Full match: 0.09\n"
     ]
    }
   ],
   "source": [
    "print_result(train_trees, test_trees, clf, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.87      0.89      0.88      6371\n",
      "      reduce       0.86      0.81      0.83      6875\n",
      "       right       0.78      0.80      0.79      5996\n",
      "       shift       0.87      0.88      0.87      6578\n",
      "\n",
      "    accuracy                           0.85     25820\n",
      "   macro avg       0.85      0.85      0.85     25820\n",
      "weighted avg       0.85      0.85      0.85     25820\n",
      "\n",
      "== Attachment score report ==\n",
      "Total:  12574\n",
      "Match unlabeled:  9089\n",
      "UAS:  0.72\n",
      "Full match: 0.11\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats)\n",
    "print_result(train_trees, test_trees, clf, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With deprels (показує покращення, але на нових даних працює погано, губить слова... тому далі не використовую)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.94      0.96      0.95      6371\n",
      "      reduce       0.92      0.87      0.90      6875\n",
      "       right       0.89      0.91      0.90      5996\n",
      "       shift       0.93      0.94      0.94      6578\n",
      "\n",
      "    accuracy                           0.92     25820\n",
      "   macro avg       0.92      0.92      0.92     25820\n",
      "weighted avg       0.92      0.92      0.92     25820\n",
      "\n",
      "== Attachment score report ==\n",
      "Total:  12574\n",
      "Match unlabeled:  10495\n",
      "UAS:  0.83\n",
      "Full match: 0.23\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats,\n",
    "                            feature_extractor_deprels)\n",
    "print_result(train_trees, test_trees, clf, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With deprel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.87      0.89      0.88      6371\n",
      "      reduce       0.85      0.81      0.83      6875\n",
      "       right       0.79      0.80      0.79      5996\n",
      "       shift       0.86      0.88      0.87      6578\n",
      "\n",
      "    accuracy                           0.85     25820\n",
      "   macro avg       0.84      0.85      0.84     25820\n",
      "weighted avg       0.85      0.85      0.85     25820\n",
      "\n",
      "== Attachment score report ==\n",
      "Total:  12574\n",
      "Match unlabeled:  9090\n",
      "UAS:  0.72\n",
      "Full match: 0.11\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats,\n",
    "                            feature_extractor_dep_feats)\n",
    "print_result(train_trees, test_trees, clf, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. With path to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.93      0.95      0.94      6371\n",
      "      reduce       0.89      0.86      0.88      6875\n",
      "       right       0.81      0.81      0.81      5996\n",
      "       shift       0.87      0.88      0.87      6578\n",
      "\n",
      "    accuracy                           0.88     25820\n",
      "   macro avg       0.88      0.88      0.88     25820\n",
      "weighted avg       0.88      0.88      0.88     25820\n",
      "\n",
      "== Attachment score report ==\n",
      "Total:  12574\n",
      "Match unlabeled:  9051\n",
      "UAS:  0.72\n",
      "Full match: 0.14\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats,\n",
    "                            feature_extractor_dep_feats,\n",
    "                            feature_extractor_path_to_root)\n",
    "print_result(train_trees, test_trees, clf, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Use parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "\n",
    "\n",
    "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
    "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
    "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
    "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
    "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
    "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
    "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
    "\n",
    "PREP = [\"до\", \"на\"]\n",
    "\n",
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if \"coord\" in word.tag:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    elif \"PNCT\" in word.tag:\n",
    "        return \"PUNCT\"\n",
    "    elif word.normal_form in PREP:\n",
    "        return \"PREP\"\n",
    "    elif word.normal_form in DET:\n",
    "        return \"DET\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS) or 'X'\n",
    "\n",
    "\n",
    "def pym2_to_conllu(tokens):\n",
    "    id = 1\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        word = {}\n",
    "        word['id'] = id\n",
    "        word['form'] = token.word\n",
    "        word['lemma'] = token.normal_form\n",
    "        word['upostag'] = str(normalize_pos(token))\n",
    "        word['feats'] = {'Animacy': token.tag.animacy,\n",
    "                         'Case': token.tag.case,\n",
    "                         'Gender': token.tag.gender,\n",
    "                         'Number': token.tag.number,\n",
    "                         'Mood': token.tag.mood,\n",
    "                         'Person': token.tag.person,\n",
    "                         }\n",
    "        word['deprel'] = None\n",
    "        res.append(word)\n",
    "        id += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def stanza_to_conllu(token):\n",
    "    res = {}\n",
    "    res['id'] = int(token.id)\n",
    "    res['form'] = token.text\n",
    "    res['lemma'] = token.lemma\n",
    "    res['upostag'] = token.upos\n",
    "    res['feats'] = {k: v for k, v in [\n",
    "        s.split('=') for s in token.feats.split('|')]} if token.feats else None\n",
    "    res['deprel'] = None\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_sent(text, tokenizer):\n",
    "    tokens = tokenizer(text)\n",
    "    relations = dep_parse(tokens, clf['lrc'],\n",
    "                          clf['dict_vect'], feature_extractor)\n",
    "    for ch, head, rel in relations:\n",
    "        print(\n",
    "            '{} <-- {} -- {}'.format(tokens[ch - 1]['form'], rel,\n",
    "                                     tokens[head - 1]['form'] if head > 0 else 'ROOT'))\n",
    "\n",
    "\n",
    "def pym2_tokenizer(text):\n",
    "    text_tokenized = tokenize_text(text)\n",
    "    return pym2_to_conllu([morph.parse(w)[0] for w in text_tokenized])\n",
    "\n",
    "\n",
    "def stanza_tokenizer(text):\n",
    "    sent = nlp(text).sentences[0]\n",
    "    return [stanza_to_conllu(t) for t in sent.words]\n",
    "\n",
    "\n",
    "def parse_text(sents):\n",
    "    for sent in sents:\n",
    "        print(f'Sentence: {sent}\\n')\n",
    "        print('== pymorphy ==\\n')\n",
    "        parse_sent(sent, pym2_tokenizer)\n",
    "        print('\\n')\n",
    "        print('== stanza ==\\n')\n",
    "        parse_sent(sent, stanza_tokenizer)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 20:46:09 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "=======================\n",
      "\n",
      "2020-05-02 20:46:09 INFO: Use device: cpu\n",
      "2020-05-02 20:46:09 INFO: Loading: tokenize\n",
      "2020-05-02 20:46:09 INFO: Loading: pos\n",
      "2020-05-02 20:46:10 INFO: Loading: lemma\n",
      "2020-05-02 20:46:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "nlp = stanza.Pipeline(lang='uk', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Отож ми з ним пiймали в лiсi пугутькало i випустили в клубi пiд час лекцiї ' + \\\n",
    "    'на тему \"Виховання дiтей у сiм\\'ї\"'\n",
    "sent_2 = 'Сінєглазка і Нєзнайка ідуть в просторну уборну, звідки в самом скором врємєні ' + \\\n",
    "    'доносяться противні женські п’яні матюки і звуки ляпасів.'\n",
    "sent_3 = 'Кривавий Пастор гаряче замолився дивною сумішшю нижньонімецької говірки і крепких ' + \\\n",
    "    'механізаторських матюків, а фаршрутка, немов космічна комета, летіла крізь ' + \\\n",
    "    'промислову зливу, що містила у собі місто Чєлябінськ.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробую з варіантом парсеру, який використовує типи залежностей (результат поганий)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Отож ми з ним пiймали в лiсi пугутькало i випустили в клубi пiд час лекцiї на тему \"Виховання дiтей у сiм'ї\"\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "i <-- None -- пугутькало\n",
      "лекцiї <-- nmod -- час\n",
      "дiтей <-- nmod -- виховання\n",
      "у <-- case -- сiм'ї\n",
      "сiм'ї <-- nmod -- дiтей\n",
      "\" <-- punct -- виховання\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "ним <-- nsubj -- пiймали\n",
      "в <-- None -- випустили\n",
      "лiсi <-- None -- випустили\n",
      "пугутькало <-- None -- лiсi\n",
      "випустили <-- xcomp -- пiймали\n",
      "лекцiї <-- nmod -- час\n",
      "на <-- case -- тему\n",
      "дiтей <-- nmod -- Виховання\n",
      "у <-- None -- сiм'ї\n",
      "сiм'ї <-- flat:title -- Виховання\n",
      "\" <-- punct -- Виховання\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Сінєглазка і Нєзнайка ідуть в просторну уборну, звідки в самом скором врємєні доносяться противні женські п’яні матюки і звуки ляпасів.\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "уборну <-- None -- просторну\n",
      "врємєні <-- nmod -- скором\n",
      "противні <-- advcl:sp -- доносяться\n",
      "женські <-- None -- противні\n",
      "п’яні <-- obj -- женські\n",
      "матюки <-- nmod -- п’яні\n",
      "ляпасів <-- nmod -- звуки\n",
      ". <-- punct -- звуки\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "уборну <-- None -- просторну\n",
      "скором <-- obj -- самом\n",
      "врємєні <-- None -- самом\n",
      "доносяться <-- None -- врємєні\n",
      "противні <-- advcl:sp -- доносяться\n",
      "матюки <-- obj -- п’яні\n",
      "ляпасів <-- nmod -- звуки\n",
      ". <-- punct -- звуки\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Кривавий Пастор гаряче замолився дивною сумішшю нижньонімецької говірки і крепких механізаторських матюків, а фаршрутка, немов космічна комета, летіла крізь промислову зливу, що містила у собі місто Чєлябінськ.\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "гаряче <-- amod -- пастор\n",
      "дивною <-- advcl:sp -- замолився\n",
      "сумішшю <-- obj -- дивною\n",
      "нижньонімецької <-- amod -- сумішшю\n",
      "матюків <-- obj -- механізаторських\n",
      "комета <-- obj -- космічна\n",
      "зливу <-- obj -- промислову\n",
      "чєлябінськ <-- nmod -- місто\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "дивною <-- advcl:sp -- замолився\n",
      "сумішшю <-- obj -- дивною\n",
      "нижньонімецької <-- amod -- сумішшю\n",
      "механізаторських <-- amod -- матюків\n",
      "матюків <-- obj -- крепких\n",
      "комета <-- obj -- космічна\n",
      ", <-- punct -- летіла\n",
      "летіла <-- acl -- комета\n",
      "зливу <-- obj -- промислову\n",
      "собі <-- None -- місто\n",
      "Чєлябінськ <-- flat:title -- місто\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents = [sent_1, sent_2, sent_3]\n",
    "parse_text(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробую з фінальним варіантом парсеру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Отож ми з ним пiймали в лiсi пугутькало i випустили в клубi пiд час лекцiї на тему \"Виховання дiтей у сiм'ї\"\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "отож <-- None -- ми\n",
      "ми <-- nsubj -- пiймали\n",
      "з <-- case -- ним\n",
      "ним <-- nsubj -- пiймали\n",
      "пiймали <-- None -- ROOT\n",
      "в <-- None -- лiсi\n",
      "лiсi <-- None -- пiймали\n",
      "пугутькало <-- xcomp -- пiймали\n",
      "i <-- None -- пугутькало\n",
      "випустили <-- xcomp -- пiймали\n",
      "в <-- None -- клубi\n",
      "клубi <-- None -- випустили\n",
      "пiд <-- None -- клубi\n",
      "час <-- obj -- випустили\n",
      "лекцiї <-- obj -- випустили\n",
      "на <-- None -- тему\n",
      "тему <-- nmod -- лекцiї\n",
      "\" <-- punct -- виховання\n",
      "виховання <-- nmod -- лекцiї\n",
      "дiтей <-- obj -- випустили\n",
      "у <-- case -- сiм'ї\n",
      "сiм'ї <-- obj -- випустили\n",
      "\" <-- punct -- пiймали\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "Отож <-- advmod -- пiймали\n",
      "ми <-- nsubj -- пiймали\n",
      "з <-- case -- ним\n",
      "ним <-- nsubj -- пiймали\n",
      "пiймали <-- None -- ROOT\n",
      "в <-- None -- лiсi\n",
      "лiсi <-- None -- пiймали\n",
      "пугутькало <-- xcomp -- пiймали\n",
      "i <-- punct -- пугутькало\n",
      "випустили <-- xcomp -- пiймали\n",
      "в <-- case -- клубi\n",
      "клубi <-- obj -- випустили\n",
      "пiд <-- case -- час\n",
      "час <-- obj -- випустили\n",
      "лекцiї <-- nmod -- час\n",
      "на <-- case -- тему\n",
      "тему <-- nmod -- лекцiї\n",
      "\" <-- punct -- Виховання\n",
      "Виховання <-- nmod -- лекцiї\n",
      "дiтей <-- nmod -- Виховання\n",
      "у <-- None -- сiм'ї\n",
      "сiм'ї <-- flat:title -- час\n",
      "\" <-- punct -- пiймали\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Сінєглазка і Нєзнайка ідуть в просторну уборну, звідки в самом скором врємєні доносяться противні женські п’яні матюки і звуки ляпасів.\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "сінєглазка <-- None -- ROOT\n",
      "і <-- cc -- ідуть\n",
      "нєзнайка <-- obj -- ідуть\n",
      "ідуть <-- acl -- сінєглазка\n",
      "в <-- None -- уборну\n",
      "просторну <-- None -- уборну\n",
      "уборну <-- advcl:sp -- ідуть\n",
      ", <-- punct -- звідки\n",
      "звідки <-- nsubj -- ідуть\n",
      "в <-- case -- скором\n",
      "самом <-- det -- скором\n",
      "скором <-- None -- звідки\n",
      "врємєні <-- nmod -- скором\n",
      "доносяться <-- xcomp -- ідуть\n",
      "противні <-- amod -- п’яні\n",
      "женські <-- amod -- п’яні\n",
      "п’яні <-- obj -- доносяться\n",
      "матюки <-- nmod -- п’яні\n",
      "і <-- cc -- звуки\n",
      "звуки <-- nmod -- матюки\n",
      "ляпасів <-- obj -- доносяться\n",
      ". <-- punct -- сінєглазка\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "Сінєглазка <-- None -- ідуть\n",
      "і <-- cc -- Нєзнайка\n",
      "Нєзнайка <-- obj -- ідуть\n",
      "ідуть <-- None -- ROOT\n",
      "в <-- None -- уборну\n",
      "просторну <-- None -- уборну\n",
      "уборну <-- advcl:sp -- ідуть\n",
      ", <-- punct -- звідки\n",
      "звідки <-- advmod -- ідуть\n",
      "в <-- case -- скором\n",
      "самом <-- amod -- скором\n",
      "скором <-- obj -- врємєні\n",
      "врємєні <-- advcl:sp -- ідуть\n",
      "доносяться <-- xcomp -- ідуть\n",
      "противні <-- advcl:sp -- доносяться\n",
      "женські <-- None -- п’яні\n",
      "п’яні <-- amod -- матюки\n",
      "матюки <-- obj -- противні\n",
      "і <-- cc -- звуки\n",
      "звуки <-- nmod -- ляпасів\n",
      "ляпасів <-- nmod -- матюки\n",
      ". <-- punct -- ідуть\n",
      "\n",
      "\n",
      "\n",
      "Sentence: Кривавий Пастор гаряче замолився дивною сумішшю нижньонімецької говірки і крепких механізаторських матюків, а фаршрутка, немов космічна комета, летіла крізь промислову зливу, що містила у собі місто Чєлябінськ.\n",
      "\n",
      "== pymorphy ==\n",
      "\n",
      "кривавий <-- amod -- пастор\n",
      "пастор <-- obj -- замолився\n",
      "гаряче <-- advcl:sp -- замолився\n",
      "замолився <-- None -- ROOT\n",
      "дивною <-- amod -- сумішшю\n",
      "сумішшю <-- obj -- замолився\n",
      "нижньонімецької <-- amod -- говірки\n",
      "говірки <-- nmod -- сумішшю\n",
      "і <-- cc -- матюків\n",
      "крепких <-- amod -- матюків\n",
      "механізаторських <-- amod -- матюків\n",
      "матюків <-- nmod -- сумішшю\n",
      ", <-- punct -- фаршрутка\n",
      "а <-- cc -- фаршрутка\n",
      "фаршрутка <-- nmod -- сумішшю\n",
      ", <-- punct -- комета\n",
      "немов <-- None -- комета\n",
      "космічна <-- amod -- комета\n",
      "комета <-- nmod -- сумішшю\n",
      ", <-- punct -- летіла\n",
      "летіла <-- xcomp -- замолився\n",
      "крізь <-- case -- зливу\n",
      "промислову <-- amod -- зливу\n",
      "зливу <-- obj -- летіла\n",
      ", <-- punct -- містила\n",
      "що <-- None -- містила\n",
      "містила <-- xcomp -- летіла\n",
      "у <-- case -- собі\n",
      "собі <-- obj -- містила\n",
      "місто <-- nmod -- собі\n",
      "чєлябінськ <-- obj -- містила\n",
      ". <-- punct -- замолився\n",
      "\n",
      "\n",
      "== stanza ==\n",
      "\n",
      "Кривавий <-- amod -- Пастор\n",
      "Пастор <-- obj -- замолився\n",
      "гаряче <-- advmod -- замолився\n",
      "замолився <-- None -- ROOT\n",
      "дивною <-- advcl:sp -- замолився\n",
      "сумішшю <-- obj -- дивною\n",
      "нижньонімецької <-- amod -- говірки\n",
      "говірки <-- nmod -- сумішшю\n",
      "і <-- cc -- матюків\n",
      "крепких <-- amod -- матюків\n",
      "механізаторських <-- amod -- матюків\n",
      "матюків <-- nmod -- сумішшю\n",
      ", <-- punct -- фаршрутка\n",
      "а <-- cc -- фаршрутка\n",
      "фаршрутка <-- obj -- замолився\n",
      ", <-- punct -- комета\n",
      "немов <-- mark -- комета\n",
      "космічна <-- amod -- комета\n",
      "комета <-- nmod -- фаршрутка\n",
      ", <-- punct -- летіла\n",
      "летіла <-- xcomp -- замолився\n",
      "крізь <-- case -- зливу\n",
      "промислову <-- amod -- зливу\n",
      "зливу <-- obj -- летіла\n",
      ", <-- punct -- містила\n",
      "що <-- None -- містила\n",
      "містила <-- xcomp -- летіла\n",
      "у <-- case -- собі\n",
      "собі <-- nsubj -- містила\n",
      "місто <-- obj -- містила\n",
      "Чєлябінськ <-- flat:title -- місто\n",
      ". <-- punct -- замолився\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents = [sent_1, sent_2, sent_3]\n",
    "parse_text(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Порівняння (pymorphy2 <-> stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Отож ми з ним пiймали в лiсi пугутькало i випустили в клубi пiд час лекцiї на тему \"Виховання дiтей у сiм'ї\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s1](./sent_1_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Сінєглазка і Нєзнайка ідуть в просторну уборну, звідки в самом скором врємєні доносяться противні женські п’яні матюки і звуки ляпасів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s2](./sent_2_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Кривавий Пастор гаряче замолився дивною сумішшю нижньонімецької говірки і крепких механізаторських матюків, а фаршрутка, немов космічна комета, летіла крізь промислову зливу, що містила у собі місто Чєлябінськ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s3](./sent_3_diff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
