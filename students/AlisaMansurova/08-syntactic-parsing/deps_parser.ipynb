{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tokenize_uk\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../UD_Ukrainian-IU'\n",
    "\n",
    "with open(PATH + '/uk_iu-ud-train.conllu') as f:\n",
    "    train_data = f.read()\n",
    "    \n",
    "with open(PATH + '/uk_iu-ud-dev.conllu') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "train_trees = parse(train_data)\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<Це, одна, з, надзвичайно, важливих, сцен, у, драмі, Лесі, Українки, «, Руфін, і, Прісцілла, », .>\n",
      "=====\n",
      "1: Це <-- nsubj -- одна\n",
      "2: одна <-- root -- root\n",
      "3: з <-- case -- сцен\n",
      "4: надзвичайно <-- advmod -- важливих\n",
      "5: важливих <-- amod -- сцен\n",
      "6: сцен <-- nmod -- одна\n",
      "7: у <-- case -- драмі\n",
      "8: драмі <-- nmod -- сцен\n",
      "9: Лесі <-- nmod -- драмі\n",
      "10: Українки <-- flat:name -- Лесі\n",
      "11: « <-- punct -- Руфін\n",
      "12: Руфін <-- flat:title -- драмі\n",
      "13: і <-- cc -- Прісцілла\n",
      "14: Прісцілла <-- conj -- Руфін\n",
      "15: » <-- punct -- Руфін\n",
      "16: . <-- punct -- одна\n"
     ]
    }
   ],
   "source": [
    "tree = train_trees[3]\n",
    "print(tree)\n",
    "print('=====')\n",
    "for node in tree:\n",
    "    head = node['head']\n",
    "    print(f\"{node['id']}: {node['form']} <-- {node['deprel']} -- {tree[head - 1]['form'] if head > 0 else 'root'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> node Це\n",
      ">> path 2_0\n",
      ">> node одна\n",
      ">> path 0\n",
      ">> node з\n",
      ">> path 6_2_0\n",
      ">> node надзвичайно\n",
      ">> path 5_6_2_0\n",
      ">> node важливих\n",
      ">> path 6_2_0\n",
      ">> node сцен\n",
      ">> path 2_0\n",
      ">> node у\n",
      ">> path 8_6_2_0\n",
      ">> node драмі\n",
      ">> path 6_2_0\n",
      ">> node Лесі\n",
      ">> path 8_6_2_0\n",
      ">> node Українки\n",
      ">> path 9_8_6_2_0\n",
      ">> node «\n",
      ">> path 12_8_6_2_0\n",
      ">> node Руфін\n",
      ">> path 8_6_2_0\n",
      ">> node і\n",
      ">> path 14_12_8_6_2_0\n",
      ">> node Прісцілла\n",
      ">> path 12_8_6_2_0\n",
      ">> node »\n",
      ">> path 12_8_6_2_0\n",
      ">> node .\n",
      ">> path 2_0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2_0',\n",
       " '0',\n",
       " '6_2_0',\n",
       " '5_6_2_0',\n",
       " '6_2_0',\n",
       " '2_0',\n",
       " '8_6_2_0',\n",
       " '6_2_0',\n",
       " '8_6_2_0',\n",
       " '9_8_6_2_0',\n",
       " '12_8_6_2_0',\n",
       " '8_6_2_0',\n",
       " '14_12_8_6_2_0',\n",
       " '12_8_6_2_0',\n",
       " '12_8_6_2_0',\n",
       " '2_0']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_path_to_root(node, tree):\n",
    "    h = node['head']\n",
    "#     print('>>> h', tree[h - 1]['form'], h)\n",
    "    if h:\n",
    "        return f'{h}_{get_path_to_root(tree[h - 1], tree)}'\n",
    "    return str(h)\n",
    "\n",
    "def get_path_to_root_tree(tree):\n",
    "    res = []\n",
    "    for node in tree:\n",
    "        print('>> node', node['form'])\n",
    "        print('>> path', get_path_to_root(node, tree))\n",
    "        res.append(get_path_to_root(node, tree))\n",
    "    return res\n",
    "\n",
    "get_path_to_root_tree(train_trees[3])\n",
    "# print(train_trees[0][2])\n",
    "# print(train_trees[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(str, Enum):\n",
    "    SHIFT = \"shift\"\n",
    "    REDUCE = \"reduce\"\n",
    "    RIGHT = \"right\"\n",
    "    LEFT = \"left\"\n",
    "    \n",
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_ldep_rdep(id, relations):\n",
    "    left = 100500\n",
    "    right = -1\n",
    "    ldep = 'NONE'\n",
    "    rdep = 'NONE'\n",
    "    for (ch, head, rel) in relations:\n",
    "        if head == id:\n",
    "            if (ch < head) and (ch < left):\n",
    "                left = ch\n",
    "                ldep = rel\n",
    "            if (ch > head) and (ch > right):\n",
    "                right = ch\n",
    "                rdep = rel\n",
    "    return ldep, rdep\n",
    "\n",
    "\n",
    "def get_path_to_root(id, relations):\n",
    "    curr_ch = id\n",
    "    steps = 0\n",
    "    rels_sorted = sorted(relations, key=lambda x: x[0] == id, reverse=True)\n",
    "    for (ch, head, rel) in rels_sorted:\n",
    "        if curr_ch == ch:\n",
    "            steps =+ 1\n",
    "            curr_ch = head\n",
    "    return steps\n",
    "\n",
    "\n",
    "def feature_extractor_base(stack, queue, _):\n",
    "    print('%%%%%%%%%%%%%%%')\n",
    "    feat = {}\n",
    "    \n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-word'] = top_stack['form']\n",
    "        feat['s0-lemma'] = top_stack['lemma']\n",
    "        feat['s0-pos'] = top_stack['upostag']\n",
    "    if (len(stack)) > 1:\n",
    "        feat['s1-pos'] = stack[-2]['upostag']\n",
    "    if queue:\n",
    "        print('---', queue)\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-word'] = top_queue['form']\n",
    "        feat['q0-lemma'] = top_queue['lemma']\n",
    "        feat['q0-pos'] = top_queue['upostag']\n",
    "    if (len(queue)) > 1:\n",
    "        q_next = queue[1]\n",
    "        feat['q1-word'] = q_next['form']\n",
    "        feat['q1-pos'] = q_next['upostag']\n",
    "    if (len(queue)) > 2:\n",
    "        feat['q2-pos'] = queue[2]['upostag']\n",
    "    if (len(queue)) > 3:\n",
    "        feat['q3-pos'] = queue[3]['upostag']\n",
    "        \n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_feats(stack, queue, _):\n",
    "    def get_feats(token):\n",
    "        token_feats = token['feats']\n",
    "        return ';'.join([f'{k}={v}' for k, v in token_feats.items()]) if token_feats else 'NONE'\n",
    "        \n",
    "    feat = {}\n",
    "    \n",
    "    if stack:\n",
    "        feat['s0-feats'] = get_feats(stack[-1])\n",
    "    if (len(stack)) > 1:\n",
    "        feat['s1-feats'] = get_feats(stack[-2])\n",
    "    if queue:\n",
    "        feat['q0-feats'] = get_feats(queue[0])\n",
    "    if (len(queue)) > 1:\n",
    "        feat['q1-feats'] = get_feats(queue[1])\n",
    "        \n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_deprels(stack, queue, relations):\n",
    "    feat = {}\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-deprel'] = top_stack['deprel'] or 'NONE'\n",
    "        ldep, rdep = get_ldep_rdep(top_stack['id'], relations)\n",
    "        feat['s0-ldep'] = ldep\n",
    "        feat['s0-rdep'] = rdep\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-deprel'] = top_queue['deprel'] or 'NONE'\n",
    "        ldep, rdep = get_ldep_rdep(top_queue['id'], relations)\n",
    "        feat['q0-ldep'] = ldep\n",
    "        feat['q0-rdep'] = rdep\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def feature_extractor_path_to_root(stack, queue, relations):\n",
    "    feat = {}\n",
    "    if stack:\n",
    "        top_stack = stack[-1]\n",
    "        feat['s0-path-root'] = get_path_to_root(top_stack['id'], relations)\n",
    "    if queue:\n",
    "        top_queue = queue[0]\n",
    "        feat['q0-path-root'] = get_path_to_root(top_queue['id'], relations)\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "def oracle(stack, top_queue, relations):\n",
    "    \"\"\"\n",
    "    Make a decision on the right action to do.\n",
    "    \"\"\"\n",
    "    top_stack = stack[-1]\n",
    "    # check if both stack and queue are non-empty\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    # check if there are any clear dependencies\n",
    "    elif top_queue[\"head\"] == top_stack[\"id\"]:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack[\"head\"] == top_queue[\"id\"]:\n",
    "        return Actions.LEFT\n",
    "    # check if we can reduce the top of the stack\n",
    "    elif top_stack[\"id\"] in [i[0] for i in relations] and \\\n",
    "         (top_queue[\"head\"] < top_stack[\"id\"] or \\\n",
    "          [s for s in stack if s[\"head\"] == top_queue[\"id\"]]):\n",
    "        return Actions.REDUCE\n",
    "    # default option\n",
    "    else:\n",
    "        return Actions.SHIFT\n",
    "\n",
    "\n",
    "\n",
    "def get_data(tree, feature_extractor):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    \n",
    "    while queue or stack:\n",
    "#         if stack and not queue:\n",
    "#             stack.pop()\n",
    "#         else:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                            queue[0] if len(queue) > 0 else None,\n",
    "                            relations)\n",
    "        features.append(feature_extractor(stack, queue, relations))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            rel = (stack[-1][\"id\"], queue[0][\"id\"], stack[-1][\"deprel\"])\n",
    "            relations.append(rel)\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            rel = (queue[0]['id'], stack[-1][\"id\"], queue[0][\"deprel\"])\n",
    "            relations.append(rel)\n",
    "            stack.append(queue.pop(0))\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def dep_parse(tree, clf, vectorizer, feature_extractor):\n",
    "    print('**')\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    \n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = feature_extractor(stack, queue, relations)\n",
    "#             print('^^ feat', len(features))\n",
    "            action = clf.predict(vectorizer.transform([features]))[0]\n",
    "#             action = clf.predict(features)\n",
    "    \n",
    "#             print('^^^ act', action)\n",
    "            \n",
    "            if action == Actions.SHIFT:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == Actions.REDUCE:\n",
    "                stack.pop()\n",
    "            elif action == Actions.LEFT:\n",
    "                rel = (stack[-1][\"id\"], queue[0][\"id\"], stack[-1][\"deprel\"])\n",
    "                relations.append(rel)\n",
    "                stack.pop()\n",
    "            elif action == Actions.RIGHT:\n",
    "                rel = (queue[0]['id'], stack[-1][\"id\"], queue[0][\"deprel\"])\n",
    "                relations.append(rel)\n",
    "                stack.append(queue.pop(0))\n",
    "\n",
    "    return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# TODO: unify funcs\n",
    "def get_train_data(trees, feature_extractor):\n",
    "    train_feat, train_lab = [], []\n",
    "    for tree in trees:\n",
    "        t_f, t_l = get_data([t for t in tree if type(t['id']) == int], feature_extractor)\n",
    "        train_feat += t_f\n",
    "        train_lab += t_l\n",
    "    return train_feat, train_lab\n",
    "\n",
    "\n",
    "def get_test_data(trees, feature_extractor):\n",
    "    test_feat, test_lab = [], []\n",
    "    for tree in trees:\n",
    "        t_f, t_l = get_data([t for t in tree if type(t['id']) == int], feature_extractor)\n",
    "        test_feat += t_f\n",
    "        test_lab += t_l\n",
    "    return test_feat, test_lab\n",
    "\n",
    "\n",
    "def calculate_as(trees, clf, vect, feature_extractor):\n",
    "    total, tpu, tpl = 0, 0, 0\n",
    "    golden_u, golden_l = None, None\n",
    "    for tree in trees:\n",
    "        tree = [t for t in tree if type(t['id']) == int]\n",
    "        golden_all = [(node['id'], node['head'], node['deprel']) for node in tree]\n",
    "        golden_u = [(x, y) for x, y, _ in golden_all]\n",
    "\n",
    "        predicted_all = dep_parse(tree, clf, vect, feature_extractor)\n",
    "        predicted_u = [(x, y) for x, y, _ in predicted_all]\n",
    "        \n",
    "        total += len(tree)\n",
    "        tpu += len(set(golden_u).intersection(set(predicted_u)))\n",
    "        tpl += len(set(golden_all).intersection(set(predicted_all)))\n",
    "\n",
    "    print('Total: ', total)\n",
    "    print('Match unlabeled: ', tpu)\n",
    "#     print('Match labeled: ', tpl)\n",
    "    print('UAS: ', round(tpu/total, 2))\n",
    "#     print('LAS: ', round(tpl/total, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = test_trees[1]\n",
    "print(tree)\n",
    "feats, labels = get_data([t for t in tree if type(t['id']) == int], feature_extractor_deps)\n",
    "vect.fit(feats)\n",
    "clf.fit(vect.transform(feats), labels)\n",
    "dep_parse(test_trees[1], clf, vect, feature_extractor_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "#                         max_iter=100, solver='sag', n_jobs=20, verbose=1)\n",
    "\n",
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = feature_extractor_base\n",
    "\n",
    "train_feat, train_lab = get_train_data(train_trees, feature_extractor)\n",
    "test_feat, test_lab = get_train_data(test_trees, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=20)]: Done   1 out of   1 | elapsed:   20.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=20, penalty='l2',\n",
       "                   random_state=42, solver='sag', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(train_feat)\n",
    "clf.fit(vect.transform(train_feat), train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.86      0.87      0.86      6371\n",
      "      reduce       0.85      0.78      0.81      6875\n",
      "       right       0.75      0.79      0.77      5996\n",
      "       shift       0.85      0.87      0.86      6578\n",
      "\n",
      "    accuracy                           0.83     25820\n",
      "   macro avg       0.83      0.83      0.83     25820\n",
      "weighted avg       0.83      0.83      0.83     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, clf.predict(vect.transform(test_feat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  12574\n",
      "Match unlabeled:  8717\n",
      "UAS:  0.69\n"
     ]
    }
   ],
   "source": [
    "calculate_as(test_trees, clf, vect, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With token features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats)\n",
    "\n",
    "train_feat, train_lab = get_train_data(train_trees, feature_extractor)\n",
    "test_feat, test_lab = get_train_data(test_trees, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend ThreadingBackend with 20 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=20)]: Done   1 out of   1 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=20, penalty='l2',\n",
       "                   random_state=42, solver='sag', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(train_feat)\n",
    "train_feat_vectorized = vect.transform(train_feat)\n",
    "clf.fit(train_feat_vectorized, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.87      0.89      0.88      6371\n",
      "      reduce       0.86      0.81      0.83      6875\n",
      "       right       0.78      0.80      0.79      5996\n",
      "       shift       0.87      0.88      0.87      6578\n",
      "\n",
      "    accuracy                           0.85     25820\n",
      "   macro avg       0.85      0.85      0.85     25820\n",
      "weighted avg       0.85      0.85      0.85     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, clf.predict(vect.transform(test_feat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  12574\n",
      "Match unlabeled:  9089\n",
      "UAS:  0.72\n"
     ]
    }
   ],
   "source": [
    "calculate_as(test_trees, clf, vect, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With deprels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats, feature_extractor_deprels)\n",
    "\n",
    "train_feat, train_lab = get_train_data(train_trees, feature_extractor)\n",
    "test_feat, test_lab = get_test_data(test_trees, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=20,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_feat, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.94      0.96      0.95      6371\n",
      "      reduce       0.92      0.87      0.90      6875\n",
      "       right       0.89      0.91      0.90      5996\n",
      "       shift       0.93      0.94      0.94      6578\n",
      "\n",
      "    accuracy                           0.92     25820\n",
      "   macro avg       0.92      0.92      0.92     25820\n",
      "weighted avg       0.92      0.92      0.92     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, clf.predict(test_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  12574\n",
      "Match unlabeled:  10495\n",
      "UAS:  0.83\n"
     ]
    }
   ],
   "source": [
    "calculate_as(test_trees, clf['lrc'], clf['dict_vect'], feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With path to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_feats, feature_extractor_deprels, feature_extractor_path_to_root)\n",
    "\n",
    "train_feat, train_lab = get_train_data(train_trees, feature_extractor)\n",
    "test_feat, test_lab = get_test_data(test_trees, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dict_vect',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=True)),\n",
       "                ('lrc',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='multinomial', n_jobs=20,\n",
       "                                    penalty='l2', random_state=42, solver='sag',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_feat, train_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.97      0.98      0.97      6371\n",
      "      reduce       0.94      0.91      0.92      6875\n",
      "       right       0.89      0.91      0.90      5996\n",
      "       shift       0.94      0.95      0.94      6578\n",
      "\n",
      "    accuracy                           0.94     25820\n",
      "   macro avg       0.94      0.94      0.94     25820\n",
      "weighted avg       0.94      0.94      0.94     25820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_lab, clf.predict(test_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  12574\n",
      "Match unlabeled:  10578\n",
      "UAS:  0.84\n"
     ]
    }
   ],
   "source": [
    "calculate_as(test_trees, clf['lrc'], clf['dict_vect'], feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_f.json', 'w') as f:\n",
    "    json.dump(train_feat, f)\n",
    "print(train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test_f.json', 'w') as f:\n",
    "    json.dump(test_feat, f)\n",
    "print(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0, None), (1, 6, 'nsubj'), (2, 3, 'cc'), (3, 1, 'nsubj'), (4, 5, 'det'), (5, 3, 'conj'), (7, 6, 'root'), (8, 7, 'obj'), (9, 11, 'punct'), (10, 11, 'mark'), (11, 6, 'root'), (12, 11, 'acl:relcl'), (13, 12, 'xcomp'), (14, 15, 'case'), (15, 13, 'xcomp:sp'), (16, 12, 'xcomp'), (17, 16, 'nmod'), (18, 20, 'punct'), (19, 20, 'amod'), (20, 12, 'xcomp'), (21, 22, 'cc'), (22, 20, 'conj'), (23, 6, 'root')]\n"
     ]
    }
   ],
   "source": [
    "arcs = [(1, 6, 'nsubj'), (2, 3, 'cc'), (3, 1, 'nsubj'), (4, 5, 'det'),\n",
    "        (5, 3, 'conj'), (6, 0, None), (7, 6, 'root'), (8, 7, 'obj'),\n",
    "        (9, 11, 'punct'), (10, 11, 'mark'), (11, 6, 'root'), (12, 11, 'acl:relcl'),\n",
    "        (13, 12, 'xcomp'), (14, 15, 'case'), (15, 13, 'xcomp:sp'), (16, 12, 'xcomp'),\n",
    "        (17, 16, 'nmod'), (18, 20, 'punct'), (19, 20, 'amod'), (20, 12, 'xcomp'),\n",
    "        (21, 22, 'cc'), (22, 20, 'conj'), (23, 6, 'root')]\n",
    "\n",
    "rels_sorted = sorted(arcs, key=lambda x: x[0] == 6, reverse=True)\n",
    "print(rels_sorted)\n",
    "# get_path_to_root_1(6, arcs)\n",
    "# print(find_left_right_dependencies(6, arcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N nsubj\n"
     ]
    }
   ],
   "source": [
    "left_most = 1000000\n",
    "right_most = -1\n",
    "dep_right_most = 'N'\n",
    "dep_left_most = 'N'\n",
    "wi, r, wj = (6, 'nsubj', 1)\n",
    "if wi == 6:\n",
    "    if (wj > wi) and (wj > right_most):\n",
    "        right_most = wj\n",
    "        dep_right_most = r\n",
    "    if (wj < wi) and (wj < left_most):\n",
    "        left_most = wj\n",
    "        dep_left_most = r\n",
    "print(dep_right_most, dep_left_most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Use parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "\n",
    "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
    "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
    "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
    "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
    "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
    "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
    "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
    "\n",
    "PREP = [\"до\", \"на\"]\n",
    "\n",
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if \"coord\" in word.tag:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    elif \"PNCT\" in word.tag:\n",
    "        return \"PUNCT\"\n",
    "    elif word.normal_form in PREP:\n",
    "        return \"PREP\"\n",
    "    elif word.normal_form in DET:\n",
    "        return \"DET\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'Отож ми з ним пiймали в лiсi пугутькало i випустили в клубi пiд час лекцiї на тему \"Виховання дiтей у сiм\\'ї\". \\\n",
    "    Лектор упав з трибуни i вилив собi на голову графин з водою.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pym2_to_conllu(tokens):\n",
    "#     id = 0\n",
    "#     res = {}\n",
    "#     for token in tokens:\n",
    "# #         print('**', token)\n",
    "#         res['id'] = id\n",
    "#         res['form'] = token.word\n",
    "#         res['lemma'] = token.normal_form\n",
    "#         res['upostag'] = token.tag\n",
    "#         res['feats'] = ''\n",
    "#         res['deprels'] = ''\n",
    "#         id += 1\n",
    "#     return res\n",
    "\n",
    "def pym2_to_conllu(i, token):\n",
    "    res = {}\n",
    "    res['id'] = i\n",
    "    res['form'] = token.word\n",
    "    res['lemma'] = token.normal_form\n",
    "    res['upostag'] = token.tag\n",
    "    res['feats'] = ''\n",
    "    res['deprel'] = ''\n",
    "    return res\n",
    "\n",
    "# ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "#                     ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "#                     ('deps', None), ('misc', None)])\n",
    "text_tokenized = tokenize_text(sent_1)\n",
    "# print(text_tokenized)\n",
    "# for w in text_tokenized:\n",
    "#     print(morph.parse(w))\n",
    "#     print('===')\n",
    "    \n",
    "    \n",
    "def dep_parse_text(text, clf, vect, feature_extractor):\n",
    "    res = []\n",
    "    for sent in text:\n",
    "        res.append(dep_parse(sent, clf, vect, feature_extractor))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'form': 'отож', 'lemma': 'отож', 'upostag': OpencorporaTag('PRCL'), 'feats': '', 'deprel': ''}, {'id': 1, 'form': 'ми', 'lemma': 'ми', 'upostag': OpencorporaTag('NPRO,pers,plur,anim nomn'), 'feats': '', 'deprel': ''}, {'id': 2, 'form': 'з', 'lemma': 'з', 'upostag': OpencorporaTag('PREP,rv_ablt,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 3, 'form': 'ним', 'lemma': 'він', 'upostag': OpencorporaTag('NPRO,pers,masc ablt'), 'feats': '', 'deprel': ''}, {'id': 4, 'form': 'пiймали', 'lemma': 'пiймати', 'upostag': OpencorporaTag('VERB,impf plur,past'), 'feats': '', 'deprel': ''}, {'id': 5, 'form': 'в', 'lemma': 'в', 'upostag': OpencorporaTag('PREP,v-u,rv_loct,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 6, 'form': 'лiсi', 'lemma': 'лiсi', 'upostag': OpencorporaTag('UNKN'), 'feats': '', 'deprel': ''}, {'id': 7, 'form': 'пугутькало', 'lemma': 'пугутькати', 'upostag': OpencorporaTag('VERB,perf neut,past'), 'feats': '', 'deprel': ''}, {'id': 8, 'form': 'i', 'lemma': 'i', 'upostag': OpencorporaTag('ROMN'), 'feats': '', 'deprel': ''}, {'id': 9, 'form': 'випустили', 'lemma': 'випустити', 'upostag': OpencorporaTag('VERB,perf plur,past'), 'feats': '', 'deprel': ''}, {'id': 10, 'form': 'в', 'lemma': 'в', 'upostag': OpencorporaTag('PREP,v-u,rv_loct,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 11, 'form': 'клубi', 'lemma': 'клубi', 'upostag': OpencorporaTag('UNKN'), 'feats': '', 'deprel': ''}, {'id': 12, 'form': 'пiд', 'lemma': 'пiд', 'upostag': OpencorporaTag('UNKN'), 'feats': '', 'deprel': ''}, {'id': 13, 'form': 'час', 'lemma': 'час', 'upostag': OpencorporaTag('NOUN,inan masc,nomn'), 'feats': '', 'deprel': ''}, {'id': 14, 'form': 'лекцiї', 'lemma': 'лекцiя', 'upostag': OpencorporaTag('NOUN,femn,inan gent'), 'feats': '', 'deprel': ''}, {'id': 15, 'form': 'на', 'lemma': 'на', 'upostag': OpencorporaTag('INTJ'), 'feats': '', 'deprel': ''}, {'id': 16, 'form': 'тему', 'lemma': 'тема', 'upostag': OpencorporaTag('NOUN,inan femn,accs'), 'feats': '', 'deprel': ''}, {'id': 17, 'form': '\"', 'lemma': '\"', 'upostag': OpencorporaTag('PNCT'), 'feats': '', 'deprel': ''}, {'id': 18, 'form': 'виховання', 'lemma': 'виховання', 'upostag': OpencorporaTag('NOUN,inan neut,nomn'), 'feats': '', 'deprel': ''}, {'id': 19, 'form': 'дiтей', 'lemma': 'дiть', 'upostag': OpencorporaTag('NOUN,inan plur,gent'), 'feats': '', 'deprel': ''}, {'id': 20, 'form': 'у', 'lemma': 'у', 'upostag': OpencorporaTag('PREP,v-u,rv_loct,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 21, 'form': \"сiм'ї\", 'lemma': \"сiм'я\", 'upostag': OpencorporaTag('NOUN,neut,inan loct'), 'feats': '', 'deprel': ''}, {'id': 22, 'form': '\"', 'lemma': '\"', 'upostag': OpencorporaTag('PNCT'), 'feats': '', 'deprel': ''}, {'id': 23, 'form': '.', 'lemma': '.', 'upostag': OpencorporaTag('PNCT'), 'feats': '', 'deprel': ''}, {'id': 24, 'form': 'лектор', 'lemma': 'лектор', 'upostag': OpencorporaTag('NOUN,anim masc,nomn'), 'feats': '', 'deprel': ''}, {'id': 25, 'form': 'упав', 'lemma': 'упасти', 'upostag': OpencorporaTag('VERB,v-u,perf masc,past'), 'feats': '', 'deprel': ''}, {'id': 26, 'form': 'з', 'lemma': 'з', 'upostag': OpencorporaTag('PREP,rv_ablt,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 27, 'form': 'трибуни', 'lemma': 'трибун', 'upostag': OpencorporaTag('NOUN,anim plur,nomn'), 'feats': '', 'deprel': ''}, {'id': 28, 'form': 'i', 'lemma': 'i', 'upostag': OpencorporaTag('ROMN'), 'feats': '', 'deprel': ''}, {'id': 29, 'form': 'вилив', 'lemma': 'вилити', 'upostag': OpencorporaTag('VERB,perf masc,past'), 'feats': '', 'deprel': ''}, {'id': 30, 'form': 'собi', 'lemma': 'собi', 'upostag': OpencorporaTag('UNKN'), 'feats': '', 'deprel': ''}, {'id': 31, 'form': 'на', 'lemma': 'на', 'upostag': OpencorporaTag('INTJ'), 'feats': '', 'deprel': ''}, {'id': 32, 'form': 'голову', 'lemma': 'голова', 'upostag': OpencorporaTag('NOUN,inan femn,accs'), 'feats': '', 'deprel': ''}, {'id': 33, 'form': 'графин', 'lemma': 'графин', 'upostag': OpencorporaTag('NOUN,inan masc,nomn'), 'feats': '', 'deprel': ''}, {'id': 34, 'form': 'з', 'lemma': 'з', 'upostag': OpencorporaTag('PREP,rv_ablt,rv_gent,rv_accs'), 'feats': '', 'deprel': ''}, {'id': 35, 'form': 'водою', 'lemma': 'вода', 'upostag': OpencorporaTag('NOUN,inan femn,ablt'), 'feats': '', 'deprel': ''}, {'id': 36, 'form': '.', 'lemma': '.', 'upostag': OpencorporaTag('PNCT'), 'feats': '', 'deprel': ''}]\n",
      "**\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "items not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-653-063b98acb31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpym2_to_conllu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_converted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredicted_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdep_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_converted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-645-73abb49c8914>\u001b[0m in \u001b[0;36mdep_parse\u001b[0;34m(tree, clf, vectorizer, feature_extractor)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m#             print('^^ feat', len(features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;31m#             action = clf.predict(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/_dict_vectorizer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/_dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: items not found"
     ]
    }
   ],
   "source": [
    "text_converted = [pym2_to_conllu(i, morph.parse(x)[0]) for i, x in enumerate(text_tokenized)]\n",
    "print(text_converted)\n",
    "predicted_all = dep_parse(text_converted, clf, vect, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
