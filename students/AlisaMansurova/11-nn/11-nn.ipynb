{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import stanza\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from conllu import parse\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "import progressbar\n",
    "import tensorflow as tf\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../UD_Ukrainian-IU'\n",
    "\n",
    "with open(PATH + '/uk_iu-ud-train.conllu') as f:\n",
    "    train_data = f.read()\n",
    "    \n",
    "with open(PATH + '/uk_iu-ud-dev.conllu') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "train_trees = parse(train_data)\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/Users/lissm/Work/Dev/NLP/corpora/vectors/ubercorpus_lowercased_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Actions(str, Enum):\n",
    "#     SHIFT = 'shift'\n",
    "#     REDUCE = 'reduce'\n",
    "#     RIGHT = 'right'\n",
    "#     LEFT = 'left'\n",
    "    \n",
    "class Actions(int, Enum):\n",
    "    SHIFT = 0\n",
    "    REDUCE = 1\n",
    "    RIGHT = 2\n",
    "    LEFT = 3\n",
    "\n",
    "\n",
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats',\n",
    "                                        None), ('head', None),  ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "\n",
    "# def dep_parse(tree, clf, vectorizer, feature_extractor):\n",
    "#     stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "#     while queue or stack:\n",
    "#         if stack and not queue:\n",
    "#             stack.pop()\n",
    "#         else:\n",
    "#             features = feature_extractor(stack, queue, relations)\n",
    "#             action = clf.predict(vectorizer.transform([features]))[0]\n",
    "\n",
    "#             if action == Actions.SHIFT:\n",
    "#                 stack.append(queue.pop(0))\n",
    "#             elif action == Actions.REDUCE:\n",
    "#                 stack.pop()\n",
    "#             elif action == Actions.LEFT:\n",
    "#                 deprel = stack[-1]['deprel'] or predict_deprel(\n",
    "#                     stack[-1], queue[0], stack + queue)\n",
    "#                 rel = (stack[-1]['id'], queue[0]['id'], deprel)\n",
    "#                 relations.append(rel)\n",
    "#                 stack.pop()\n",
    "#             elif action == Actions.RIGHT:\n",
    "#                 deprel = queue[0]['deprel'] or predict_deprel(\n",
    "#                     queue[0], stack[-1], stack + queue)\n",
    "#                 rel = (queue[0]['id'], stack[-1]['id'], deprel)\n",
    "#                 relations.append(rel)\n",
    "#                 stack.append(queue.pop(0))\n",
    "\n",
    "#     return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data & reporting utils \"\"\"\n",
    "\n",
    "\n",
    "def calculate_as(trees, clf, vect, feature_extractor):\n",
    "    total, tpu, tpl, full_match = 0, 0, 0, 0\n",
    "    golden_u, golden_l = None, None\n",
    "    for tree in trees:\n",
    "        tree = [t for t in tree if type(t['id']) == int]\n",
    "        golden_all = [(node['id'], node['head'], node['deprel'])\n",
    "                      for node in tree]\n",
    "        golden_u = [(x, y) for x, y, _ in golden_all]\n",
    "\n",
    "        predicted_all = dep_parse(tree, clf, vect, feature_extractor)\n",
    "        predicted_u = [(x, y) for x, y, _ in predicted_all]\n",
    "\n",
    "        total += len(tree)\n",
    "        tpu += len(set(golden_u).intersection(set(predicted_u)))\n",
    "        tpl += len(set(golden_all).intersection(set(predicted_all)))\n",
    "\n",
    "        if set(golden_all) == set(predicted_all):\n",
    "            full_match += 1\n",
    "\n",
    "    print('== Attachment score report ==')\n",
    "    print('Total: ', total)\n",
    "    print('Match unlabeled: ', tpu)\n",
    "#     print('Match labeled: ', tpl)\n",
    "    print('UAS: ', round(tpu/total, 2))\n",
    "#     print('LAS: ', round(tpl/total, 2))\n",
    "    print(\"Full match:\", round(full_match/len(trees), 2))\n",
    "\n",
    "\n",
    "def get_lrc_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def print_result(train_trees, test_trees, clf, feature_extractor):\n",
    "    train_feat, train_lab = get_data(train_trees, feature_extractor)\n",
    "    test_feat, test_lab = get_data(test_trees, feature_extractor)\n",
    "\n",
    "    clf.fit(train_feat, train_lab)\n",
    "    print(classification_report(test_lab, clf.predict(test_feat)))\n",
    "    calculate_as(test_trees, clf['lrc'], clf['dict_vect'], feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_model(trees):\n",
    "    words = []\n",
    "    for tree in trees:\n",
    "        words.append([x['lemma'] for x in tree])\n",
    "    model = Word2Vec(\n",
    "        words, size=300, min_count=1, workers=6,\n",
    "        window=5, alpha=0.030, negative=7, iter=5\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_pos_tag_map(trees):\n",
    "    pos_map = {'ROOT': 0, 'UNK': 1}\n",
    "    cnt = 1\n",
    "\n",
    "    for tree in trees:\n",
    "        for tok in tree:\n",
    "            if not pos_map.get(tok['upostag']):\n",
    "                pos_map[tok['upostag']] = cnt\n",
    "                cnt += 1\n",
    "    return pos_map\n",
    "    \n",
    "\n",
    "def oracle(stack, top_queue, relations):\n",
    "    top_stack = stack[-1]\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    elif top_queue['head'] == top_stack['id']:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack['head'] == top_queue['id']:\n",
    "        return Actions.LEFT\n",
    "    elif top_stack['id'] in [i[0] for i in relations] and \\\n",
    "        (top_queue['head'] < top_stack['id'] or\n",
    "         [s for s in stack if s['head'] == top_queue['id']]):\n",
    "        return Actions.REDUCE\n",
    "    else:\n",
    "        return Actions.SHIFT\n",
    "\n",
    "\n",
    "def get_data_for_tree(tree, pos_map):\n",
    "    res = []\n",
    "    stack, buffer, relations = [ROOT], tree[:], []\n",
    "    top_feats = 0\n",
    "    \n",
    "    while buffer or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        buffer[0] if len(buffer) > 0 else None,\n",
    "                        relations)\n",
    "        # we need 3 words from stack and buffer to train the LSTM model\n",
    "        if top_feats < 3 and len(stack) and len(buffer):\n",
    "            top_stack = stack[-1]\n",
    "            top_buff = buffer[0]\n",
    "            res.append((top_stack['lemma'], pos_map[top_stack['upostag'] or pos_map['UNK']],\n",
    "                        top_buff['lemma'], pos_map[top_buff['upostag'] or pos_map['UNK']],\n",
    "                        action.value))\n",
    "            top_feats += 1\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(buffer.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            rel = (stack[-1]['id'], buffer[0]['id'])\n",
    "            relations.append(rel)\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            rel = (buffer[0]['id'], stack[-1]['id'])\n",
    "            relations.append(rel)\n",
    "            stack.append(buffer.pop(0))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_data(trees, v2w_model, pos_map):\n",
    "    vectors, labels = [], []\n",
    "    for tree in trees:\n",
    "        t_vectors, t_labels = [], []\n",
    "        tree_data = get_data_for_tree(\n",
    "            [t for t in tree if type(t['id']) == int], pos_map)\n",
    "        for stack, stack_pos, buff, buff_pos, label in tree_data:\n",
    "            try:\n",
    "                stack_vect = v2w_model.wv[stack]\n",
    "                buff_vect = v2w_model.wv[buff]\n",
    "                stack_vect_pos = np.append(stack_vect, [stack_pos])\n",
    "                buff_vect_pos = np.append(buff_vect, [buff_pos])\n",
    "                vc = np.concatenate((stack_vect_pos, buff_vect_pos))\n",
    "                vectors.append(vc)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return np.dstack(vectors), np.dstack(labels)\n",
    "\n",
    "\n",
    "def get_predicted_label(labels):\n",
    "    l = labels[0][0]\n",
    "    return np.argmax(l)\n",
    "\n",
    "\n",
    "def get_target_data(pairs, v2w_model):\n",
    "    vectors = []\n",
    "    for stack, buffer in pairs:\n",
    "        try:\n",
    "            stack_vect = v2w_model.wv[stack]\n",
    "            buff_vect = v2w_model.wv[buff]\n",
    "            vectors.append(np.concatenate((stack_vect, buff_vect)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.dstack(vectors)\n",
    "\n",
    "\n",
    "def get_data_with_timesteps(data, steps):\n",
    "    res = []\n",
    "    for i, x in enumerate(data):\n",
    "        a = []\n",
    "        a.append(data[i][0])\n",
    "        for j in range(steps - 1):\n",
    "            if i + j < len(data) - 1:\n",
    "                a.append(data[i+j][0])\n",
    "            else:\n",
    "                a.append(data[i][0])\n",
    "        res.append(np.array(a))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(n_vocab, n_classes, X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(LSTM(512, return_sequences=True, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def to_cat(y_int, vocab_size):\n",
    "    return to_categorical(y_int, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, x, y, n_classes):\n",
    "    model.fit(x, y, epochs=epochs, verbose=1)\n",
    "\n",
    "    \n",
    "def predict(model, x):\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "python_random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-26 00:39:11 WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "v2w_model = get_w2v_model(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = get_pos_tag_map(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_data(train_trees, v2w_model, pos_map)\n",
    "n_vocab = len(train_x)\n",
    "n_classes = 4\n",
    "x = np.moveaxis(train_x, -1, 0)\n",
    "y = np.moveaxis(train_y, -1, 0)\n",
    "y = np.array([to_cat(x, n_classes) for x in y])\n",
    "x = get_data_with_timesteps(x, 20)\n",
    "y = get_data_with_timesteps(y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_135\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_212 (LSTM)              (None, 20, 256)           879616    \n",
      "_________________________________________________________________\n",
      "lstm_213 (LSTM)              (None, 20, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 20, 64)            32832     \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 20, 4)             260       \n",
      "=================================================================\n",
      "Total params: 2,487,620\n",
      "Trainable params: 2,487,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(n_vocab, n_classes, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1545,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8804/8804 [==============================] - 67s 8ms/step - loss: 0.8726 - accuracy: 0.6474\n",
      "Epoch 2/100\n",
      "8804/8804 [==============================] - 59s 7ms/step - loss: 0.7005 - accuracy: 0.7139\n",
      "Epoch 3/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.4888 - accuracy: 0.7982\n",
      "Epoch 4/100\n",
      "8804/8804 [==============================] - 70s 8ms/step - loss: 0.2744 - accuracy: 0.8941\n",
      "Epoch 5/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.1870 - accuracy: 0.9292\n",
      "Epoch 6/100\n",
      "8804/8804 [==============================] - 67s 8ms/step - loss: 0.1607 - accuracy: 0.9376\n",
      "Epoch 7/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.1475 - accuracy: 0.9432\n",
      "Epoch 8/100\n",
      "8804/8804 [==============================] - 66s 7ms/step - loss: 0.1355 - accuracy: 0.9470\n",
      "Epoch 9/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.1321 - accuracy: 0.9485\n",
      "Epoch 10/100\n",
      "8804/8804 [==============================] - 60s 7ms/step - loss: 0.1272 - accuracy: 0.9507\n",
      "Epoch 11/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.1183 - accuracy: 0.9544\n",
      "Epoch 12/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.1154 - accuracy: 0.9554\n",
      "Epoch 13/100\n",
      "8804/8804 [==============================] - 69s 8ms/step - loss: 0.1163 - accuracy: 0.9556\n",
      "Epoch 14/100\n",
      "8804/8804 [==============================] - 68s 8ms/step - loss: 0.1100 - accuracy: 0.9581\n",
      "Epoch 15/100\n",
      "8804/8804 [==============================] - 73s 8ms/step - loss: 0.1094 - accuracy: 0.9587\n",
      "Epoch 16/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.1005 - accuracy: 0.9622\n",
      "Epoch 17/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0983 - accuracy: 0.9626\n",
      "Epoch 18/100\n",
      "8804/8804 [==============================] - 64s 7ms/step - loss: 0.0989 - accuracy: 0.9627\n",
      "Epoch 19/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0998 - accuracy: 0.9628\n",
      "Epoch 20/100\n",
      "8804/8804 [==============================] - 66s 8ms/step - loss: 0.0922 - accuracy: 0.9651\n",
      "Epoch 21/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.0890 - accuracy: 0.9665\n",
      "Epoch 22/100\n",
      "8804/8804 [==============================] - 66s 7ms/step - loss: 0.0870 - accuracy: 0.9673\n",
      "Epoch 23/100\n",
      "8804/8804 [==============================] - 69s 8ms/step - loss: 0.0920 - accuracy: 0.9656\n",
      "Epoch 24/100\n",
      "8804/8804 [==============================] - 60s 7ms/step - loss: 0.0871 - accuracy: 0.9678\n",
      "Epoch 25/100\n",
      "8804/8804 [==============================] - 67s 8ms/step - loss: 0.0866 - accuracy: 0.9681\n",
      "Epoch 26/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0830 - accuracy: 0.9692\n",
      "Epoch 27/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0898 - accuracy: 0.9681\n",
      "Epoch 28/100\n",
      "8804/8804 [==============================] - 68s 8ms/step - loss: 0.1129 - accuracy: 0.9594\n",
      "Epoch 29/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0792 - accuracy: 0.9704\n",
      "Epoch 30/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.0742 - accuracy: 0.9721\n",
      "Epoch 31/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.0705 - accuracy: 0.9737\n",
      "Epoch 32/100\n",
      "8804/8804 [==============================] - 59s 7ms/step - loss: 0.0770 - accuracy: 0.9717\n",
      "Epoch 33/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0743 - accuracy: 0.9724\n",
      "Epoch 34/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0736 - accuracy: 0.9725\n",
      "Epoch 35/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.0765 - accuracy: 0.9716\n",
      "Epoch 36/100\n",
      "8804/8804 [==============================] - 70s 8ms/step - loss: 0.0707 - accuracy: 0.9741\n",
      "Epoch 37/100\n",
      "8804/8804 [==============================] - 66s 7ms/step - loss: 0.0669 - accuracy: 0.9751\n",
      "Epoch 38/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0687 - accuracy: 0.9746\n",
      "Epoch 39/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0728 - accuracy: 0.9730\n",
      "Epoch 40/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.0665 - accuracy: 0.9750\n",
      "Epoch 41/100\n",
      "8804/8804 [==============================] - 70s 8ms/step - loss: 0.0650 - accuracy: 0.9753\n",
      "Epoch 42/100\n",
      "8804/8804 [==============================] - 59s 7ms/step - loss: 0.0723 - accuracy: 0.9737\n",
      "Epoch 43/100\n",
      "8804/8804 [==============================] - 67s 8ms/step - loss: 0.0644 - accuracy: 0.9758\n",
      "Epoch 44/100\n",
      "8804/8804 [==============================] - 64s 7ms/step - loss: 0.0649 - accuracy: 0.9758\n",
      "Epoch 45/100\n",
      "8804/8804 [==============================] - 66s 7ms/step - loss: 0.0638 - accuracy: 0.9758\n",
      "Epoch 46/100\n",
      "8804/8804 [==============================] - 65s 7ms/step - loss: 0.0650 - accuracy: 0.9756\n",
      "Epoch 47/100\n",
      "8804/8804 [==============================] - 58s 7ms/step - loss: 0.0884 - accuracy: 0.9680\n",
      "Epoch 48/100\n",
      "8804/8804 [==============================] - 59s 7ms/step - loss: 0.0633 - accuracy: 0.9761\n",
      "Epoch 49/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0648 - accuracy: 0.9762\n",
      "Epoch 50/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0607 - accuracy: 0.9773\n",
      "Epoch 51/100\n",
      "8804/8804 [==============================] - 60s 7ms/step - loss: 0.0635 - accuracy: 0.9767\n",
      "Epoch 52/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0600 - accuracy: 0.9777\n",
      "Epoch 53/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0614 - accuracy: 0.9773\n",
      "Epoch 54/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0588 - accuracy: 0.9778\n",
      "Epoch 55/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0613 - accuracy: 0.9777\n",
      "Epoch 56/100\n",
      "8804/8804 [==============================] - 63s 7ms/step - loss: 0.0618 - accuracy: 0.9771\n",
      "Epoch 57/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0563 - accuracy: 0.9787\n",
      "Epoch 58/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0565 - accuracy: 0.9784\n",
      "Epoch 59/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0605 - accuracy: 0.9777\n",
      "Epoch 60/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0617 - accuracy: 0.9777\n",
      "Epoch 61/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0557 - accuracy: 0.9788\n",
      "Epoch 62/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0589 - accuracy: 0.9783\n",
      "Epoch 63/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0601 - accuracy: 0.9774\n",
      "Epoch 64/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0539 - accuracy: 0.9793\n",
      "Epoch 65/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0576 - accuracy: 0.9784\n",
      "Epoch 66/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0714 - accuracy: 0.9740\n",
      "Epoch 67/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0547 - accuracy: 0.9790\n",
      "Epoch 68/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0510 - accuracy: 0.9804\n",
      "Epoch 69/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0528 - accuracy: 0.9799\n",
      "Epoch 70/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0516 - accuracy: 0.9801\n",
      "Epoch 71/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0577 - accuracy: 0.9784\n",
      "Epoch 72/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0546 - accuracy: 0.9799\n",
      "Epoch 73/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0515 - accuracy: 0.9802\n",
      "Epoch 74/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0520 - accuracy: 0.9801\n",
      "Epoch 75/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0520 - accuracy: 0.9802\n",
      "Epoch 76/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0554 - accuracy: 0.9790\n",
      "Epoch 77/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0520 - accuracy: 0.9804\n",
      "Epoch 78/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0516 - accuracy: 0.9805\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0522 - accuracy: 0.9804\n",
      "Epoch 80/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0501 - accuracy: 0.9809\n",
      "Epoch 81/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0517 - accuracy: 0.9804\n",
      "Epoch 82/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0518 - accuracy: 0.9800\n",
      "Epoch 83/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0545 - accuracy: 0.9793\n",
      "Epoch 84/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0639 - accuracy: 0.9766\n",
      "Epoch 85/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0488 - accuracy: 0.9813\n",
      "Epoch 86/100\n",
      "8804/8804 [==============================] - 62s 7ms/step - loss: 0.0468 - accuracy: 0.9817\n",
      "Epoch 87/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0457 - accuracy: 0.9822\n",
      "Epoch 88/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0508 - accuracy: 0.9808\n",
      "Epoch 89/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0516 - accuracy: 0.9803\n",
      "Epoch 90/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0476 - accuracy: 0.9817\n",
      "Epoch 91/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0471 - accuracy: 0.9819\n",
      "Epoch 92/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0513 - accuracy: 0.9806\n",
      "Epoch 93/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0506 - accuracy: 0.9805\n",
      "Epoch 94/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0481 - accuracy: 0.9815\n",
      "Epoch 95/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0496 - accuracy: 0.9808 2s - loss: 0.0\n",
      "Epoch 96/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0674 - accuracy: 0.9779\n",
      "Epoch 97/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0762 - accuracy: 0.9756\n",
      "Epoch 98/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0875 - accuracy: 0.9718\n",
      "Epoch 99/100\n",
      "8804/8804 [==============================] - 61s 7ms/step - loss: 0.0726 - accuracy: 0.9753\n",
      "Epoch 100/100\n",
      "8804/8804 [==============================] - 60s 7ms/step - loss: 0.0544 - accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "train_model(model, 100, x, y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1546,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('oracle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
