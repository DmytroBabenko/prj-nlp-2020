{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import stanza\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Dropout, Masking\n",
    "from keras.utils import to_categorical\n",
    "from conllu import parse\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "import progressbar\n",
    "import tensorflow as tf\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../UD_Ukrainian-IU'\n",
    "\n",
    "with open(PATH + '/uk_iu-ud-train.conllu') as f:\n",
    "    train_data = f.read()\n",
    "    \n",
    "with open(PATH + '/uk_iu-ud-dev.conllu') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "train_trees = parse(train_data)\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_uk = spacy.load('/Users/lissm/Work/Dev/NLP/corpora/vectors/ubercorpus_lowercased_tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Actions(str, Enum):\n",
    "#     SHIFT = 'shift'\n",
    "#     REDUCE = 'reduce'\n",
    "#     RIGHT = 'right'\n",
    "#     LEFT = 'left'\n",
    "    \n",
    "class Actions(int, Enum):\n",
    "    SHIFT = 0\n",
    "    REDUCE = 1\n",
    "    RIGHT = 2\n",
    "    LEFT = 3\n",
    "\n",
    "\n",
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats',\n",
    "                                        None), ('head', None),  ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "\n",
    "# def dep_parse(tree, clf, vectorizer, feature_extractor):\n",
    "#     stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "#     while queue or stack:\n",
    "#         if stack and not queue:\n",
    "#             stack.pop()\n",
    "#         else:\n",
    "#             features = feature_extractor(stack, queue, relations)\n",
    "#             action = clf.predict(vectorizer.transform([features]))[0]\n",
    "\n",
    "#             if action == Actions.SHIFT:\n",
    "#                 stack.append(queue.pop(0))\n",
    "#             elif action == Actions.REDUCE:\n",
    "#                 stack.pop()\n",
    "#             elif action == Actions.LEFT:\n",
    "#                 deprel = stack[-1]['deprel'] or predict_deprel(\n",
    "#                     stack[-1], queue[0], stack + queue)\n",
    "#                 rel = (stack[-1]['id'], queue[0]['id'], deprel)\n",
    "#                 relations.append(rel)\n",
    "#                 stack.pop()\n",
    "#             elif action == Actions.RIGHT:\n",
    "#                 deprel = queue[0]['deprel'] or predict_deprel(\n",
    "#                     queue[0], stack[-1], stack + queue)\n",
    "#                 rel = (queue[0]['id'], stack[-1]['id'], deprel)\n",
    "#                 relations.append(rel)\n",
    "#                 stack.append(queue.pop(0))\n",
    "\n",
    "#     return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data & reporting utils \"\"\"\n",
    "\n",
    "\n",
    "def calculate_as(trees, clf, vect, feature_extractor):\n",
    "    total, tpu, tpl, full_match = 0, 0, 0, 0\n",
    "    golden_u, golden_l = None, None\n",
    "    for tree in trees:\n",
    "        tree = [t for t in tree if type(t['id']) == int]\n",
    "        golden_all = [(node['id'], node['head'], node['deprel'])\n",
    "                      for node in tree]\n",
    "        golden_u = [(x, y) for x, y, _ in golden_all]\n",
    "\n",
    "        predicted_all = dep_parse(tree, clf, vect, feature_extractor)\n",
    "        predicted_u = [(x, y) for x, y, _ in predicted_all]\n",
    "\n",
    "        total += len(tree)\n",
    "        tpu += len(set(golden_u).intersection(set(predicted_u)))\n",
    "        tpl += len(set(golden_all).intersection(set(predicted_all)))\n",
    "\n",
    "        if set(golden_all) == set(predicted_all):\n",
    "            full_match += 1\n",
    "\n",
    "    print('== Attachment score report ==')\n",
    "    print('Total: ', total)\n",
    "    print('Match unlabeled: ', tpu)\n",
    "#     print('Match labeled: ', tpl)\n",
    "    print('UAS: ', round(tpu/total, 2))\n",
    "#     print('LAS: ', round(tpl/total, 2))\n",
    "    print(\"Full match:\", round(full_match/len(trees), 2))\n",
    "\n",
    "\n",
    "def get_lrc_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def print_result(train_trees, test_trees, clf, feature_extractor):\n",
    "    train_feat, train_lab = get_data(train_trees, feature_extractor)\n",
    "    test_feat, test_lab = get_data(test_trees, feature_extractor)\n",
    "\n",
    "    clf.fit(train_feat, train_lab)\n",
    "    print(classification_report(test_lab, clf.predict(test_feat)))\n",
    "    calculate_as(test_trees, clf['lrc'], clf['dict_vect'], feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_model(trees):\n",
    "    words = []\n",
    "    for tree in trees:\n",
    "        words.append([x['lemma'] for x in tree])\n",
    "    model = Word2Vec(\n",
    "        words, size=300, min_count=1, workers=6,\n",
    "        window=5, alpha=0.030, negative=7, iter=5\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def oracle(stack, top_queue, relations):\n",
    "    \"\"\"\n",
    "    Make a decision on the right action to do.\n",
    "    \"\"\"\n",
    "    top_stack = stack[-1]\n",
    "    # check if both stack and queue are non-empty\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    # check if there are any clear dependencies\n",
    "    elif top_queue['head'] == top_stack['id']:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack['head'] == top_queue['id']:\n",
    "        return Actions.LEFT\n",
    "    # check if we can reduce the top of the stack\n",
    "    elif top_stack['id'] in [i[0] for i in relations] and \\\n",
    "        (top_queue['head'] < top_stack['id'] or\n",
    "         [s for s in stack if s['head'] == top_queue['id']]):\n",
    "        return Actions.REDUCE\n",
    "    # default option\n",
    "    else:\n",
    "        return Actions.SHIFT\n",
    "\n",
    "\n",
    "def get_data_for_tree(tree):\n",
    "    res = []\n",
    "    stack, buffer, relations = [ROOT], tree[:], []\n",
    "\n",
    "    while buffer or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        buffer[0] if len(buffer) > 0 else None,\n",
    "                        relations)\n",
    "        if len(stack) and len(buffer):\n",
    "            res.append((stack[-1]['lemma'], buffer[0]['lemma'], action.value))\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(buffer.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            rel = (stack[-1]['id'], buffer[0]['id'])\n",
    "            relations.append(rel)\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            rel = (buffer[0]['id'], stack[-1]['id'])\n",
    "            relations.append(rel)\n",
    "            stack.append(buffer.pop(0))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_data(trees, v2w_model):\n",
    "    vectors, labels = [], []\n",
    "    for tree in trees:\n",
    "        t_vectors, t_labels = [], []\n",
    "        tree_data = get_data_for_tree(\n",
    "            [t for t in tree if type(t['id']) == int])\n",
    "        for stack, buff, label in tree_data:\n",
    "            try:\n",
    "                stack_vect = v2w_model.wv[stack]\n",
    "                buff_vect = v2w_model.wv[buff]\n",
    "                vectors.append(np.concatenate((stack_vect, buff_vect)))\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return np.moveaxis(np.dstack(vectors), -1, 0), np.dstack(labels)\n",
    "\n",
    "\n",
    "def get_predicted_label(labels):\n",
    "    l = labels[0][0]\n",
    "    return np.argmax(l)\n",
    "\n",
    "\n",
    "def get_target_data(pairs, v2w_model):\n",
    "    vectors = []\n",
    "    for stack, buffer in pairs:\n",
    "        try:\n",
    "            stack_vect = v2w_model.wv[stack]\n",
    "            buff_vect = v2w_model.wv[buff]\n",
    "            vectors.append(np.concatenate((stack_vect, buff_vect)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.dstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(n_vocab, X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(LSTM(512, return_sequences=True, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def to_cat(y_int, vocab_size):\n",
    "    return to_categorical(y_int, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, x, y, n_classes):\n",
    "    model.fit(x, y, epochs=epochs, verbose=1)\n",
    "\n",
    "    \n",
    "def predict(model, x):\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "python_random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-23 22:48:14 WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "v2w_model = get_w2v_model(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = get_data(train_trees, v2w_model)\n",
    "n_vocab = len(train_x)\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_70 (LSTM)               (None, 1, 4)              9680      \n",
      "_________________________________________________________________\n",
      "lstm_71 (LSTM)               (None, 1, 512)            1058816   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1, 4)              2052      \n",
      "=================================================================\n",
      "Total params: 1,070,548\n",
      "Trainable params: 1,070,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(n_vocab, train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160186/160186 [==============================] - 90s 564us/step - loss: 1.1018 - accuracy: 0.5126\n",
      "Epoch 2/20\n",
      "160186/160186 [==============================] - 91s 566us/step - loss: 1.0641 - accuracy: 0.5495\n",
      "Epoch 3/20\n",
      "160186/160186 [==============================] - 91s 568us/step - loss: 1.0426 - accuracy: 0.5604\n",
      "Epoch 4/20\n",
      "160186/160186 [==============================] - 90s 560us/step - loss: 1.0302 - accuracy: 0.5646\n",
      "Epoch 5/20\n",
      "160186/160186 [==============================] - 91s 568us/step - loss: 1.0250 - accuracy: 0.5664\n",
      "Epoch 6/20\n",
      "160186/160186 [==============================] - 91s 571us/step - loss: 1.0220 - accuracy: 0.5676\n",
      "Epoch 7/20\n",
      "160186/160186 [==============================] - 92s 575us/step - loss: 1.0211 - accuracy: 0.5675\n",
      "Epoch 8/20\n",
      "160186/160186 [==============================] - 91s 570us/step - loss: 1.0166 - accuracy: 0.5698\n",
      "Epoch 9/20\n",
      "160186/160186 [==============================] - 91s 569us/step - loss: 1.0151 - accuracy: 0.5717\n",
      "Epoch 10/20\n",
      "160186/160186 [==============================] - 90s 562us/step - loss: 1.0120 - accuracy: 0.5733\n",
      "Epoch 11/20\n",
      "160186/160186 [==============================] - 91s 568us/step - loss: 1.0103 - accuracy: 0.5740\n",
      "Epoch 12/20\n",
      "160186/160186 [==============================] - 92s 573us/step - loss: 1.0091 - accuracy: 0.5744\n",
      "Epoch 13/20\n",
      "160186/160186 [==============================] - 93s 578us/step - loss: 1.0078 - accuracy: 0.5748\n",
      "Epoch 14/20\n",
      "160186/160186 [==============================] - 91s 566us/step - loss: 1.0071 - accuracy: 0.5752\n",
      "Epoch 15/20\n",
      "160186/160186 [==============================] - 91s 571us/step - loss: 1.0037 - accuracy: 0.5771\n",
      "Epoch 16/20\n",
      "160186/160186 [==============================] - 91s 570us/step - loss: 1.0012 - accuracy: 0.5786\n",
      "Epoch 17/20\n",
      "160186/160186 [==============================] - 93s 581us/step - loss: 0.9995 - accuracy: 0.5792\n",
      "Epoch 18/20\n",
      "160186/160186 [==============================] - 93s 583us/step - loss: 0.9973 - accuracy: 0.5804\n",
      "Epoch 19/20\n",
      "160186/160186 [==============================] - 92s 576us/step - loss: 0.9961 - accuracy: 0.5805\n",
      "Epoch 20/20\n",
      "160186/160186 [==============================] - 93s 580us/step - loss: 0.9939 - accuracy: 0.5812\n"
     ]
    }
   ],
   "source": [
    "y = np.moveaxis(train_y, -1, 0)\n",
    "y = np.array([to_cat(x, n_classes) for x in y])\n",
    "train_model(model, 20, train_x, y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
