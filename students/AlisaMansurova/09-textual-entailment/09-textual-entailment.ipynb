{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../../corpora/snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    train_data = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    dev_data = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_test.jsonl') as f:\n",
    "    test_data = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "# FOR FUTURE IMPROVEMENTS\n",
    "def get_nlp_docs(s1, s2):\n",
    "    return nlp(s1), nlp(s2)\n",
    "\n",
    "\n",
    "def get_intersection(ents1, ents2):\n",
    "    setA = set(ents1)\n",
    "    setB = set(ents2)\n",
    "    universe = set(doc1) | set(doc2)\n",
    "\n",
    "    return len(setA & setB)/(len(universe))\n",
    "\n",
    "\n",
    "def feature_extractor_base(doc1, doc2):\n",
    "    feats = {}\n",
    "    feats['similarity'] = doc1.similarity(doc2)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# It makes it a bit worse\n",
    "# TODO: investigate and improve\n",
    "def feature_extractor_inter_ner(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['ner-inter'] = get_intersection(\n",
    "        [(x.label_, x.lemma_) for x in doc1.ents],\n",
    "        [(x.label_, x.lemma_) for x in doc2.ents]\n",
    "    )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_word(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['w-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1],\n",
    "            [x.lemma_ for x in doc2]\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_noun(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['n-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'NOUN'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'NOUN']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_verb(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['v-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'VERB'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'VERB']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_num(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['v-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'NUM'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'NUM']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "    \n",
    "\n",
    "def get_data(dataset, feature_extractor):\n",
    "    features = [feature_extractor(nlp(x['sentence1']), nlp(x['sentence2'])) for x in dataset]\n",
    "    labels = [x['gold_label'] for x in dataset]\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def print_result(train_data, dev_data, feature_extractor):\n",
    "    X_train, y_train = get_data(train_data[:200], feature_extractor)\n",
    "    X_dev, y_dev = get_data(dev_data[:200], feature_extractor)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_dev, clf.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (just simply use sentence similarity from spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.41      0.32      0.36        69\n",
      "   entailment       0.38      0.67      0.48        66\n",
      "      neutral       0.17      0.08      0.11        64\n",
      "\n",
      "     accuracy                           0.36       200\n",
      "    macro avg       0.24      0.27      0.24       200\n",
      " weighted avg       0.32      0.35      0.32       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_result(train_data, dev_data, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With NER intersection (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.40      0.30      0.34        69\n",
      "   entailment       0.38      0.67      0.48        66\n",
      "      neutral       0.16      0.08      0.11        64\n",
      "\n",
      "     accuracy                           0.35       200\n",
      "    macro avg       0.23      0.26      0.23       200\n",
      " weighted avg       0.31      0.35      0.31       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_inter_ner)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With word intersection without NER intersection (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.40      0.48      0.44        69\n",
      "   entailment       0.37      0.59      0.46        66\n",
      "      neutral       0.15      0.03      0.05        64\n",
      "\n",
      "     accuracy                           0.37       200\n",
      "    macro avg       0.23      0.28      0.24       200\n",
      " weighted avg       0.31      0.37      0.32       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_word)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With word intersection with NER intersection (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.40      0.48      0.44        69\n",
      "   entailment       0.37      0.59      0.46        66\n",
      "      neutral       0.15      0.03      0.05        64\n",
      "\n",
      "     accuracy                           0.37       200\n",
      "    macro avg       0.23      0.28      0.24       200\n",
      " weighted avg       0.31      0.37      0.32       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# TODO: NER words, lemma etc\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. With NOUN intersection (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.42      0.48      0.45        69\n",
      "   entailment       0.50      0.53      0.51        66\n",
      "      neutral       0.41      0.33      0.37        64\n",
      "\n",
      "     accuracy                           0.45       200\n",
      "    macro avg       0.33      0.33      0.33       200\n",
      " weighted avg       0.44      0.45      0.44       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.43      0.48      0.46        69\n",
      "   entailment       0.47      0.58      0.52        66\n",
      "      neutral       0.30      0.20      0.24        64\n",
      "\n",
      "     accuracy                           0.42       200\n",
      "    macro avg       0.30      0.31      0.30       200\n",
      " weighted avg       0.40      0.42      0.41       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# VERB inter (-)\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00         1\n",
      "contradiction       0.42      0.48      0.45        69\n",
      "   entailment       0.50      0.53      0.51        66\n",
      "      neutral       0.41      0.33      0.37        64\n",
      "\n",
      "     accuracy                           0.45       200\n",
      "    macro avg       0.33      0.33      0.33       200\n",
      " weighted avg       0.44      0.45      0.44       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# NUM inter (-)\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb,\n",
    "                            feature_extractor_inter_num\n",
    "                           )\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{,, one, Sent, one, another}\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('Sent one, another one')\n",
    "doc2 = nlp('Sent two')\n",
    "set([x for x in doc1]) | set([x for x in doc2])\n",
    "print(set(doc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'d': 9}, 'ww', 'gg')\n",
      "({'u': 8}, 'ww', 'gg')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d': 9, 'u': 8}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            print(f(*arg))\n",
    "            res.update(f(*arg)[0])\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "def f1(s1, s2):\n",
    "    return {'d': 9}, s1, s2\n",
    "\n",
    "def f2(s1, s2):\n",
    "    return {'u': 8}, s1, s2\n",
    "\n",
    "comp = compose(f1, f2)\n",
    "comp('ww', 'gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(listA)&set(listB)) / float(len(set(listA) | set(listB))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fff oö\n"
     ]
    }
   ],
   "source": [
    "a = ({}, 'fff', 'oö')\n",
    "print(*(a[1], a[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
