{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../../../corpora/snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    train_data = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    dev_data = [json.loads(line) for line in f.readlines()]\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_test.jsonl') as f:\n",
    "    test_data = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=20))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def get_intersection(ents1, ents2):\n",
    "    setA = set(ents1)\n",
    "    setB = set(ents2)\n",
    "    universe = set(doc1) | set(doc2)\n",
    "\n",
    "    return len(setA & setB)/(len(universe))\n",
    "\n",
    "\n",
    "def get_tokens_similarity(toks1, toks2):\n",
    "    setA = set(toks1)\n",
    "    setB = set(toks2)\n",
    "    universe = set(toks1) | set(toks2)\n",
    "    \n",
    "    sim = [x.similarity(y) for x in setA for y in setB if x.vector_norm and y.vector_norm]\n",
    "    return len(sim)/(len(universe))\n",
    "\n",
    "\n",
    "def get_ngrams(text):\n",
    "    res = []\n",
    "    for i in range(0, len(text), 3):\n",
    "        if i > 0 and i + 3 <= len(text):\n",
    "            res.append(text[i:i + 3])\n",
    "        elif i > 0 and i + 3 > len(text):\n",
    "            res.append(text[i:i + 3] + '</S>')\n",
    "        else:\n",
    "            res.append('<S>' + text[i:i + 3])\n",
    "    return res\n",
    "\n",
    "\n",
    "def feature_extractor_base(doc1, doc2):\n",
    "    feats = {}\n",
    "    feats['similarity'] = doc1.similarity(doc2)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# It makes it a bit worse\n",
    "# TODO: investigate and improve\n",
    "def feature_extractor_inter_ner(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['ner-inter'] = get_intersection(\n",
    "        [x.ent_type_ for x in doc1],\n",
    "        [x.ent_type_ for x in doc2]\n",
    "    )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_word(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['w-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1],\n",
    "            [x.lemma_ for x in doc2]\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_noun(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['nn-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'NOUN'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'NOUN']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_verb(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['v-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'VERB'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'VERB']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_num(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['nm-inter'] = get_intersection(\n",
    "            [x.lemma_ for x in doc1 if x.pos_ == 'NUM'],\n",
    "            [x.lemma_ for x in doc2 if x.pos_ == 'NUM']\n",
    "        )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_verb_nn_sim(doc1, doc2):\n",
    "    def get_by_pos(pos):\n",
    "        t1 = [x for x in doc1 if x.pos_ == pos]\n",
    "        t2 = [x for x in doc2 if x.pos_ == pos]\n",
    "        return t1, t2\n",
    "        \n",
    "    feats = {}\n",
    "    \n",
    "    sent1_verbs, sent2_verbs = get_by_pos('VERB')\n",
    "    sent1_nouns, sent2_nouns = get_by_pos('NOUN')\n",
    "    \n",
    "    if sent1_verbs and sent2_verbs:\n",
    "        feats['v-similar'] = get_tokens_similarity(sent1_verbs, sent2_verbs)\n",
    "    \n",
    "    if sent1_nouns and sent2_nouns:\n",
    "        feats['nn-similar'] = get_tokens_similarity(sent1_nouns, sent2_nouns)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "def feature_extractor_inter_ngrams(doc1, doc2):\n",
    "    feats = {}\n",
    "    \n",
    "    n1 = get_ngrams(' '.join([x.lemma_ for x in doc1]))\n",
    "    n2 = get_ngrams(' '.join([x.lemma_ for x in doc2]))\n",
    "\n",
    "    feats['ngr-inter'] = get_intersection(n1, n2)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "# TODO: make smth with unknown gold label:\n",
    "# a) filter those\n",
    "# b) mark as neutral\n",
    "def get_data(dataset, feature_extractor):\n",
    "    features = [feature_extractor(nlp(x['sentence1']), nlp(x['sentence2'])) for x in dataset]\n",
    "    labels = [x['gold_label'] for x in dataset]\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def print_result(train_data, dev_data, feature_extractor):\n",
    "    X_train, y_train = get_data(train_data[:500], feature_extractor)\n",
    "    X_dev, y_dev = get_data(dev_data[:500], feature_extractor)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_dev, clf.predict(X_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (just simply use sentence similarity from spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.47      0.47      0.47       169\n",
      "   entailment       0.43      0.65      0.52       165\n",
      "      neutral       0.33      0.18      0.23       156\n",
      "\n",
      "     accuracy                           0.43       500\n",
      "    macro avg       0.31      0.32      0.31       500\n",
      " weighted avg       0.41      0.43      0.40       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_result(train_data, dev_data, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With NER intersection (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.47      0.47      0.47       169\n",
      "   entailment       0.43      0.65      0.51       165\n",
      "      neutral       0.31      0.16      0.21       156\n",
      "\n",
      "     accuracy                           0.42       500\n",
      "    macro avg       0.30      0.32      0.30       500\n",
      " weighted avg       0.40      0.42      0.39       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_inter_ner)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With word intersection without NER intersection (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.41      0.46      0.43       169\n",
      "   entailment       0.45      0.56      0.50       165\n",
      "      neutral       0.29      0.19      0.23       156\n",
      "\n",
      "     accuracy                           0.40       500\n",
      "    macro avg       0.29      0.30      0.29       500\n",
      " weighted avg       0.38      0.40      0.38       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_word)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With word intersection with NER intersection (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.40      0.49      0.44       169\n",
      "   entailment       0.46      0.58      0.51       165\n",
      "      neutral       0.30      0.17      0.22       156\n",
      "\n",
      "     accuracy                           0.41       500\n",
      "    macro avg       0.29      0.31      0.29       500\n",
      " weighted avg       0.38      0.41      0.39       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# TODO: NER words, lemma etc\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. With NOUN intersection (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.42      0.49      0.45       169\n",
      "   entailment       0.45      0.50      0.47       165\n",
      "      neutral       0.26      0.21      0.23       156\n",
      "\n",
      "     accuracy                           0.39       500\n",
      "    macro avg       0.28      0.30      0.29       500\n",
      " weighted avg       0.37      0.39      0.38       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.47      0.63      0.54       169\n",
      "   entailment       0.48      0.69      0.57       165\n",
      "      neutral       0.30      0.07      0.11       156\n",
      "\n",
      "     accuracy                           0.46       500\n",
      "    macro avg       0.31      0.35      0.31       500\n",
      " weighted avg       0.41      0.46      0.41       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# VERB inter (-)\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb)\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.47      0.62      0.53       169\n",
      "   entailment       0.48      0.69      0.57       165\n",
      "      neutral       0.30      0.08      0.12       156\n",
      "\n",
      "     accuracy                           0.46       500\n",
      "    macro avg       0.31      0.35      0.31       500\n",
      " weighted avg       0.41      0.46      0.41       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# NUM inter (-)\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb,\n",
    "                            feature_extractor_inter_num\n",
    "                           )\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.51      0.52      0.52       169\n",
      "   entailment       0.50      0.70      0.58       165\n",
      "      neutral       0.38      0.23      0.29       156\n",
      "\n",
      "     accuracy                           0.48       500\n",
      "    macro avg       0.35      0.36      0.35       500\n",
      " weighted avg       0.46      0.48      0.46       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# VERB & NOUN tokens similarity (+)\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb,\n",
    "                            feature_extractor_inter_num,\n",
    "                            feature_extractor_verb_nn_sim,\n",
    "                           )\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "            -       0.00      0.00      0.00        10\n",
      "contradiction       0.52      0.52      0.52       169\n",
      "   entailment       0.52      0.69      0.59       165\n",
      "      neutral       0.38      0.27      0.31       156\n",
      "\n",
      "     accuracy                           0.49       500\n",
      "    macro avg       0.35      0.37      0.36       500\n",
      " weighted avg       0.46      0.49      0.47       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ngrams\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_inter_ner,\n",
    "                            feature_extractor_inter_word,\n",
    "                            feature_extractor_inter_noun,\n",
    "                            feature_extractor_inter_verb,\n",
    "                            feature_extractor_inter_num,\n",
    "                            feature_extractor_verb_nn_sim,\n",
    "                            feature_extractor_inter_ngrams\n",
    "                           )\n",
    "print_result(train_data, dev_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[x for x in train_data if x['gold_label'] == '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
