{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import en_core_web_md\n",
    "# from spacy.tokens import Doc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknowns(data):\n",
    "    return [x for x in data if x['gold_label'] != '-']\n",
    "\n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    train_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    dev_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_test.jsonl') as f:\n",
    "    test_data = filter_unknowns([json.loads(line) for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "def get_nlp_normalized(sent):\n",
    "    doc = nlp(sent)\n",
    "    neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "    neg_processed = []\n",
    "    neg_ind = 100500\n",
    "    for tok in doc:\n",
    "        if tok.lower_ in neg:\n",
    "            neg_ind = tok.i\n",
    "        elif tok.pos_ == 'PUNCT':\n",
    "            neg_ind = 100500\n",
    "            neg_processed.append(tok.text)\n",
    "        elif tok.i > neg_ind:\n",
    "            neg_processed.append('NOT_' + tok.lemma_)\n",
    "        else:\n",
    "            neg_processed.append(tok.text)\n",
    "    return nlp(' '.join(neg_processed))\n",
    "\n",
    "\n",
    "def filter_stop_words(doc):\n",
    "    return [x for x in doc if not (x.pos_ == 'DET' or x.pos_ == 'NUM' or x.is_stop and x.dep_ != 'ROOT')]\n",
    "\n",
    "\n",
    "def normalize_sent(func):\n",
    "    def inner(s1, s2): \n",
    "        return func(filter_stop_words(s1), filter_stop_words(s2)) \n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "# TODO: use UAS for deps, compare UAS\n",
    "def get_intersection(ents1, ents2):\n",
    "    setA = set(ents1)\n",
    "    setB = set(ents2)\n",
    "    universe = setA | setB\n",
    "#     if not universe:\n",
    "    if not setB:\n",
    "        return 'NONE'\n",
    "\n",
    "#     return len(setA & setB)/(len(universe))\n",
    "    return len(setA & setB)/(len(setB))\n",
    "\n",
    "# TODO: try to use, see if it's better than ^^\n",
    "def get_intersection_alt(ents1, ents2):\n",
    "    if not ents2:\n",
    "        return 'NONE'\n",
    "    i = [x for x in ents2 if x in ents1]\n",
    "\n",
    "    return len(i)/(len(setB))\n",
    "\n",
    "\n",
    "def get_tokens_similarity(toks1, toks2):\n",
    "    setA = set(toks1)\n",
    "    setB = set(toks2)\n",
    "    universe = set(toks1) | set(toks2)\n",
    "    sim = [x.similarity(y) for x in setA for y in setB if x.has_vector and y.has_vector]\n",
    "    return len(sim)/(len(universe))\n",
    "\n",
    "\n",
    "def get_ngrams(text):\n",
    "    res = []\n",
    "    for i in range(0, len(text), 3):\n",
    "        if i > 0 and i + 3 <= len(text):\n",
    "            res.append(text[i:i + 3])\n",
    "        elif i > 0 and i + 3 > len(text):\n",
    "            res.append(text[i:i + 3] + '</S>')\n",
    "        else:\n",
    "            res.append('<S>' + text[i:i + 3])\n",
    "    return res\n",
    "\n",
    "\n",
    "def feature_extractor_base(doc1, doc2):\n",
    "    feats = {}\n",
    "    feats['similarity'] = doc1.similarity(doc2)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# It makes it a bit worse\n",
    "# TODO: investigate and improve\n",
    "@normalize_sent\n",
    "def feature_extractor_ner(doc1, doc2):\n",
    "    def _inner(doc):\n",
    "        return [x.ent_type_ for x in doc]\n",
    "    feats = {}\n",
    "\n",
    "    feats['ner'] = get_intersection(_inner(doc1), _inner(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_word(doc1, doc2):\n",
    "    def _lemm(doc):\n",
    "        return [x.lemma_ for x in doc]\n",
    "    def _noun(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'NOUN']\n",
    "    def _verb(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'VERB']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['lemma'] = get_intersection(_lemm(doc1), _lemm(doc2))\n",
    "    feats['noun'] = get_intersection(_noun(doc1), _noun(doc2))\n",
    "    feats['verb'] = get_intersection(_verb(doc1), _verb(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_verb_nn_sim(doc1, doc2):\n",
    "    def get_by_pos(pos):\n",
    "        t1 = [x for x in doc1 if x.pos_ == pos]\n",
    "        t2 = [x for x in doc2 if x.pos_ == pos]\n",
    "        return t1, t2\n",
    "        \n",
    "    feats = {}\n",
    "    \n",
    "    sent1_verbs, sent2_verbs = get_by_pos('VERB')\n",
    "    sent1_nouns, sent2_nouns = get_by_pos('NOUN')\n",
    "    \n",
    "    if sent1_verbs and sent2_verbs:\n",
    "        feats['v-similar'] = get_tokens_similarity(sent1_verbs, sent2_verbs)\n",
    "    \n",
    "    if sent1_nouns and sent2_nouns:\n",
    "        feats['nn-similar'] = get_tokens_similarity(sent1_nouns, sent2_nouns)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_ngrams(doc1, doc2):\n",
    "    def _ng_pos(doc):\n",
    "        return get_ngrams(' '.join([x.pos_ for x in doc]))\n",
    "    def _ng_dep(doc):\n",
    "        return get_ngrams(' '.join([x.dep_ for x in doc]))\n",
    "    def _ng_lemma(doc):\n",
    "        return get_ngrams(' '.join([x.lemma_ for x in doc]))\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['ngr-pos'] = get_intersection(_ng_pos(doc1), _ng_pos(doc2))\n",
    "    feats['ngr-dep'] = get_intersection(_ng_dep(doc1), _ng_dep(doc2))\n",
    "    feats['ngr-lemma'] = get_intersection(_ng_lemma(doc1), _ng_lemma(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_stemm(doc1, doc2):\n",
    "    stemmer = PorterStemmer()\n",
    "    def _stem_w(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc]\n",
    "    def _stem_v(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'VERB']\n",
    "    def _stem_n(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'NOUN']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['stemm-w'] = get_intersection(_stem_w(doc1), _stem_w(doc2))\n",
    "    feats['stemm-v'] = get_intersection(_stem_v(doc1), _stem_v(doc2))\n",
    "    feats['stemm-n'] = get_intersection(_stem_n(doc1), _stem_n(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_neg(doc1, doc2):\n",
    "    def _get_neg(doc):\n",
    "        neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "        neg_processed = []\n",
    "        neg_ind = 100500\n",
    "        for tok in doc:\n",
    "            if tok.lower_ in neg:\n",
    "                neg_ind = tok.i\n",
    "            elif tok.pos_ == 'PUNCT':\n",
    "                neg_ind = 100500\n",
    "                neg_processed.append(tok.lemma_)\n",
    "            elif tok.i > neg_ind:\n",
    "                neg_processed.append('NOT_' + tok.lemma_)\n",
    "            else:\n",
    "                neg_processed.append(tok.lemma_)\n",
    "        return neg_processed\n",
    "\n",
    "    feats = {}\n",
    "    feats['neg'] = get_intersection(_get_neg(doc1), _get_neg(doc2))\n",
    "    return feats\n",
    "\n",
    "\n",
    "#### Grammatical similarity\n",
    "def feature_extractor_deps(doc1, doc2):\n",
    "    def _inner_1(doc):\n",
    "        return [x.dep for x in doc]\n",
    "    def _inner_2(doc):\n",
    "        return [x.head.dep for x in doc]\n",
    "    def _inner_3(doc):\n",
    "        return [(x.lemma_, x.head.lemma_, x.dep) for x in doc]\n",
    "    \n",
    "    feats = {}\n",
    "\n",
    "    feats['dep'] = get_intersection(_inner_1(doc1), _inner_1(doc2))\n",
    "    feats['head-dep'] = get_intersection(_inner_2(doc1), _inner_2(doc2))\n",
    "#     feats['l-edge-dep'] = get_intersection(_inner_3(doc1), _inner_3(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# TODO: syntactic relations (x[0].dep <-- x[1])\n",
    "\n",
    "\n",
    "def feature_extractor_semant(cache):\n",
    "#     @normalize_sent\n",
    "    def inner(doc1, doc2):\n",
    "        feats = {}\n",
    "\n",
    "        def _get_rels(doc):\n",
    "            for tok in doc:\n",
    "                if tok.dep_ == 'ROOT':\n",
    "                    if tok.lemma_ not in cache:\n",
    "                        rels = get_rels(tok.lemma_)\n",
    "                        cache[tok.lemma_] = rels\n",
    "                        return rels\n",
    "                    else:\n",
    "                        return cache[tok.lemma_]\n",
    "\n",
    "\n",
    "        rels1 = _get_rels(doc1)\n",
    "        rels2 = _get_rels(doc2)\n",
    "        root1 = [x for x in doc1 if x.dep_ == 'ROOT'][0].lemma_\n",
    "                    \n",
    "        feats['syn'] = len([x for x in set(rels2['synonyms']) if x == root1])\n",
    "        feats['rel'] = len([x for x in set(rels2['related']) if x == root1])\n",
    "        feats['mean'] = len([x for x in set(rels2['meanings']) if x == root1 or x in rels1['meanings']])\n",
    "        feats['sim'] = len([x for x in set(rels2['similarities']) if x == root1])\n",
    "        feats['hyp'] = len([x for x in set(rels2['hyponyms']) if x == root1])\n",
    "        feats['mer'] = len([x for x in set(rels2['meronyms']) if x == root1])\n",
    "        feats['common'] = len([x for x in set(rels2['common_origins']) if x == root1 or x in rels1['common_origins']])\n",
    "        feats['form'] = len([x for x in set(rels2['forms']) if x == root1 or x in rels1['forms']])\n",
    "        feats['ant'] = len([x for x in set(rels2['antonyms']) if x in rels1['antonyms']])\n",
    "\n",
    "        return feats\n",
    "    return inner\n",
    "\n",
    "\n",
    "# def get_data(dataset, feature_extractor, cache):\n",
    "#     features = []\n",
    "#     labels = []\n",
    "\n",
    "    \n",
    "#     for i, ds in enumerate(dataset):\n",
    "#         sent1 = ds['sentence1']\n",
    "#         sent2 = ds['sentence2']\n",
    "#         md5_1 = hashlib.md5(b'{sent1}')\n",
    "#         md5_2 = hashlib.md5(b'{sent2}')\n",
    "        \n",
    "#         nlp1 = None\n",
    "#         nlp2 = None\n",
    "\n",
    "#         if md5_1 not in cache:\n",
    "#             nlp1 = nlp(sent1)\n",
    "#             cache[md5_1] = nlp1\n",
    "#         else:\n",
    "#             nlp1 = cache[md5_1]\n",
    "#         if md5_2 not in cache:\n",
    "#             nlp2 = nlp(sent2)\n",
    "#             cache[md5_2] = nlp2\n",
    "#         else:\n",
    "#             nlp2 = cache[md5_2]\n",
    "            \n",
    "\n",
    "#         features.append(feature_extractor(nlp1, nlp2))\n",
    "#         labels.append(ds['gold_label'])\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "                        \n",
    "#     return features, labels\n",
    "\n",
    "\n",
    "def get_data(docs, raw_data, feature_extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i, doc_pair in enumerate(docs):\n",
    "        nlp1, nlp2 = doc_pair\n",
    "            \n",
    "        features.append(feature_extractor(nlp1, nlp2))\n",
    "        labels.append(raw_data[i]['gold_label'])\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "                        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def print_result(train_docs, test_docs, train_raw_data, test_raw_data, feature_extractor):\n",
    "    X_train, y_train = get_data(train_docs, train_raw_data, feature_extractor)\n",
    "    X_dev, y_dev = get_data(test_docs, test_raw_data, feature_extractor)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_dev, clf.predict(X_dev)))\n",
    "    \n",
    "# def print_result(train_data, test_data, feature_extractor, cache):\n",
    "#     X_train, y_train = get_data(train_data, feature_extractor, cache)\n",
    "#     X_dev, y_dev = get_data(test_data, feature_extractor, cache)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(classification_report(y_dev, clf.predict(X_dev)))\n",
    "    \n",
    "    \n",
    "def get_concepts(concept):\n",
    "    offset = 0\n",
    "    req = requests.get('http://api.conceptnet.io/c/en/' + concept + '?offset=' + str(offset) + '&limit=100').json()\n",
    "    all_edges = req\n",
    "    return all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (just simply use sentence similarity from spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.51      0.46      3237\n",
      "   entailment       0.43      0.62      0.51      3368\n",
      "      neutral       0.35      0.11      0.17      3219\n",
      "\n",
      "     accuracy                           0.42      9824\n",
      "    macro avg       0.40      0.42      0.38      9824\n",
      " weighted avg       0.40      0.42      0.38      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With NER intersection (-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 8 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.53      0.47      3237\n",
      "   entailment       0.42      0.68      0.52      3368\n",
      "      neutral       0.60      0.07      0.13      3219\n",
      "\n",
      "     accuracy                           0.43      9824\n",
      "    macro avg       0.48      0.43      0.37      9824\n",
      " weighted avg       0.48      0.43      0.38      9824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_ner)\n",
    "# print_result(train_data, dev_data, feature_extractor, {})\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With word intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.55      0.66      0.60      3368\n",
      "      neutral       0.45      0.27      0.34      3219\n",
      "\n",
      "     accuracy                           0.50      9824\n",
      "    macro avg       0.49      0.50      0.49      9824\n",
      " weighted avg       0.49      0.50      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word)\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERB & NOUN tokens similarity (+) !!!! IT TAKES AGES !!!. mab try later\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_verb_nn_sim,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.56      0.66      0.60      3368\n",
      "      neutral       0.45      0.29      0.35      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.50      0.49      9824\n",
      " weighted avg       0.50      0.51      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стеми. Покращення практично нема, а стеммер віджирає багато часу. Викидаємо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.58      0.53      3237\n",
      "   entailment       0.57      0.66      0.61      3368\n",
      "      neutral       0.45      0.29      0.35      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.50      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_stemm\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1677,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.56      0.66      0.60      3368\n",
      "      neutral       0.45      0.29      0.35      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.50      0.49      9824\n",
      " weighted avg       0.50      0.51      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# neg - throw it\n",
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_neg\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1678,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.56      0.52      3237\n",
      "   entailment       0.56      0.65      0.60      3368\n",
      "      neutral       0.45      0.31      0.36      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.50      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1679,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.56      0.52      3237\n",
      "   entailment       0.56      0.65      0.60      3368\n",
      "      neutral       0.45      0.31      0.36      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.50      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_semant({})\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "s1 = 'three bikers stop in town.'\n",
    "s2 = 'The bikers didn\\'t stop in the town.'\n",
    "print(get_intersection([x.head for x in nlp(s1)], [x.head for x in nlp(s2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rel(t1, t2, doc):\n",
    "    if t1.head == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rels(word):\n",
    "    concepts = get_concepts_local(word)\n",
    "#     print(word)\n",
    "\n",
    "    synonyms = []\n",
    "    related = []\n",
    "    forms = []\n",
    "    hyponyms = []\n",
    "    meronyms = []\n",
    "    holonyms = []\n",
    "    capabilities = []\n",
    "    causes = []\n",
    "    antonyms = []\n",
    "    meanings = []\n",
    "    similarities = []\n",
    "    common_origins = []\n",
    "    can_be_done_to = []\n",
    "    \n",
    "    def _check_rel(rel_type, rel_list):\n",
    "        if concept['rel']['label'] == rel_type:\n",
    "            lab = concept['end']['label']\n",
    "            if not lab in rel_list:\n",
    "                rel_list.append(lab)\n",
    "    \n",
    "    for concept in concepts:\n",
    "        _check_rel('Synonym', synonyms)\n",
    "        _check_rel('RelatedTo', related)\n",
    "        _check_rel('FormOf', forms)\n",
    "        _check_rel('IsA', hyponyms)\n",
    "        _check_rel('PartOf', meronyms)\n",
    "        _check_rel('UsedFor', holonyms)\n",
    "        _check_rel('CapableOf', capabilities)\n",
    "        _check_rel('Antonym', antonyms)\n",
    "        _check_rel('DefinedAs', meanings)\n",
    "        _check_rel('SimilarTo', similarities)\n",
    "        _check_rel('EtymologicallyRelatedTo', common_origins)\n",
    "        _check_rel('ReceivesAction', can_be_done_to)\n",
    "        \n",
    "    return {\n",
    "        'synonyms': synonyms,\n",
    "        'related': related,\n",
    "        'forms': forms,\n",
    "        'hyponyms': hyponyms,\n",
    "        'meronyms': meronyms,\n",
    "        'holonyms': holonyms,\n",
    "        'capabilities': capabilities,\n",
    "        'antonyms': antonyms,\n",
    "        'meanings': meanings,\n",
    "        'similarities': similarities,\n",
    "        'common_origins': common_origins,\n",
    "        'can_be_done_to': can_be_done_to,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_syns(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    rels1 = [get_rels(x.lemma_) for x in doc1 if x.dep_ == 'ROOT']\n",
    "    rels2 = [get_rels(x.lemma_) for x in doc2 if x.dep_ == 'ROOT']\n",
    "    \n",
    "    syns1 = [x['synonyms'] for x in rels1]\n",
    "    ss = 0\n",
    "    for rel in rels2:\n",
    "        for syn in rel['synonyms']:\n",
    "            if syn in syns1:\n",
    "                ss += 1\n",
    "    feats['ss'] = ss\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_for_roots(data):\n",
    "    ex_conc = []\n",
    "    def get_concepts_for_sent(sent):\n",
    "        s_conc = None\n",
    "        if os.path.isfile('./concs.txt'):\n",
    "            with open('./concs.txt') as f:\n",
    "                ex_conc = [x.rstrip() for x in f.readlines()]\n",
    "        else:\n",
    "            ex_conc = []\n",
    "        for tok in nlp(sent):\n",
    "            if tok.lemma_ not in ex_conc and tok.dep_ == 'ROOT':\n",
    "                s_conc = get_concepts(tok.lemma_)['edges']\n",
    "                with open('./concs.txt', 'a') as f:\n",
    "                    f.write(tok.lemma_ + '\\n')\n",
    "                ex_conc.append(tok.lemma_)\n",
    "                \n",
    "        return s_conc\n",
    "\n",
    "    conc = []\n",
    "    for i, item in enumerate(data):\n",
    "        conc.append(get_concepts_for_sent(item['sentence1']))\n",
    "        conc.append(get_concepts_for_sent(item['sentence2']))\n",
    "    return conc\n",
    "\n",
    "valid_relations = ['Synonym', 'RelatedTo', 'FormOf', 'IsA', 'PartOf', 'UsedFor', 'CapableOf',\n",
    "                  'Antonym', 'DefinedAs', 'SimilarTo', 'EtymologicallyRelatedTo', 'ReceivesAction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_conc(data, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    chunk = list(chunks(data, 500))\n",
    "    i = 0\n",
    "    for ch in chunk:\n",
    "        train_conc_root = get_concepts_for_roots(ch)\n",
    "        with open(f'./{path}/{i}.json', 'w') as f:\n",
    "            non_null = [x for x in train_conc_root if x]\n",
    "            filtered = [x for conc in non_null for x in conc if x['rel']['label'] in valid_relations \\\n",
    "                        and x['start']['language'] == 'en' and x['end']['language'] == 'en']\n",
    "            json.dump(filtered, f)\n",
    "            i+= 1\n",
    "            \n",
    "\n",
    "def get_concepts_local(word):\n",
    "    return [v for dic in normalized for k, v in dic.items() if k == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_conc(test_data, 'test_conc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_concepts(dirs):\n",
    "    res = []\n",
    "    for d in dirs:\n",
    "        files = os.listdir(d)\n",
    "        for file in files:\n",
    "            with open(os.path.join(d, file)) as f:\n",
    "                cont = json.load(f)\n",
    "                res += cont\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_concepts(concepts):\n",
    "    res = []\n",
    "    for concept in concepts:\n",
    "        if concept['start']['language'] == 'en':\n",
    "            res.append({concept['start']['label'].lower(): concept})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = merge_concepts(['train_conc', 'dev_conc', 'test_conc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonyms': ['human', 'ljudski', 'umano'],\n",
       " 'related': ['human', 'person', 'being', 'humane'],\n",
       " 'forms': ['human'],\n",
       " 'hyponyms': ['a biped', 'a human', 'primate'],\n",
       " 'meronyms': ['an ecology'],\n",
       " 'holonyms': [],\n",
       " 'capabilities': ['think critically',\n",
       "  'laugh about a joke',\n",
       "  'talk to a human',\n",
       "  'torture',\n",
       "  'taste dish'],\n",
       " 'antonyms': [],\n",
       " 'meanins': [],\n",
       " 'similarities': ['human', 'manlike', 'earthborn', 'imperfect'],\n",
       " 'common_origins': [],\n",
       " 'can_be_done_to': []}"
      ]
     },
     "execution_count": 1261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_concepts_local(normalized, 'human')\n",
    "get_rels('human')\n",
    "# # print(all_concepts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_concepts.json', 'w') as f:\n",
    "    json.dump(normalized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = normalize_concepts(all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72021\n"
     ]
    }
   ],
   "source": [
    "print(len(normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = nlp('A person is at a diner, ordering an omelette.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "d = nlp('I love you.')\n",
    "print(dir(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9223372036230035196\n"
     ]
    }
   ],
   "source": [
    "print(d.__hash__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634785270\n"
     ]
    }
   ],
   "source": [
    "print(d.__hash__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotator_labels': ['neutral'], 'captionID': '3416050480.jpg#4', 'gold_label': 'neutral', 'pairID': '3416050480.jpg#4r1n', 'sentence1': 'A person on a horse jumps over a broken down airplane.', 'sentence1_binary_parse': '( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )', 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))', 'sentence2': 'A person is training his horse for a competition.', 'sentence2_binary_parse': '( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )', 'sentence2_parse': '(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotator_labels': ['entailment'], 'captionID': '3416050480.jpg#4', 'gold_label': 'entailment', 'pairID': '3416050480.jpg#4r1e', 'sentence1': 'A person on a horse jumps over a broken down airplane.', 'sentence1_binary_parse': '( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )', 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))', 'sentence2': 'A person is outdoors, on a horse.', 'sentence2_binary_parse': '( ( A person ) ( ( ( ( is outdoors ) , ) ( on ( a horse ) ) ) . ) )', 'sentence2_parse': '(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (ADVP (RB outdoors)) (, ,) (PP (IN on) (NP (DT a) (NN horse)))) (. .)))'}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "549367\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(deduped_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlps(data):\n",
    "    docs = []\n",
    "    for i, sent in enumerate(data):\n",
    "        docs.append((nlp(sent['sentence1']), nlp(sent['sentence2'])))\n",
    "        if i % 1000 == 0:\n",
    "            print('>> doc', i)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> doc 0\n",
      ">> doc 1000\n",
      ">> doc 2000\n",
      ">> doc 3000\n",
      ">> doc 4000\n",
      ">> doc 5000\n",
      ">> doc 6000\n",
      ">> doc 7000\n",
      ">> doc 8000\n",
      ">> doc 9000\n",
      ">> doc 10000\n",
      ">> doc 11000\n",
      ">> doc 12000\n",
      ">> doc 13000\n",
      ">> doc 14000\n",
      ">> doc 15000\n",
      ">> doc 16000\n",
      ">> doc 17000\n",
      ">> doc 18000\n",
      ">> doc 19000\n",
      ">> doc 20000\n",
      ">> doc 21000\n",
      ">> doc 22000\n",
      ">> doc 23000\n",
      ">> doc 24000\n",
      ">> doc 25000\n",
      ">> doc 26000\n",
      ">> doc 27000\n",
      ">> doc 28000\n",
      ">> doc 29000\n",
      ">> doc 30000\n",
      ">> doc 31000\n",
      ">> doc 32000\n",
      ">> doc 33000\n",
      ">> doc 34000\n",
      ">> doc 35000\n",
      ">> doc 36000\n",
      ">> doc 37000\n",
      ">> doc 38000\n",
      ">> doc 39000\n",
      ">> doc 40000\n",
      ">> doc 41000\n",
      ">> doc 42000\n",
      ">> doc 43000\n",
      ">> doc 44000\n",
      ">> doc 45000\n",
      ">> doc 46000\n",
      ">> doc 47000\n",
      ">> doc 48000\n",
      ">> doc 49000\n",
      ">> doc 50000\n",
      ">> doc 51000\n",
      ">> doc 52000\n",
      ">> doc 53000\n",
      ">> doc 54000\n",
      ">> doc 55000\n",
      ">> doc 56000\n",
      ">> doc 57000\n",
      ">> doc 58000\n",
      ">> doc 59000\n",
      ">> doc 60000\n",
      ">> doc 61000\n",
      ">> doc 62000\n",
      ">> doc 63000\n",
      ">> doc 64000\n",
      ">> doc 65000\n",
      ">> doc 66000\n",
      ">> doc 67000\n",
      ">> doc 68000\n",
      ">> doc 69000\n",
      ">> doc 70000\n",
      ">> doc 71000\n",
      ">> doc 72000\n",
      ">> doc 73000\n",
      ">> doc 74000\n",
      ">> doc 75000\n",
      ">> doc 76000\n",
      ">> doc 77000\n",
      ">> doc 78000\n",
      ">> doc 79000\n",
      ">> doc 80000\n",
      ">> doc 81000\n",
      ">> doc 82000\n",
      ">> doc 83000\n",
      ">> doc 84000\n",
      ">> doc 85000\n",
      ">> doc 86000\n",
      ">> doc 87000\n",
      ">> doc 88000\n",
      ">> doc 89000\n",
      ">> doc 90000\n",
      ">> doc 91000\n",
      ">> doc 92000\n",
      ">> doc 93000\n",
      ">> doc 94000\n",
      ">> doc 95000\n",
      ">> doc 96000\n",
      ">> doc 97000\n",
      ">> doc 98000\n",
      ">> doc 99000\n",
      ">> doc 100000\n",
      ">> doc 101000\n",
      ">> doc 102000\n",
      ">> doc 103000\n",
      ">> doc 104000\n",
      ">> doc 105000\n",
      ">> doc 106000\n",
      ">> doc 107000\n",
      ">> doc 108000\n",
      ">> doc 109000\n",
      ">> doc 110000\n",
      ">> doc 111000\n",
      ">> doc 112000\n",
      ">> doc 113000\n",
      ">> doc 114000\n",
      ">> doc 115000\n",
      ">> doc 116000\n",
      ">> doc 117000\n",
      ">> doc 118000\n",
      ">> doc 119000\n",
      ">> doc 120000\n",
      ">> doc 121000\n",
      ">> doc 122000\n",
      ">> doc 123000\n",
      ">> doc 124000\n",
      ">> doc 125000\n",
      ">> doc 126000\n",
      ">> doc 127000\n",
      ">> doc 128000\n",
      ">> doc 129000\n",
      ">> doc 130000\n",
      ">> doc 131000\n",
      ">> doc 132000\n",
      ">> doc 133000\n",
      ">> doc 134000\n",
      ">> doc 135000\n",
      ">> doc 136000\n",
      ">> doc 137000\n",
      ">> doc 138000\n",
      ">> doc 139000\n",
      ">> doc 140000\n",
      ">> doc 141000\n",
      ">> doc 142000\n",
      ">> doc 143000\n",
      ">> doc 144000\n",
      ">> doc 145000\n",
      ">> doc 146000\n",
      ">> doc 147000\n",
      ">> doc 148000\n",
      ">> doc 149000\n",
      ">> doc 150000\n",
      ">> doc 151000\n",
      ">> doc 152000\n",
      ">> doc 153000\n",
      ">> doc 154000\n",
      ">> doc 155000\n",
      ">> doc 156000\n",
      ">> doc 157000\n",
      ">> doc 158000\n",
      ">> doc 159000\n",
      ">> doc 160000\n",
      ">> doc 161000\n",
      ">> doc 162000\n",
      ">> doc 163000\n",
      ">> doc 164000\n",
      ">> doc 165000\n",
      ">> doc 166000\n",
      ">> doc 167000\n",
      ">> doc 168000\n",
      ">> doc 169000\n",
      ">> doc 170000\n",
      ">> doc 171000\n",
      ">> doc 172000\n",
      ">> doc 173000\n",
      ">> doc 174000\n",
      ">> doc 175000\n",
      ">> doc 176000\n",
      ">> doc 177000\n",
      ">> doc 178000\n",
      ">> doc 179000\n",
      ">> doc 180000\n",
      ">> doc 181000\n",
      ">> doc 182000\n",
      ">> doc 183000\n",
      ">> doc 184000\n",
      ">> doc 185000\n",
      ">> doc 186000\n",
      ">> doc 187000\n",
      ">> doc 188000\n",
      ">> doc 189000\n",
      ">> doc 190000\n",
      ">> doc 191000\n",
      ">> doc 192000\n",
      ">> doc 193000\n",
      ">> doc 194000\n",
      ">> doc 195000\n",
      ">> doc 196000\n",
      ">> doc 197000\n",
      ">> doc 198000\n",
      ">> doc 199000\n",
      ">> doc 200000\n",
      ">> doc 201000\n",
      ">> doc 202000\n",
      ">> doc 203000\n",
      ">> doc 204000\n",
      ">> doc 205000\n",
      ">> doc 206000\n",
      ">> doc 207000\n",
      ">> doc 208000\n",
      ">> doc 209000\n",
      ">> doc 210000\n",
      ">> doc 211000\n",
      ">> doc 212000\n",
      ">> doc 213000\n",
      ">> doc 214000\n",
      ">> doc 215000\n",
      ">> doc 216000\n",
      ">> doc 217000\n",
      ">> doc 218000\n",
      ">> doc 219000\n",
      ">> doc 220000\n",
      ">> doc 221000\n",
      ">> doc 222000\n",
      ">> doc 223000\n",
      ">> doc 224000\n",
      ">> doc 225000\n",
      ">> doc 226000\n",
      ">> doc 227000\n",
      ">> doc 228000\n",
      ">> doc 229000\n",
      ">> doc 230000\n",
      ">> doc 231000\n",
      ">> doc 232000\n",
      ">> doc 233000\n",
      ">> doc 234000\n",
      ">> doc 235000\n",
      ">> doc 236000\n",
      ">> doc 237000\n",
      ">> doc 238000\n",
      ">> doc 239000\n",
      ">> doc 240000\n",
      ">> doc 241000\n",
      ">> doc 242000\n",
      ">> doc 243000\n",
      ">> doc 244000\n",
      ">> doc 245000\n",
      ">> doc 246000\n",
      ">> doc 247000\n",
      ">> doc 248000\n",
      ">> doc 249000\n",
      ">> doc 250000\n",
      ">> doc 251000\n",
      ">> doc 252000\n",
      ">> doc 253000\n",
      ">> doc 254000\n",
      ">> doc 255000\n",
      ">> doc 256000\n",
      ">> doc 257000\n",
      ">> doc 258000\n",
      ">> doc 259000\n",
      ">> doc 260000\n",
      ">> doc 261000\n",
      ">> doc 262000\n",
      ">> doc 263000\n",
      ">> doc 264000\n",
      ">> doc 265000\n",
      ">> doc 266000\n",
      ">> doc 267000\n",
      ">> doc 268000\n",
      ">> doc 269000\n",
      ">> doc 270000\n",
      ">> doc 271000\n",
      ">> doc 272000\n",
      ">> doc 273000\n",
      ">> doc 274000\n",
      ">> doc 275000\n",
      ">> doc 276000\n",
      ">> doc 277000\n",
      ">> doc 278000\n",
      ">> doc 279000\n",
      ">> doc 280000\n",
      ">> doc 281000\n",
      ">> doc 282000\n",
      ">> doc 283000\n",
      ">> doc 284000\n",
      ">> doc 285000\n",
      ">> doc 286000\n",
      ">> doc 287000\n",
      ">> doc 288000\n",
      ">> doc 289000\n",
      ">> doc 290000\n",
      ">> doc 291000\n",
      ">> doc 292000\n",
      ">> doc 293000\n",
      ">> doc 294000\n",
      ">> doc 295000\n",
      ">> doc 296000\n",
      ">> doc 297000\n",
      ">> doc 298000\n",
      ">> doc 299000\n",
      ">> doc 300000\n",
      ">> doc 301000\n",
      ">> doc 302000\n",
      ">> doc 303000\n",
      ">> doc 304000\n",
      ">> doc 305000\n",
      ">> doc 306000\n",
      ">> doc 307000\n",
      ">> doc 308000\n",
      ">> doc 309000\n",
      ">> doc 310000\n",
      ">> doc 311000\n",
      ">> doc 312000\n",
      ">> doc 313000\n",
      ">> doc 314000\n",
      ">> doc 315000\n",
      ">> doc 316000\n",
      ">> doc 317000\n",
      ">> doc 318000\n",
      ">> doc 319000\n",
      ">> doc 320000\n",
      ">> doc 321000\n",
      ">> doc 322000\n",
      ">> doc 323000\n",
      ">> doc 324000\n",
      ">> doc 325000\n",
      ">> doc 326000\n",
      ">> doc 327000\n",
      ">> doc 328000\n",
      ">> doc 329000\n",
      ">> doc 330000\n",
      ">> doc 331000\n",
      ">> doc 332000\n",
      ">> doc 333000\n",
      ">> doc 334000\n",
      ">> doc 335000\n",
      ">> doc 336000\n",
      ">> doc 337000\n",
      ">> doc 338000\n",
      ">> doc 339000\n",
      ">> doc 340000\n",
      ">> doc 341000\n",
      ">> doc 342000\n",
      ">> doc 343000\n",
      ">> doc 344000\n",
      ">> doc 345000\n",
      ">> doc 346000\n",
      ">> doc 347000\n",
      ">> doc 348000\n",
      ">> doc 349000\n",
      ">> doc 350000\n",
      ">> doc 351000\n",
      ">> doc 352000\n",
      ">> doc 353000\n",
      ">> doc 354000\n",
      ">> doc 355000\n",
      ">> doc 356000\n",
      ">> doc 357000\n",
      ">> doc 358000\n",
      ">> doc 359000\n",
      ">> doc 360000\n",
      ">> doc 361000\n",
      ">> doc 362000\n",
      ">> doc 363000\n",
      ">> doc 364000\n",
      ">> doc 365000\n",
      ">> doc 366000\n",
      ">> doc 367000\n",
      ">> doc 368000\n",
      ">> doc 369000\n",
      ">> doc 370000\n",
      ">> doc 371000\n",
      ">> doc 372000\n",
      ">> doc 373000\n",
      ">> doc 374000\n",
      ">> doc 375000\n",
      ">> doc 376000\n",
      ">> doc 377000\n",
      ">> doc 378000\n",
      ">> doc 379000\n",
      ">> doc 380000\n",
      ">> doc 381000\n",
      ">> doc 382000\n",
      ">> doc 383000\n",
      ">> doc 384000\n",
      ">> doc 385000\n",
      ">> doc 386000\n",
      ">> doc 387000\n",
      ">> doc 388000\n",
      ">> doc 389000\n",
      ">> doc 390000\n",
      ">> doc 391000\n",
      ">> doc 392000\n",
      ">> doc 393000\n",
      ">> doc 394000\n",
      ">> doc 395000\n",
      ">> doc 396000\n",
      ">> doc 397000\n",
      ">> doc 398000\n",
      ">> doc 399000\n",
      ">> doc 400000\n",
      ">> doc 401000\n",
      ">> doc 402000\n",
      ">> doc 403000\n",
      ">> doc 404000\n",
      ">> doc 405000\n",
      ">> doc 406000\n",
      ">> doc 407000\n",
      ">> doc 408000\n",
      ">> doc 409000\n",
      ">> doc 410000\n",
      ">> doc 411000\n",
      ">> doc 412000\n",
      ">> doc 413000\n",
      ">> doc 414000\n",
      ">> doc 415000\n",
      ">> doc 416000\n",
      ">> doc 417000\n",
      ">> doc 418000\n",
      ">> doc 419000\n",
      ">> doc 420000\n",
      ">> doc 421000\n",
      ">> doc 422000\n",
      ">> doc 423000\n",
      ">> doc 424000\n",
      ">> doc 425000\n",
      ">> doc 426000\n",
      ">> doc 427000\n",
      ">> doc 428000\n",
      ">> doc 429000\n",
      ">> doc 430000\n",
      ">> doc 431000\n",
      ">> doc 432000\n",
      ">> doc 433000\n",
      ">> doc 434000\n",
      ">> doc 435000\n",
      ">> doc 436000\n",
      ">> doc 437000\n",
      ">> doc 438000\n",
      ">> doc 439000\n",
      ">> doc 440000\n",
      ">> doc 441000\n",
      ">> doc 442000\n",
      ">> doc 443000\n",
      ">> doc 444000\n",
      ">> doc 445000\n",
      ">> doc 446000\n",
      ">> doc 447000\n",
      ">> doc 448000\n",
      ">> doc 449000\n",
      ">> doc 450000\n",
      ">> doc 451000\n",
      ">> doc 452000\n",
      ">> doc 453000\n",
      ">> doc 454000\n",
      ">> doc 455000\n",
      ">> doc 456000\n",
      ">> doc 457000\n",
      ">> doc 458000\n",
      ">> doc 459000\n",
      ">> doc 460000\n",
      ">> doc 461000\n",
      ">> doc 462000\n",
      ">> doc 463000\n",
      ">> doc 464000\n",
      ">> doc 465000\n",
      ">> doc 466000\n",
      ">> doc 467000\n",
      ">> doc 468000\n",
      ">> doc 469000\n",
      ">> doc 470000\n",
      ">> doc 471000\n",
      ">> doc 472000\n",
      ">> doc 473000\n",
      ">> doc 474000\n",
      ">> doc 475000\n",
      ">> doc 476000\n",
      ">> doc 477000\n",
      ">> doc 478000\n",
      ">> doc 479000\n",
      ">> doc 480000\n",
      ">> doc 481000\n",
      ">> doc 482000\n",
      ">> doc 483000\n",
      ">> doc 484000\n",
      ">> doc 485000\n",
      ">> doc 486000\n",
      ">> doc 487000\n",
      ">> doc 488000\n",
      ">> doc 489000\n",
      ">> doc 490000\n",
      ">> doc 491000\n",
      ">> doc 492000\n",
      ">> doc 493000\n",
      ">> doc 494000\n",
      ">> doc 495000\n",
      ">> doc 496000\n",
      ">> doc 497000\n",
      ">> doc 498000\n",
      ">> doc 499000\n",
      ">> doc 500000\n",
      ">> doc 501000\n",
      ">> doc 502000\n",
      ">> doc 503000\n",
      ">> doc 504000\n",
      ">> doc 505000\n",
      ">> doc 506000\n",
      ">> doc 507000\n",
      ">> doc 508000\n",
      ">> doc 509000\n",
      ">> doc 510000\n",
      ">> doc 511000\n",
      ">> doc 512000\n",
      ">> doc 513000\n",
      ">> doc 514000\n",
      ">> doc 515000\n",
      ">> doc 516000\n",
      ">> doc 517000\n",
      ">> doc 518000\n",
      ">> doc 519000\n",
      ">> doc 520000\n",
      ">> doc 521000\n",
      ">> doc 522000\n",
      ">> doc 523000\n",
      ">> doc 524000\n",
      ">> doc 525000\n",
      ">> doc 526000\n",
      ">> doc 527000\n",
      ">> doc 528000\n",
      ">> doc 529000\n",
      ">> doc 530000\n",
      ">> doc 531000\n",
      ">> doc 532000\n",
      ">> doc 533000\n",
      ">> doc 534000\n",
      ">> doc 535000\n",
      ">> doc 536000\n",
      ">> doc 537000\n",
      ">> doc 538000\n",
      ">> doc 539000\n",
      ">> doc 540000\n",
      ">> doc 541000\n",
      ">> doc 542000\n",
      ">> doc 543000\n",
      ">> doc 544000\n",
      ">> doc 545000\n",
      ">> doc 546000\n",
      ">> doc 547000\n",
      ">> doc 548000\n",
      ">> doc 549000\n"
     ]
    }
   ],
   "source": [
    "docs = get_nlps(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1564-bd587f1b1142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1564-bd587f1b1142>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_docs_json = [[x.to_json(), y.to_json()] for x, y in doc.values() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove\n",
    "def get_doc_pairs(docs):\n",
    "    pairs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        for x, y in doc.values():\n",
    "            pairs.append((x, y))\n",
    "            if i % 1000 == 0:\n",
    "                print('>> doc', i)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> doc 0\n",
      ">> doc 1000\n",
      ">> doc 2000\n",
      ">> doc 3000\n",
      ">> doc 4000\n",
      ">> doc 5000\n",
      ">> doc 6000\n",
      ">> doc 7000\n",
      ">> doc 8000\n",
      ">> doc 9000\n",
      ">> doc 10000\n",
      ">> doc 11000\n",
      ">> doc 12000\n",
      ">> doc 13000\n",
      ">> doc 14000\n",
      ">> doc 15000\n",
      ">> doc 16000\n",
      ">> doc 17000\n",
      ">> doc 18000\n",
      ">> doc 19000\n",
      ">> doc 20000\n",
      ">> doc 21000\n",
      ">> doc 22000\n",
      ">> doc 23000\n",
      ">> doc 24000\n",
      ">> doc 25000\n",
      ">> doc 26000\n",
      ">> doc 27000\n",
      ">> doc 28000\n",
      ">> doc 29000\n",
      ">> doc 30000\n",
      ">> doc 31000\n",
      ">> doc 32000\n",
      ">> doc 33000\n",
      ">> doc 34000\n",
      ">> doc 35000\n",
      ">> doc 36000\n",
      ">> doc 37000\n",
      ">> doc 38000\n",
      ">> doc 39000\n",
      ">> doc 40000\n",
      ">> doc 41000\n",
      ">> doc 42000\n",
      ">> doc 43000\n",
      ">> doc 44000\n",
      ">> doc 45000\n",
      ">> doc 46000\n",
      ">> doc 47000\n",
      ">> doc 48000\n",
      ">> doc 49000\n",
      ">> doc 50000\n",
      ">> doc 51000\n",
      ">> doc 52000\n",
      ">> doc 53000\n",
      ">> doc 54000\n",
      ">> doc 55000\n",
      ">> doc 56000\n",
      ">> doc 57000\n",
      ">> doc 58000\n",
      ">> doc 59000\n",
      ">> doc 60000\n",
      ">> doc 61000\n",
      ">> doc 62000\n",
      ">> doc 63000\n",
      ">> doc 64000\n",
      ">> doc 65000\n",
      ">> doc 66000\n",
      ">> doc 67000\n",
      ">> doc 68000\n",
      ">> doc 69000\n",
      ">> doc 70000\n",
      ">> doc 71000\n",
      ">> doc 72000\n",
      ">> doc 73000\n",
      ">> doc 74000\n",
      ">> doc 75000\n",
      ">> doc 76000\n",
      ">> doc 77000\n",
      ">> doc 78000\n",
      ">> doc 79000\n",
      ">> doc 80000\n",
      ">> doc 81000\n",
      ">> doc 82000\n",
      ">> doc 83000\n",
      ">> doc 84000\n",
      ">> doc 85000\n",
      ">> doc 86000\n",
      ">> doc 87000\n",
      ">> doc 88000\n",
      ">> doc 89000\n",
      ">> doc 90000\n",
      ">> doc 91000\n",
      ">> doc 92000\n",
      ">> doc 93000\n",
      ">> doc 94000\n",
      ">> doc 95000\n",
      ">> doc 96000\n",
      ">> doc 97000\n",
      ">> doc 98000\n",
      ">> doc 99000\n",
      ">> doc 100000\n",
      ">> doc 101000\n",
      ">> doc 102000\n",
      ">> doc 103000\n",
      ">> doc 104000\n",
      ">> doc 105000\n",
      ">> doc 106000\n",
      ">> doc 107000\n",
      ">> doc 108000\n",
      ">> doc 109000\n",
      ">> doc 110000\n",
      ">> doc 111000\n",
      ">> doc 112000\n",
      ">> doc 113000\n",
      ">> doc 114000\n",
      ">> doc 115000\n",
      ">> doc 116000\n",
      ">> doc 117000\n",
      ">> doc 118000\n",
      ">> doc 119000\n",
      ">> doc 120000\n",
      ">> doc 121000\n",
      ">> doc 122000\n",
      ">> doc 123000\n",
      ">> doc 124000\n",
      ">> doc 125000\n",
      ">> doc 126000\n",
      ">> doc 127000\n",
      ">> doc 128000\n",
      ">> doc 129000\n",
      ">> doc 130000\n",
      ">> doc 131000\n",
      ">> doc 132000\n",
      ">> doc 133000\n",
      ">> doc 134000\n",
      ">> doc 135000\n",
      ">> doc 136000\n",
      ">> doc 137000\n",
      ">> doc 138000\n",
      ">> doc 139000\n",
      ">> doc 140000\n",
      ">> doc 141000\n",
      ">> doc 142000\n",
      ">> doc 143000\n",
      ">> doc 144000\n",
      ">> doc 145000\n",
      ">> doc 146000\n",
      ">> doc 147000\n",
      ">> doc 148000\n",
      ">> doc 149000\n",
      ">> doc 150000\n",
      ">> doc 151000\n",
      ">> doc 152000\n",
      ">> doc 153000\n",
      ">> doc 154000\n",
      ">> doc 155000\n",
      ">> doc 156000\n",
      ">> doc 157000\n",
      ">> doc 158000\n",
      ">> doc 159000\n",
      ">> doc 160000\n",
      ">> doc 161000\n",
      ">> doc 162000\n",
      ">> doc 163000\n",
      ">> doc 164000\n",
      ">> doc 165000\n",
      ">> doc 166000\n",
      ">> doc 167000\n",
      ">> doc 168000\n",
      ">> doc 169000\n",
      ">> doc 170000\n",
      ">> doc 171000\n",
      ">> doc 172000\n",
      ">> doc 173000\n",
      ">> doc 174000\n",
      ">> doc 175000\n",
      ">> doc 176000\n",
      ">> doc 177000\n",
      ">> doc 178000\n",
      ">> doc 179000\n",
      ">> doc 180000\n",
      ">> doc 181000\n",
      ">> doc 182000\n",
      ">> doc 183000\n",
      ">> doc 184000\n",
      ">> doc 185000\n",
      ">> doc 186000\n",
      ">> doc 187000\n",
      ">> doc 188000\n",
      ">> doc 189000\n",
      ">> doc 190000\n",
      ">> doc 191000\n",
      ">> doc 192000\n",
      ">> doc 193000\n",
      ">> doc 194000\n",
      ">> doc 195000\n",
      ">> doc 196000\n",
      ">> doc 197000\n",
      ">> doc 198000\n",
      ">> doc 199000\n",
      ">> doc 200000\n",
      ">> doc 201000\n",
      ">> doc 202000\n",
      ">> doc 203000\n",
      ">> doc 204000\n",
      ">> doc 205000\n",
      ">> doc 206000\n",
      ">> doc 207000\n",
      ">> doc 208000\n",
      ">> doc 209000\n",
      ">> doc 210000\n",
      ">> doc 211000\n",
      ">> doc 212000\n",
      ">> doc 213000\n",
      ">> doc 214000\n",
      ">> doc 215000\n",
      ">> doc 216000\n",
      ">> doc 217000\n",
      ">> doc 218000\n",
      ">> doc 219000\n",
      ">> doc 220000\n",
      ">> doc 221000\n",
      ">> doc 222000\n",
      ">> doc 223000\n",
      ">> doc 224000\n",
      ">> doc 225000\n",
      ">> doc 226000\n",
      ">> doc 227000\n",
      ">> doc 228000\n",
      ">> doc 229000\n",
      ">> doc 230000\n",
      ">> doc 231000\n",
      ">> doc 232000\n",
      ">> doc 233000\n",
      ">> doc 234000\n",
      ">> doc 235000\n",
      ">> doc 236000\n",
      ">> doc 237000\n",
      ">> doc 238000\n",
      ">> doc 239000\n",
      ">> doc 240000\n",
      ">> doc 241000\n",
      ">> doc 242000\n",
      ">> doc 243000\n",
      ">> doc 244000\n",
      ">> doc 245000\n",
      ">> doc 246000\n",
      ">> doc 247000\n",
      ">> doc 248000\n",
      ">> doc 249000\n",
      ">> doc 250000\n",
      ">> doc 251000\n",
      ">> doc 252000\n",
      ">> doc 253000\n",
      ">> doc 254000\n",
      ">> doc 255000\n",
      ">> doc 256000\n",
      ">> doc 257000\n",
      ">> doc 258000\n",
      ">> doc 259000\n",
      ">> doc 260000\n",
      ">> doc 261000\n",
      ">> doc 262000\n",
      ">> doc 263000\n",
      ">> doc 264000\n",
      ">> doc 265000\n",
      ">> doc 266000\n",
      ">> doc 267000\n",
      ">> doc 268000\n",
      ">> doc 269000\n",
      ">> doc 270000\n",
      ">> doc 271000\n",
      ">> doc 272000\n",
      ">> doc 273000\n",
      ">> doc 274000\n",
      ">> doc 275000\n",
      ">> doc 276000\n",
      ">> doc 277000\n",
      ">> doc 278000\n",
      ">> doc 279000\n",
      ">> doc 280000\n",
      ">> doc 281000\n",
      ">> doc 282000\n",
      ">> doc 283000\n",
      ">> doc 284000\n",
      ">> doc 285000\n",
      ">> doc 286000\n",
      ">> doc 287000\n",
      ">> doc 288000\n",
      ">> doc 289000\n",
      ">> doc 290000\n",
      ">> doc 291000\n",
      ">> doc 292000\n",
      ">> doc 293000\n",
      ">> doc 294000\n",
      ">> doc 295000\n",
      ">> doc 296000\n",
      ">> doc 297000\n",
      ">> doc 298000\n",
      ">> doc 299000\n",
      ">> doc 300000\n",
      ">> doc 301000\n",
      ">> doc 302000\n",
      ">> doc 303000\n",
      ">> doc 304000\n",
      ">> doc 305000\n",
      ">> doc 306000\n",
      ">> doc 307000\n",
      ">> doc 308000\n",
      ">> doc 309000\n",
      ">> doc 310000\n",
      ">> doc 311000\n",
      ">> doc 312000\n",
      ">> doc 313000\n",
      ">> doc 314000\n",
      ">> doc 315000\n",
      ">> doc 316000\n",
      ">> doc 317000\n",
      ">> doc 318000\n",
      ">> doc 319000\n",
      ">> doc 320000\n",
      ">> doc 321000\n",
      ">> doc 322000\n",
      ">> doc 323000\n",
      ">> doc 324000\n",
      ">> doc 325000\n",
      ">> doc 326000\n",
      ">> doc 327000\n",
      ">> doc 328000\n",
      ">> doc 329000\n",
      ">> doc 330000\n",
      ">> doc 331000\n",
      ">> doc 332000\n",
      ">> doc 333000\n",
      ">> doc 334000\n",
      ">> doc 335000\n",
      ">> doc 336000\n",
      ">> doc 337000\n",
      ">> doc 338000\n",
      ">> doc 339000\n",
      ">> doc 340000\n",
      ">> doc 341000\n",
      ">> doc 342000\n",
      ">> doc 343000\n",
      ">> doc 344000\n",
      ">> doc 345000\n",
      ">> doc 346000\n",
      ">> doc 347000\n",
      ">> doc 348000\n",
      ">> doc 349000\n",
      ">> doc 350000\n",
      ">> doc 351000\n",
      ">> doc 352000\n",
      ">> doc 353000\n",
      ">> doc 354000\n",
      ">> doc 355000\n",
      ">> doc 356000\n",
      ">> doc 357000\n",
      ">> doc 358000\n",
      ">> doc 359000\n",
      ">> doc 360000\n",
      ">> doc 361000\n",
      ">> doc 362000\n",
      ">> doc 363000\n",
      ">> doc 364000\n",
      ">> doc 365000\n",
      ">> doc 366000\n",
      ">> doc 367000\n",
      ">> doc 368000\n",
      ">> doc 369000\n",
      ">> doc 370000\n",
      ">> doc 371000\n",
      ">> doc 372000\n",
      ">> doc 373000\n",
      ">> doc 374000\n",
      ">> doc 375000\n",
      ">> doc 376000\n",
      ">> doc 377000\n",
      ">> doc 378000\n",
      ">> doc 379000\n",
      ">> doc 380000\n",
      ">> doc 381000\n",
      ">> doc 382000\n",
      ">> doc 383000\n",
      ">> doc 384000\n",
      ">> doc 385000\n",
      ">> doc 386000\n",
      ">> doc 387000\n",
      ">> doc 388000\n",
      ">> doc 389000\n",
      ">> doc 390000\n",
      ">> doc 391000\n",
      ">> doc 392000\n",
      ">> doc 393000\n",
      ">> doc 394000\n",
      ">> doc 395000\n",
      ">> doc 396000\n",
      ">> doc 397000\n",
      ">> doc 398000\n",
      ">> doc 399000\n",
      ">> doc 400000\n",
      ">> doc 401000\n",
      ">> doc 402000\n",
      ">> doc 403000\n",
      ">> doc 404000\n",
      ">> doc 405000\n",
      ">> doc 406000\n",
      ">> doc 407000\n",
      ">> doc 408000\n",
      ">> doc 409000\n",
      ">> doc 410000\n",
      ">> doc 411000\n",
      ">> doc 412000\n",
      ">> doc 413000\n",
      ">> doc 414000\n",
      ">> doc 415000\n",
      ">> doc 416000\n",
      ">> doc 417000\n",
      ">> doc 418000\n",
      ">> doc 419000\n",
      ">> doc 420000\n",
      ">> doc 421000\n",
      ">> doc 422000\n",
      ">> doc 423000\n",
      ">> doc 424000\n",
      ">> doc 425000\n",
      ">> doc 426000\n",
      ">> doc 427000\n",
      ">> doc 428000\n",
      ">> doc 429000\n",
      ">> doc 430000\n",
      ">> doc 431000\n",
      ">> doc 432000\n",
      ">> doc 433000\n",
      ">> doc 434000\n",
      ">> doc 435000\n",
      ">> doc 436000\n",
      ">> doc 437000\n",
      ">> doc 438000\n",
      ">> doc 439000\n",
      ">> doc 440000\n",
      ">> doc 441000\n",
      ">> doc 442000\n",
      ">> doc 443000\n",
      ">> doc 444000\n",
      ">> doc 445000\n",
      ">> doc 446000\n",
      ">> doc 447000\n",
      ">> doc 448000\n",
      ">> doc 449000\n",
      ">> doc 450000\n",
      ">> doc 451000\n",
      ">> doc 452000\n",
      ">> doc 453000\n",
      ">> doc 454000\n",
      ">> doc 455000\n",
      ">> doc 456000\n",
      ">> doc 457000\n",
      ">> doc 458000\n",
      ">> doc 459000\n",
      ">> doc 460000\n",
      ">> doc 461000\n",
      ">> doc 462000\n",
      ">> doc 463000\n",
      ">> doc 464000\n",
      ">> doc 465000\n",
      ">> doc 466000\n",
      ">> doc 467000\n",
      ">> doc 468000\n",
      ">> doc 469000\n",
      ">> doc 470000\n",
      ">> doc 471000\n",
      ">> doc 472000\n",
      ">> doc 473000\n",
      ">> doc 474000\n",
      ">> doc 475000\n",
      ">> doc 476000\n",
      ">> doc 477000\n",
      ">> doc 478000\n",
      ">> doc 479000\n",
      ">> doc 480000\n",
      ">> doc 481000\n",
      ">> doc 482000\n",
      ">> doc 483000\n",
      ">> doc 484000\n",
      ">> doc 485000\n",
      ">> doc 486000\n",
      ">> doc 487000\n",
      ">> doc 488000\n",
      ">> doc 489000\n",
      ">> doc 490000\n",
      ">> doc 491000\n",
      ">> doc 492000\n",
      ">> doc 493000\n",
      ">> doc 494000\n",
      ">> doc 495000\n",
      ">> doc 496000\n",
      ">> doc 497000\n",
      ">> doc 498000\n",
      ">> doc 499000\n",
      ">> doc 500000\n",
      ">> doc 501000\n",
      ">> doc 502000\n",
      ">> doc 503000\n",
      ">> doc 504000\n",
      ">> doc 505000\n",
      ">> doc 506000\n",
      ">> doc 507000\n",
      ">> doc 508000\n",
      ">> doc 509000\n",
      ">> doc 510000\n",
      ">> doc 511000\n",
      ">> doc 512000\n",
      ">> doc 513000\n",
      ">> doc 514000\n",
      ">> doc 515000\n",
      ">> doc 516000\n",
      ">> doc 517000\n",
      ">> doc 518000\n",
      ">> doc 519000\n",
      ">> doc 520000\n",
      ">> doc 521000\n",
      ">> doc 522000\n",
      ">> doc 523000\n",
      ">> doc 524000\n",
      ">> doc 525000\n",
      ">> doc 526000\n",
      ">> doc 527000\n",
      ">> doc 528000\n",
      ">> doc 529000\n",
      ">> doc 530000\n",
      ">> doc 531000\n",
      ">> doc 532000\n",
      ">> doc 533000\n",
      ">> doc 534000\n",
      ">> doc 535000\n",
      ">> doc 536000\n",
      ">> doc 537000\n",
      ">> doc 538000\n",
      ">> doc 539000\n",
      ">> doc 540000\n",
      ">> doc 541000\n",
      ">> doc 542000\n",
      ">> doc 543000\n",
      ">> doc 544000\n",
      ">> doc 545000\n",
      ">> doc 546000\n",
      ">> doc 547000\n",
      ">> doc 548000\n",
      ">> doc 549000\n"
     ]
    }
   ],
   "source": [
    "train_docs = get_doc_pairs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> doc 0\n",
      ">> doc 1000\n",
      ">> doc 2000\n",
      ">> doc 3000\n",
      ">> doc 4000\n",
      ">> doc 5000\n",
      ">> doc 6000\n",
      ">> doc 7000\n",
      ">> doc 8000\n",
      ">> doc 9000\n"
     ]
    }
   ],
   "source": [
    "dev_docs = get_nlps(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> doc 0\n",
      ">> doc 1000\n",
      ">> doc 2000\n",
      ">> doc 3000\n",
      ">> doc 4000\n",
      ">> doc 5000\n",
      ">> doc 6000\n",
      ">> doc 7000\n",
      ">> doc 8000\n",
      ">> doc 9000\n"
     ]
    }
   ],
   "source": [
    "test_docs = get_nlps(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A person on a horse jumps over a broken down airplane.,\n",
       " A person is training his horse for a competition.)"
      ]
     },
     "execution_count": 1629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
