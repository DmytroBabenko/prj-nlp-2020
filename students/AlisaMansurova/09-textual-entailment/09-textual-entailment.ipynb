{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import hashlib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import en_core_web_md\n",
    "# from spacy.tokens import Doc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknowns(data):\n",
    "    return [x for x in data if x['gold_label'] != '-']\n",
    "\n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_train.jsonl') as f:\n",
    "    train_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_dev.jsonl') as f:\n",
    "    dev_data = filter_unknowns([json.loads(line) for line in f.readlines()])\n",
    "    \n",
    "with open('../../../../../corpora/snli_1.0/snli_1.0_test.jsonl') as f:\n",
    "    test_data = filter_unknowns([json.loads(line) for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    def inner(*arg):\n",
    "        res = {}\n",
    "        for f in funcs:\n",
    "            res.update(f(*arg))\n",
    "        return res\n",
    "    return inner\n",
    "\n",
    "def get_nlp_normalized(sent):\n",
    "    doc = nlp(sent)\n",
    "    neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "    neg_processed = []\n",
    "    neg_ind = 100500\n",
    "    for tok in doc:\n",
    "        if tok.lower_ in neg:\n",
    "            neg_ind = tok.i\n",
    "        elif tok.pos_ == 'PUNCT':\n",
    "            neg_ind = 100500\n",
    "            neg_processed.append(tok.text)\n",
    "        elif tok.i > neg_ind:\n",
    "            neg_processed.append('NOT_' + tok.lemma_)\n",
    "        else:\n",
    "            neg_processed.append(tok.text)\n",
    "    return nlp(' '.join(neg_processed))\n",
    "\n",
    "\n",
    "def filter_stop_words(doc):\n",
    "    return [x for x in doc if not (x.pos_ == 'DET' or x.pos_ == 'NUM' or x.is_stop and x.dep_ != 'ROOT')]\n",
    "\n",
    "\n",
    "def normalize_sent(func):\n",
    "    def inner(s1, s2): \n",
    "        return func(filter_stop_words(s1), filter_stop_words(s2)) \n",
    "    return inner\n",
    "\n",
    "\n",
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=-1))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "# TODO: use UAS for deps, compare UAS\n",
    "def get_intersection(ents1, ents2):\n",
    "    setA = set(ents1)\n",
    "    setB = set(ents2)\n",
    "    universe = setA | setB\n",
    "#     if not universe:\n",
    "    if not setB:\n",
    "        return 'NONE'\n",
    "\n",
    "#     return len(setA & setB)/(len(universe))\n",
    "    return len(setA & setB)/(len(setB))\n",
    "\n",
    "# TODO: try to use, see if it's better than ^^\n",
    "def get_intersection_alt(ents1, ents2):\n",
    "    if not ents2:\n",
    "        return 'NONE'\n",
    "    i = [x for x in ents2 if x in ents1]\n",
    "\n",
    "    return len(i)/(len(setB))\n",
    "\n",
    "\n",
    "def get_tokens_similarity(toks1, toks2):\n",
    "    setA = set(toks1)\n",
    "    setB = set(toks2)\n",
    "    universe = set(toks1) | set(toks2)\n",
    "    sim = [x.similarity(y) for x in setA for y in setB if x.has_vector and y.has_vector]\n",
    "    return len(sim)/(len(universe))\n",
    "\n",
    "\n",
    "def get_ngrams(text):\n",
    "    res = []\n",
    "    for i in range(0, len(text), 3):\n",
    "        if i > 0 and i + 3 <= len(text):\n",
    "            res.append(text[i:i + 3])\n",
    "        elif i > 0 and i + 3 > len(text):\n",
    "            res.append(text[i:i + 3] + '</S>')\n",
    "        else:\n",
    "            res.append('<S>' + text[i:i + 3])\n",
    "    return res\n",
    "\n",
    "\n",
    "def feature_extractor_base(doc1, doc2):\n",
    "    feats = {}\n",
    "    feats['similarity'] = doc1.similarity(doc2)\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# It makes it a bit worse\n",
    "# TODO: investigate and improve\n",
    "@normalize_sent\n",
    "def feature_extractor_ner(doc1, doc2):\n",
    "    def _inner(doc):\n",
    "        return [x.ent_type_ for x in doc]\n",
    "    feats = {}\n",
    "\n",
    "    feats['ner'] = get_intersection(_inner(doc1), _inner(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_word(doc1, doc2):\n",
    "    def _lemm(doc):\n",
    "        return [x.lemma_ for x in doc]\n",
    "    def _noun(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'NOUN']\n",
    "    def _verb(doc):\n",
    "        return [x.lemma_ for x in doc if x.pos_ == 'VERB']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['lemma'] = get_intersection(_lemm(doc1), _lemm(doc2))\n",
    "    feats['noun'] = get_intersection(_noun(doc1), _noun(doc2))\n",
    "    feats['verb'] = get_intersection(_verb(doc1), _verb(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_spacy_sim(doc1, doc2):\n",
    "    feats = {}\n",
    "\n",
    "    feats['similar'] = get_tokens_similarity(doc1, doc2)\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_ngrams(doc1, doc2):\n",
    "    def _ng_pos(doc):\n",
    "        return get_ngrams(' '.join([x.pos_ for x in doc]))\n",
    "    def _ng_dep(doc):\n",
    "        return get_ngrams(' '.join([x.dep_ for x in doc]))\n",
    "    def _ng_lemma(doc):\n",
    "        return get_ngrams(' '.join([x.lemma_ for x in doc]))\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['ngr-pos'] = get_intersection(_ng_pos(doc1), _ng_pos(doc2))\n",
    "    feats['ngr-dep'] = get_intersection(_ng_dep(doc1), _ng_dep(doc2))\n",
    "    feats['ngr-lemma'] = get_intersection(_ng_lemma(doc1), _ng_lemma(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_stemm(doc1, doc2):\n",
    "    stemmer = PorterStemmer()\n",
    "    def _stem_v(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'VERB']\n",
    "    def _stem_n(doc):\n",
    "        return [stemmer.stem(x.text) for x in doc if x.pos_ == 'NOUN']\n",
    "\n",
    "    feats = {}\n",
    "\n",
    "    feats['stemm-v'] = get_intersection(_stem_v(doc1), _stem_v(doc2))\n",
    "    feats['stemm-n'] = get_intersection(_stem_n(doc1), _stem_n(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "@normalize_sent\n",
    "def feature_extractor_neg(doc1, doc2):\n",
    "    def _get_neg(doc):\n",
    "        neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "        neg_processed = []\n",
    "        neg_ind = 100500\n",
    "        for tok in doc:\n",
    "            if tok.lower_ in neg:\n",
    "                neg_ind = tok.i\n",
    "            elif tok.pos_ == 'PUNCT':\n",
    "                neg_ind = 100500\n",
    "                neg_processed.append(tok.lemma_)\n",
    "            elif tok.i > neg_ind:\n",
    "                neg_processed.append('NOT_' + tok.lemma_)\n",
    "            else:\n",
    "                neg_processed.append(tok.lemma_)\n",
    "        return neg_processed\n",
    "\n",
    "    feats = {}\n",
    "    feats['neg'] = get_intersection(_get_neg(doc1), _get_neg(doc2))\n",
    "    return feats\n",
    "\n",
    "\n",
    "#### Grammatical similarity\n",
    "def feature_extractor_deps(doc1, doc2):\n",
    "    def _inner_1(doc):\n",
    "        return [x.dep for x in doc]\n",
    "    def _inner_2(doc):\n",
    "        return [x.head.dep for x in doc]\n",
    "    \n",
    "    feats = {}\n",
    "\n",
    "    feats['dep'] = get_intersection(_inner_1(doc1), _inner_1(doc2))\n",
    "    feats['head-dep'] = get_intersection(_inner_2(doc1), _inner_2(doc2))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "# TODO: syntactic relations (x[0].dep <-- x[1])\n",
    "\n",
    "\n",
    "def feature_extractor_semant(cache):\n",
    "    def inner(doc1, doc2):\n",
    "        feats = {}\n",
    "\n",
    "        def _get_rels(doc):\n",
    "            for tok in doc:\n",
    "                if tok.dep_ == 'ROOT':\n",
    "                    if tok.lemma_ not in cache:\n",
    "                        rels = get_rels(tok.lemma_)\n",
    "                        cache[tok.lemma_] = rels\n",
    "                        return rels\n",
    "                    else:\n",
    "                        return cache[tok.lemma_]\n",
    "\n",
    "\n",
    "        rels1 = _get_rels(doc1)\n",
    "        rels2 = _get_rels(doc2)\n",
    "        root1_tok = [x for x in doc1 if x.dep_ == 'ROOT'][0]\n",
    "        root2_tok = [x for x in doc2 if x.dep_ == 'ROOT'][0]\n",
    "        root1 = root1_tok.lemma_\n",
    "        root2 = root2_tok.lemma_\n",
    "\n",
    "        neg = ['not', 'n\\'t', 'neither', 'nor', 'never', 'none', 'nowhere']\n",
    "                    \n",
    "        feats['syn'] = len([x for x in set(rels2['synonyms']) \\\n",
    "                            if x == root1 and doc2[root2_tok.i - 1].text not in neg]) + \\\n",
    "                        len([x for x in set(rels1['synonyms']) \\\n",
    "                            if x == root2 and doc1[root1_tok.i - 1].text not in neg])\n",
    "        feats['mean'] = len([x for x in set(rels2['meanings']) if x == root1 or x in rels1['meanings']])\n",
    "        feats['sim'] = len([x for x in set(rels2['similarities']) if x == root1])\n",
    "        feats['form'] = len([x for x in set(rels2['forms']) if x == root1 or x in rels1['forms']])\n",
    "        feats['ant'] = len([x for x in set(rels2['antonyms']) if x in rels1['antonyms']])\n",
    "\n",
    "        return feats\n",
    "    return inner\n",
    "\n",
    "\n",
    "# def get_data(dataset, feature_extractor, cache):\n",
    "#     features = []\n",
    "#     labels = []\n",
    "\n",
    "    \n",
    "#     for i, ds in enumerate(dataset):\n",
    "#         sent1 = ds['sentence1']\n",
    "#         sent2 = ds['sentence2']\n",
    "#         md5_1 = hashlib.md5(b'{sent1}')\n",
    "#         md5_2 = hashlib.md5(b'{sent2}')\n",
    "        \n",
    "#         nlp1 = None\n",
    "#         nlp2 = None\n",
    "\n",
    "#         if md5_1 not in cache:\n",
    "#             nlp1 = nlp(sent1)\n",
    "#             cache[md5_1] = nlp1\n",
    "#         else:\n",
    "#             nlp1 = cache[md5_1]\n",
    "#         if md5_2 not in cache:\n",
    "#             nlp2 = nlp(sent2)\n",
    "#             cache[md5_2] = nlp2\n",
    "#         else:\n",
    "#             nlp2 = cache[md5_2]\n",
    "            \n",
    "\n",
    "#         features.append(feature_extractor(nlp1, nlp2))\n",
    "#         labels.append(ds['gold_label'])\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "                        \n",
    "#     return features, labels\n",
    "\n",
    "\n",
    "def get_data(docs, raw_data, feature_extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i, doc_pair in enumerate(docs):\n",
    "        nlp1, nlp2 = doc_pair\n",
    "            \n",
    "        features.append(feature_extractor(nlp1, nlp2))\n",
    "        labels.append(raw_data[i]['gold_label'])\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "                        \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def print_result(train_docs, test_docs, train_raw_data, test_raw_data, feature_extractor):\n",
    "    X_train, y_train = get_data(train_docs, train_raw_data, feature_extractor)\n",
    "    X_dev, y_dev = get_data(test_docs, test_raw_data, feature_extractor)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classification_report(y_dev, clf.predict(X_dev)))\n",
    "    \n",
    "# def print_result(train_data, test_data, feature_extractor, cache):\n",
    "#     X_train, y_train = get_data(train_data, feature_extractor, cache)\n",
    "#     X_dev, y_dev = get_data(test_data, feature_extractor, cache)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(classification_report(y_dev, clf.predict(X_dev)))\n",
    "    \n",
    "    \n",
    "def get_concepts(concept):\n",
    "    offset = 0\n",
    "    req = requests.get('http://api.conceptnet.io/c/en/' + concept + '?offset=' + str(offset) + '&limit=100').json()\n",
    "    all_edges = req\n",
    "    return all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (just simply use sentence similarity fn from spacy ¯\\_(ツ)_/¯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 20 epochs took 8 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.51      0.46      3237\n",
      "   entailment       0.43      0.62      0.51      3368\n",
      "      neutral       0.35      0.11      0.17      3219\n",
      "\n",
      "     accuracy                           0.42      9824\n",
      "    macro avg       0.40      0.42      0.38      9824\n",
      " weighted avg       0.40      0.42      0.38      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With NER intersection (погіршення, викидаємо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 8 seconds\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.42      0.53      0.47      3237\n",
      "   entailment       0.42      0.68      0.52      3368\n",
      "      neutral       0.60      0.07      0.13      3219\n",
      "\n",
      "     accuracy                           0.43      9824\n",
      "    macro avg       0.48      0.43      0.37      9824\n",
      " weighted avg       0.48      0.43      0.38      9824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base, feature_extractor_ner)\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With words intersection (покращення є)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.55      0.66      0.60      3368\n",
      "      neutral       0.45      0.27      0.34      3219\n",
      "\n",
      "     accuracy                           0.50      9824\n",
      "    macro avg       0.49      0.50      0.49      9824\n",
      " weighted avg       0.49      0.50      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word)\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With ngrams (покращення практично нема)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1669,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.56      0.66      0.60      3368\n",
      "      neutral       0.45      0.29      0.35      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.50      0.49      9824\n",
      " weighted avg       0.50      0.51      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Намагалась опрацювати заперечення. Не вийшло :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1677,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.57      0.52      3237\n",
      "   entailment       0.56      0.66      0.60      3368\n",
      "      neutral       0.45      0.29      0.35      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.50      0.49      9824\n",
      " weighted avg       0.50      0.51      0.49      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_neg\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. With dependencies (схоже, я не зрозуміла, як їх використати, бо покращення мізерне)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1678,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.56      0.52      3237\n",
      "   entailment       0.56      0.65      0.60      3368\n",
      "      neutral       0.45      0.31      0.36      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.50      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. With stemms (не допомогло, тому викидаємо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.56      0.52      3237\n",
      "   entailment       0.57      0.66      0.61      3368\n",
      "      neutral       0.45      0.31      0.37      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.51      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_stemm\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. With semantic relations (моє найбільше розчарування...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1742,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.49      0.56      0.52      3237\n",
      "   entailment       0.56      0.65      0.60      3368\n",
      "      neutral       0.45      0.31      0.37      3219\n",
      "\n",
      "     accuracy                           0.51      9824\n",
      "    macro avg       0.50      0.51      0.50      9824\n",
      " weighted avg       0.50      0.51      0.50      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_semant({})\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. With spacy similarity (остання надія - на штучний інтелект, бо мій скінчився)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.50      0.55      0.53      3237\n",
      "   entailment       0.57      0.64      0.61      3368\n",
      "      neutral       0.48      0.38      0.42      3219\n",
      "\n",
      "     accuracy                           0.52      9824\n",
      "    macro avg       0.52      0.52      0.52      9824\n",
      " weighted avg       0.52      0.52      0.52      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = compose(feature_extractor_base,\n",
    "                            feature_extractor_word,\n",
    "                            feature_extractor_ngrams,\n",
    "                            feature_extractor_deps,\n",
    "                            feature_extractor_spacy_sim,\n",
    "                           )\n",
    "print_result(train_docs, test_docs, train_data, test_data, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rels(word):\n",
    "    concepts = get_concepts_local(word)\n",
    "\n",
    "    synonyms = []\n",
    "    related = []\n",
    "    forms = []\n",
    "    hyponyms = []\n",
    "    meronyms = []\n",
    "    holonyms = []\n",
    "    capabilities = []\n",
    "    causes = []\n",
    "    antonyms = []\n",
    "    meanings = []\n",
    "    similarities = []\n",
    "    common_origins = []\n",
    "    can_be_done_to = []\n",
    "    \n",
    "    def _check_rel(rel_type, rel_list):\n",
    "        if concept['rel']['label'] == rel_type:\n",
    "            lab = concept['end']['label']\n",
    "            if not lab in rel_list:\n",
    "                rel_list.append(lab)\n",
    "    \n",
    "    for concept in concepts:\n",
    "        _check_rel('Synonym', synonyms)\n",
    "        _check_rel('RelatedTo', related)\n",
    "        _check_rel('FormOf', forms)\n",
    "        _check_rel('IsA', hyponyms)\n",
    "        _check_rel('PartOf', meronyms)\n",
    "        _check_rel('UsedFor', holonyms)\n",
    "        _check_rel('CapableOf', capabilities)\n",
    "        _check_rel('Antonym', antonyms)\n",
    "        _check_rel('DefinedAs', meanings)\n",
    "        _check_rel('SimilarTo', similarities)\n",
    "        _check_rel('EtymologicallyRelatedTo', common_origins)\n",
    "        _check_rel('ReceivesAction', can_be_done_to)\n",
    "        \n",
    "    return {\n",
    "        'synonyms': synonyms,\n",
    "        'related': related,\n",
    "        'forms': forms,\n",
    "        'hyponyms': hyponyms,\n",
    "        'meronyms': meronyms,\n",
    "        'holonyms': holonyms,\n",
    "        'capabilities': capabilities,\n",
    "        'antonyms': antonyms,\n",
    "        'meanings': meanings,\n",
    "        'similarities': similarities,\n",
    "        'common_origins': common_origins,\n",
    "        'can_be_done_to': can_be_done_to,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_for_roots(data):\n",
    "    ex_conc = []\n",
    "    def get_concepts_for_sent(sent):\n",
    "        s_conc = None\n",
    "        if os.path.isfile('./concs.txt'):\n",
    "            with open('./concs.txt') as f:\n",
    "                ex_conc = [x.rstrip() for x in f.readlines()]\n",
    "        else:\n",
    "            ex_conc = []\n",
    "        for tok in nlp(sent):\n",
    "            if tok.lemma_ not in ex_conc and tok.dep_ == 'ROOT':\n",
    "                s_conc = get_concepts(tok.lemma_)['edges']\n",
    "                with open('./concs.txt', 'a') as f:\n",
    "                    f.write(tok.lemma_ + '\\n')\n",
    "                ex_conc.append(tok.lemma_)\n",
    "                \n",
    "        return s_conc\n",
    "\n",
    "    conc = []\n",
    "    for i, item in enumerate(data):\n",
    "        conc.append(get_concepts_for_sent(item['sentence1']))\n",
    "        conc.append(get_concepts_for_sent(item['sentence2']))\n",
    "    return conc\n",
    "\n",
    "valid_relations = ['Synonym', 'RelatedTo', 'FormOf', 'IsA', 'PartOf', 'UsedFor', 'CapableOf',\n",
    "                  'Antonym', 'DefinedAs', 'SimilarTo', 'EtymologicallyRelatedTo', 'ReceivesAction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def get_conc(data, path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    chunk = list(chunks(data, 500))\n",
    "    i = 0\n",
    "    for ch in chunk:\n",
    "        train_conc_root = get_concepts_for_roots(ch)\n",
    "        with open(f'./{path}/{i}.json', 'w') as f:\n",
    "            non_null = [x for x in train_conc_root if x]\n",
    "            filtered = [x for conc in non_null for x in conc if x['rel']['label'] in valid_relations \\\n",
    "                        and x['start']['language'] == 'en' and x['end']['language'] == 'en']\n",
    "            json.dump(filtered, f)\n",
    "            i+= 1\n",
    "            \n",
    "\n",
    "def get_concepts_local(word):\n",
    "    return [v for dic in normalized for k, v in dic.items() if k == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_conc(test_data, 'test_conc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_concepts(dirs):\n",
    "    res = []\n",
    "    for d in dirs:\n",
    "        files = os.listdir(d)\n",
    "        for file in files:\n",
    "            with open(os.path.join(d, file)) as f:\n",
    "                cont = json.load(f)\n",
    "                res += cont\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_concepts(concepts):\n",
    "    res = []\n",
    "    for concept in concepts:\n",
    "        if concept['start']['language'] == 'en':\n",
    "            res.append({concept['start']['label'].lower(): concept})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = merge_concepts(['train_conc', 'dev_conc', 'test_conc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_concepts.json', 'w') as f:\n",
    "    json.dump(normalized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = normalize_concepts(all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlps(data):\n",
    "    docs = []\n",
    "    for i, sent in enumerate(data):\n",
    "        docs.append((nlp(sent['sentence1']), nlp(sent['sentence2'])))\n",
    "        if i % 1000 == 0:\n",
    "            print('>> doc', i)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_nlps(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove\n",
    "def get_doc_pairs(docs):\n",
    "    pairs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        for x, y in doc.values():\n",
    "            pairs.append((x, y))\n",
    "            if i % 1000 == 0:\n",
    "                print('>> doc', i)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = get_doc_pairs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_docs = get_nlps(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = get_nlps(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
