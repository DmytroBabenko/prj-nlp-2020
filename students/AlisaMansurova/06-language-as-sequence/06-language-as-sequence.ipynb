{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1943,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(root, results_file_path):\n",
    "    def sp(text):\n",
    "        return reduce(lambda acc, el: acc[:-1] + [(acc[-1].strip() if acc else '') + el]\n",
    "                      if el and re.findall('[\\\\.\\\\?\\\\!]+', el)\n",
    "                      else acc + [el.strip()], re.split('([\\\\.\\\\?\\\\!]+)(?=\\\\s)', text),\n",
    "                      [])\n",
    "\n",
    "    for root, subdirs, files in os.walk(root):\n",
    "        res = []\n",
    "        src_files = [x for x in files if x.endswith('.txt')]\n",
    "        for file in src_files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path) as f:\n",
    "                content = f.read()\n",
    "\n",
    "                content = re.sub('\\\\s{2,}', ' ', content.replace(\n",
    "                    '\\n ', ' ').replace('\\n', ' '))\n",
    "                content = sp(content)\n",
    "                for sent in content:\n",
    "                    res.append(sent)\n",
    "        with open(results_file_path, 'w') as f:\n",
    "            f.write(json.dumps(res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_glued_sents(text):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    num_sents = random.randint(2, 4)\n",
    "    text_split = [text[i:i+num_sents] for i in range(0, len(text), num_sents)]\n",
    "\n",
    "    for spl in text_split:\n",
    "        sents_spl = spl\n",
    "        out_of_seq = num_sents\n",
    "        num_lower = random.randint(0, out_of_seq)\n",
    "        if num_lower < len(sents_spl):\n",
    "            sent_to_lower = sents_spl[num_lower]\n",
    "            s_lower = sent_to_lower[:1].lower() + sent_to_lower[1:]\n",
    "            sents_spl[num_lower] = s_lower\n",
    "\n",
    "        for i, sent in enumerate(sents_spl):\n",
    "            doc = nlp(sent)\n",
    "                \n",
    "            eos_syms = ['.', '?', '!', '...']\n",
    "            for token in doc:\n",
    "                ln = len(doc)\n",
    "                if token.i == ln - 2 and token.i + 1 < ln and doc[token.i + 1].text in eos_syms:\n",
    "                    labels.append(True)\n",
    "                    tokens.append(token)\n",
    "                else:\n",
    "                    if token.text not in eos_syms:\n",
    "                        labels.append(False)\n",
    "                        tokens.append(token)\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "def get_glued_data(train_data, results_file_path):\n",
    "    tokens, labels = generate_glued_sents(train_data)\n",
    "    with open(results_file_path, 'w') as f:\n",
    "        f.write(json.dumps([x.text for x in tokens]))\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "def get_train_data(extracted_data_file, train_data_file):\n",
    "    with open(extracted_data_file) as f:\n",
    "        extracted_data = json.load(f)\n",
    "    return get_glued_data(extracted_data, train_data_file)\n",
    "\n",
    "\n",
    "def get_test_data(text):\n",
    "    sents = [x for sub in text for x in sub]\n",
    "    test_tokens = [nlp(x[0])[0] for x in sents]\n",
    "    test_labels = [x[1] for x in sents]\n",
    "\n",
    "    return test_tokens, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "def split_unique_words_into_chunks(filename):\n",
    "    SIZE = 500\n",
    "    with open(f'{filename}.txt') as f:\n",
    "        lines = f.readlines()\n",
    "        ln = len(lines)\n",
    "        for i in range(1, int(ln/SIZE) + 2):\n",
    "            end = i*SIZE\n",
    "            if end > ln:\n",
    "                start = (i - 1)*SIZE + 1\n",
    "                chunk = lines[start:]\n",
    "\n",
    "            else:\n",
    "                start = (i - 1)*SIZE + 1 if i > 1 else 0\n",
    "                chunk = lines[start:end]\n",
    "            with open(f'./{filename}_{i}.txt', 'w') as ch:\n",
    "                ch.writelines(chunk)\n",
    "\n",
    "\n",
    "def get_words_for_ngrams(train_tokens):\n",
    "    with open(test_data_file) as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    syms = ['.', '?', '!', '...', ',', ':', ';',\n",
    "            '-', '>', '<', '&', '(', '=', '/', '\\\\', '[', '{']\n",
    "\n",
    "    words = [x[0] for sub in test_data for x in sub]\n",
    "    with open(unique_words_file, 'a') as f:\n",
    "        for i, token in enumerate(train_tokens):\n",
    "            if token not in words and token not in syms:\n",
    "                words.append(token)\n",
    "                f.write(token + '\\n')\n",
    "            print('>>>>>', i)\n",
    "\n",
    "\n",
    "def merge_gnrams_from_chuks(num_chunks):\n",
    "    res = []\n",
    "    for i in range(1, num_chunks):\n",
    "        with open(f'./trigrams_{i}.txt') as f:\n",
    "            content = f.readlines()\n",
    "            res += content\n",
    "    with open(f'./trigrams.txt', 'w') as f:\n",
    "        f.writelines(res)\n",
    "\n",
    "\n",
    "def tsv_to_json(content):\n",
    "    res = []\n",
    "    for line in content:\n",
    "        if line.startswith('{\"error\"'):\n",
    "            continue\n",
    "        obj = {'tks': []}\n",
    "        parts = line.split()\n",
    "        for x in [parts[0], parts[1], parts[2]]:\n",
    "            r = re.split('_(\\\\d)', x)\n",
    "            obj['tks'].append({'tt': r[0], 'tg': r[1]})\n",
    "        obj['mc'] = parts[3]\n",
    "        obj['vc'] = parts[4]\n",
    "        obj['sc'] = parts[8]\n",
    "        res.append(obj)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1879,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    pipe = Pipeline([\n",
    "        ('dict_vect', DictVectorizer()),\n",
    "        ('lrc', LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                                   max_iter=100, solver='sag', n_jobs=50))])\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def check_freq_in_ngrams(tokens, i, ngrams):\n",
    "    token = tokens[i]\n",
    "    next_token = tokens[i + 1] if i + 1 < len(tokens) else None\n",
    "    \n",
    "    word_freq = 0.00\n",
    "    next_pos = 'NONE'\n",
    "\n",
    "    for i, ngram in enumerate(ngrams):\n",
    "        tks = ngram['tks']\n",
    "        for i, t in enumerate(tks):\n",
    "            if t['tg'] == 0 and t['tt'] == token:\n",
    "                if tks[i + 1]['tt'] == next_token:\n",
    "                    word_freq = ngram['sc']\n",
    "                    if next_token:\n",
    "                        next_pos = next_token.pos_\n",
    "        return word_freq, next_pos\n",
    "\n",
    "\n",
    "\"\"\" start feature extractors \"\"\"\n",
    "\n",
    "\n",
    "def word_feature_extractor(tokens, i):\n",
    "    token = tokens[i]\n",
    "    features = {}\n",
    "    features['word'] = token.text\n",
    "    return features\n",
    "\n",
    "\n",
    "def adj_words_feature_extractor(tokens, i):\n",
    "    token = tokens[i]\n",
    "    features = {}\n",
    "\n",
    "    tk_len = len(tokens)\n",
    "\n",
    "    features['word-1'] = tokens[i - 1].text if i > 1 else '<S>'\n",
    "    features['word-2'] = tokens[i - 2].text if i > 2 else '<S>'\n",
    "    features['word+1'] = tokens[i + 1].text if i + 1 < tk_len else '</S>'\n",
    "    features['word+2'] = tokens[i + 2].text if i + 2 < tk_len else '</S>'\n",
    "    return features\n",
    "\n",
    "\n",
    "def pos_feature_extractor(tokens, i):\n",
    "    token = tokens[i]\n",
    "    features = {}\n",
    "\n",
    "    features['POS'] = token.pos_\n",
    "    return features\n",
    "\n",
    "\n",
    "def adj_pos_feature_extractor(tokens, i):\n",
    "    token = tokens[i]\n",
    "    features = {}\n",
    "\n",
    "    tk_len = len(tokens)\n",
    "\n",
    "    features['POS-1'] = tokens[i - 1].pos_ if i > 1 else 'NONE'\n",
    "    features['POS-2'] = tokens[i - 2].pos_ if i > 2 else 'NONE'\n",
    "    features['POS+1'] = tokens[i + 1].pos_ if i + 1 < tk_len else 'NONE'\n",
    "    features['POS+2'] = tokens[i + 2].pos_ if i + 2 < tk_len else 'NONE'\n",
    "    return features\n",
    "\n",
    "\n",
    "def shape_feature_extractor(tokens, i):\n",
    "    token = tokens[i]\n",
    "\n",
    "    def get_shape(w):\n",
    "        if w.isupper():\n",
    "            return 'is_upper'\n",
    "        elif w.istitle():\n",
    "            return 'is_title'\n",
    "        elif w.islower():\n",
    "            return 'is_lower'\n",
    "        elif w.isdigit():\n",
    "            return 'is_digit'\n",
    "        elif w.isalpha():\n",
    "            return 'is_alpha'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    features = {}\n",
    "    \n",
    "    tk_len = len(tokens)\n",
    "\n",
    "    features['shape'] = get_shape(token.text)\n",
    "    features['shape-1'] = get_shape(tokens[i - 1].text) if i > 1 else 'other'\n",
    "    features['shape-2'] = get_shape(tokens[i - 2].text) if i > 2 else 'other'\n",
    "    features['shape+1'] = get_shape(tokens[i + 1].text) if i + 1 < tk_len else 'other'\n",
    "    features['shape+2'] = get_shape(tokens[i + 2].text) if i + 2 < tk_len else 'other'\n",
    "    return features\n",
    "\n",
    "\n",
    "def ngrams_feature_extractor(ngrams):\n",
    "    def inner(tokens, i):\n",
    "        token = tokens[i]\n",
    "        features = {}\n",
    "        word_freq, next_pos = check_freq_in_ngrams(tokens, i, ngrams)\n",
    "        features['freq_in_ngrams'] = word_freq\n",
    "        features['next_pos_in_ngrams'] = next_pos\n",
    "        return features\n",
    "    return inner\n",
    "\n",
    "\n",
    "\"\"\" end feature extractors \"\"\"\n",
    "\n",
    "\n",
    "def get_features(tokens, extractors):\n",
    "    features = []\n",
    "    seen_features = {}\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        curr_token = tokens[i]\n",
    "        if curr_token not in seen_features.keys():\n",
    "            feat = {}\n",
    "            for extractor in extractors:\n",
    "                feat.update(extractor(tokens, i))\n",
    "            seen_features[curr_token] = feat\n",
    "            features.append(feat)\n",
    "        else:\n",
    "            features.append(seen_features[curr_token])\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_cross_validation_report(clf, X_train, y_train):\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision_true': make_scorer(precision_score, average=None, labels=[True]),\n",
    "               'precision_false': make_scorer(precision_score, average=None, labels=[False]),\n",
    "               'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "               'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "               'recall_true': make_scorer(recall_score, average=None, labels=[True]),\n",
    "               'recall_false': make_scorer(recall_score, average=None, labels=[False]),\n",
    "               'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "               'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "               'f1_true': make_scorer(f1_score, average=None, labels=[True]),\n",
    "               'f1_false': make_scorer(f1_score, average=None, labels=[False]),\n",
    "               'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "               'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "               }\n",
    "    res = cross_validate(clf, X_train, y_train, scoring=scoring)\n",
    "\n",
    "    def get_score(field):\n",
    "        return round(res[field].mean(), 2)\n",
    "\n",
    "    accuracy = get_score('test_accuracy')\n",
    "    precision_false = get_score('test_precision_false')\n",
    "    precision_true = get_score('test_precision_true')\n",
    "    recall_false = get_score('test_recall_false')\n",
    "    recall_true = get_score('test_recall_true')\n",
    "    f1_false = get_score('test_f1_false')\n",
    "    f1_true = get_score('test_f1_true')\n",
    "    precision_macro = get_score('test_precision_macro')\n",
    "    precision_weighted = get_score('test_precision_weighted')\n",
    "    recall_macro = get_score('test_recall_macro')\n",
    "    recall_weighted = get_score('test_recall_weighted')\n",
    "    f1_macro = get_score('test_f1_macro')\n",
    "    f1_weighted = get_score('test_f1_weighted')\n",
    "\n",
    "    scores = ['precision', 'recall', 'f1-score']\n",
    "    labels = ['False', 'True', '', 'accuracy', 'macro avg', 'weighted avg']\n",
    "\n",
    "    data = np.array([[precision_false, recall_false, f1_false],\n",
    "                     [precision_true, recall_true, f1_true],\n",
    "                     ['', '', ''],\n",
    "                     ['', '', accuracy],\n",
    "                     [precision_macro, recall_macro, f1_macro],\n",
    "                     [precision_weighted, recall_weighted, f1_weighted],\n",
    "                     ])\n",
    "    print(pandas.DataFrame(data, labels, scores))\n",
    "    \n",
    "    \n",
    "def get_cross_validation_result(clf, train_tokens, feature_extractors):\n",
    "    train_features = get_features(train_tokens, feature_extractors)\n",
    "    return get_cross_validation_report(clf, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1944,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_file= './extracted_data.json'\n",
    "train_data_file = './train_data_glued.json'\n",
    "train_tokens, train_labels = get_train_data(extracted_data_file, train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1867,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../tasks/06-language-as-sequence/run-on-test.json') as f:\n",
    "    test_content = json.load(f)\n",
    "    test_tokens, test_labels = get_test_data(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./trigrams.json') as f:\n",
    "    ngrams = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1946,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Проста фіча - текст слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1948,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.96    1.0     0.98\n",
      "True              0.75   0.05      0.1\n",
      "                                      \n",
      "accuracy                          0.96\n",
      "macro avg         0.85   0.53     0.54\n",
      "weighted avg      0.95   0.96     0.94\n"
     ]
    }
   ],
   "source": [
    "feature_extractors = [word_feature_extractor]\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Сусідні (по два вправо і вліво) слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1949,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.98    1.0     0.99\n",
      "True              0.88   0.62     0.73\n",
      "                                      \n",
      "accuracy                          0.98\n",
      "macro avg         0.93   0.81     0.86\n",
      "weighted avg      0.98   0.98     0.98\n"
     ]
    }
   ],
   "source": [
    "feature_extractors.append(adj_words_feature_extractor)\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Частина мови"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1950,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.98    1.0     0.99\n",
      "True              0.88   0.62     0.73\n",
      "                                      \n",
      "accuracy                          0.98\n",
      "macro avg         0.93   0.81     0.86\n",
      "weighted avg      0.98   0.98     0.98\n"
     ]
    }
   ],
   "source": [
    "feature_extractors.append(pos_feature_extractor)\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Частини мови сусідніх (по два вправо і вліво) слів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.98    1.0     0.99\n",
      "True              0.88   0.62     0.73\n",
      "                                      \n",
      "accuracy                          0.98\n",
      "macro avg         0.93   0.81     0.86\n",
      "weighted avg      0.98   0.98     0.98\n"
     ]
    }
   ],
   "source": [
    "feature_extractors.append(adj_pos_feature_extractor)\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Форма слова (+ форми слова сусідніх слів)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1953,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.99    1.0     0.99\n",
      "True              0.87   0.71     0.78\n",
      "                                      \n",
      "accuracy                          0.98\n",
      "macro avg         0.93   0.85     0.89\n",
      "weighted avg      0.98   0.98     0.98\n"
     ]
    }
   ],
   "source": [
    "feature_extractors.append(shape_feature_extractor)\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. н-грами (частота вживання слів поруч + наступна частина мови)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "False             0.99    1.0     0.99\n",
      "True              0.87   0.71     0.78\n",
      "                                      \n",
      "accuracy                          0.98\n",
      "macro avg         0.93   0.85     0.89\n",
      "weighted avg      0.98   0.98     0.98\n"
     ]
    }
   ],
   "source": [
    "feature_extractors.append(ngrams_feature_extractor(ngrams))\n",
    "get_cross_validation_result(clf, train_tokens, feature_extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Тестова вибірка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1945,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      4542\n",
      "        True       0.37      0.26      0.30       155\n",
      "\n",
      "    accuracy                           0.96      4697\n",
      "   macro avg       0.67      0.62      0.64      4697\n",
      "weighted avg       0.95      0.96      0.96      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vect = DictVectorizer()\n",
    "train_features = get_features(train_tokens, feature_extractors)\n",
    "train_feat_vectorized = vect.fit_transform(train_features)\n",
    "clf = LogisticRegression(random_state=42, multi_class='multinomial',\n",
    "                         max_iter=100, solver='sag', n_jobs=50)\n",
    "clf.fit(train_feat_vectorized, train_labels)\n",
    "test_features = get_features(test_tokens, feature_extractors)\n",
    "print(classification_report(test_labels, clf.predict(vect.transform(test_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
