{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from langdetect import detect\n",
    "import tokenize_uk\n",
    "import pymorphy2\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_reviews.json') as f:\n",
    "    reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = [r for r in reviews if r \\\n",
    "               and (r['text'].strip() and r['rating'] \\\n",
    "                or r.get('pros', '').strip() or r.get('cons', '').strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skip = ['-', '...']\n",
    "\n",
    "ukr_reviews = []\n",
    "failed = 0\n",
    "for i, review in enumerate(all_reviews):\n",
    "    text = review['text']\n",
    "    pros = review.get('pros')\n",
    "    cons = review.get('cons')\n",
    "    \n",
    "    if pros and (pros.endswith('?') or pros in skip):\n",
    "        del review['pros']\n",
    "        pros = None\n",
    "    if cons and (cons.endswith('?') or cons in skip):\n",
    "        del review['cons']\n",
    "        cons = None\n",
    "\n",
    "    try:\n",
    "        t_lang = detect(text) if text else None\n",
    "        p_lang = detect(pros) if pros else None\n",
    "        c_lang = detect(cons) if cons else None\n",
    "        \n",
    "        if t_lang == 'uk':\n",
    "            if pros and not p_lang:\n",
    "                del review['pros']\n",
    "            if cons and not c_lang:\n",
    "                del review['cons']\n",
    "            ukr_reviews.append(review)\n",
    "\n",
    "        \n",
    "#         if t_lang != 'uk':\n",
    "#             text = translate_text(text)\n",
    "#             review.update({'text': text})\n",
    "#         if p_lang and p_lang != 'uk':\n",
    "#             pros = translate_text(pros)\n",
    "#             review.update({'pros': pros})\n",
    "#         if c_lang and c_lang != 'uk':\n",
    "#             cons = translate_text(cons)\n",
    "#             cons = translator.translate(cons, dest='uk').text\n",
    "#             review.update({'cons': cons})\n",
    "                \n",
    "#         ukr_reviews.append(review)\n",
    "      \n",
    "    except Exception:\n",
    "        failed += 1\n",
    "        pass\n",
    "    if i%100 == 0:\n",
    "        print('>>>', i, failed)\n",
    "print('Failed to process: ', failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed once\n",
    "# with open('./rozetka_uk.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(ukr_reviews, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# with open('./rozetka_uk.json') as f:\n",
    "#     ukr_reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, text_processor):\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def get_features(self):\n",
    "        feature_words = []\n",
    "\n",
    "        for review in self.X_data:\n",
    "            processed = self.text_processor(review['text'])\n",
    "\n",
    "            for word in processed:\n",
    "                if word not in feature_words:\n",
    "                    feature_words.append(word)\n",
    "\n",
    "        features = {x: i for i, x in enumerate(feature_words)}\n",
    "        features.update({'UNK': len(features)})\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_senses(self):\n",
    "        pos = []\n",
    "        neg = []\n",
    "        neut = []\n",
    "\n",
    "        for review in self.X_data:\n",
    "            text = self.text_processor(review['text'])\n",
    "            sens = review['sens']\n",
    "\n",
    "            if sens == 'neut':\n",
    "                for t in text:\n",
    "\n",
    "                    word = t\n",
    "                    neut.append(word)\n",
    "            elif sens == 'pos':\n",
    "                for t in text:\n",
    "                    word = t\n",
    "                    pos.append(word)\n",
    "            else:\n",
    "                for t in text:\n",
    "                    word = t\n",
    "                    neg.append(word)\n",
    "        return pos, neg, neut\n",
    "\n",
    "    def get_feature_counts_by_class(self):\n",
    "        pos, neg, neut = self.get_senses()\n",
    "        features = self.get_features()\n",
    "\n",
    "        count = {'pos': [], 'neg': [], 'neut': []}\n",
    "        cnt_pos = Counter(pos)\n",
    "        cnt_neg = Counter(neg)\n",
    "        cnt_neut = Counter(neut)\n",
    "\n",
    "        for w in features.keys():\n",
    "            pos_c = cnt_pos[w]\n",
    "            neg_c = cnt_neg[w]\n",
    "            neut_c = cnt_neut[w]\n",
    "            count['pos'].append(pos_c)\n",
    "            count['neg'].append(neg_c)\n",
    "            count['neut'].append(neut_c)\n",
    "        return count\n",
    "\n",
    "    def get_feature_weights_by_class(self):\n",
    "        res = {}\n",
    "        feat_counts = self.get_feature_counts_by_class()\n",
    "\n",
    "        for k, v in feat_counts.items():\n",
    "            res[k] = [log(x/len(v)) if x else log(0.1/len(v)) for x in v]\n",
    "        return res\n",
    "\n",
    "    def calculate_bias_by_class(self):\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neut = 0\n",
    "        all_count = len(self.X_data)\n",
    "\n",
    "        for review in self.X_data:\n",
    "            sens = review['sens']\n",
    "\n",
    "            if sens == 'neut':\n",
    "                neut += 1\n",
    "            elif sens == 'pos':\n",
    "                pos += 1\n",
    "            else:\n",
    "                neg += 1\n",
    "\n",
    "        return {'pos': round(log(pos/all_count), 4),\n",
    "                'neg': round(log(neg/all_count), 4),\n",
    "                'neut': round(log(neut/all_count), 4)\n",
    "                }\n",
    "\n",
    "    def predict_class(self, text, weights, bias):\n",
    "        text_words = self.text_processor(text)\n",
    "        features = self.features\n",
    "\n",
    "        p_pos = bias['pos'] + sum(weights['pos'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "        p_neg = bias['neg'] + sum(weights['neg'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "        p_neut = bias['neut'] + sum(weights['neut'][features.get(\n",
    "            word, features['UNK'])] for word in text_words)\n",
    "\n",
    "        p_dict = {'pos': p_pos, 'neg': p_neg, 'neut': p_neut}\n",
    "\n",
    "        return max(p_dict, key=p_dict.get)\n",
    "\n",
    "    def fit(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        self.bias = self.calculate_bias_by_class()\n",
    "        self.weights = self.get_feature_weights_by_class()\n",
    "\n",
    "    def predict(self, y_data):\n",
    "        res = []\n",
    "\n",
    "        for review in y_data:\n",
    "            text = review['text']\n",
    "            pros = review.get('pros', '')\n",
    "            cons = review.get('cons', '')\n",
    "            all_text = text + pros + cons\n",
    "            res.append(self.predict_class(all_text, self.weights, self.bias))\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_corpus(reviews):\n",
    "    res = []\n",
    "    for review in reviews:\n",
    "        rating = review['rating']\n",
    "        text = review['text']\n",
    "        pros = review.get('pros')\n",
    "        cons = review.get('cons')\n",
    "        \n",
    "        neut_texts = ['не знаю', 'незнаю', 'не купив', 'не купила',\n",
    "            'час покаже', 'поки не', 'ще не', 'поки що не', 'трохи', 'трошки', 'але '\n",
    "            ]\n",
    "        \n",
    "        rev_pros = None\n",
    "        rev_neut = None\n",
    "        rev_cons = None\n",
    "\n",
    "        rev_text = {'text': text}\n",
    "        if pros:\n",
    "            if any([x for x in neut_texts if x in pros.lower()]):\n",
    "                rev_neut = {'text': pros, 'sens': 'neut'}\n",
    "            else:\n",
    "                rev_pros = {'text': pros, 'sens': 'pos'}\n",
    "        if cons:\n",
    "            if any([x for x in neut_texts if x in cons.lower()]):\n",
    "                rev_neut = {'text': cons, 'sens': 'neut'}\n",
    "            else:\n",
    "                rev_cons = {'text': cons, 'sens': 'neg'}\n",
    "\n",
    "        if rating:\n",
    "            if rating == 5:\n",
    "                rev_text['sens'] = 'pos'\n",
    "            elif rating >= 3:\n",
    "                rev_text['sens'] = 'neut'\n",
    "            else:\n",
    "                rev_text['sens'] = 'neg'\n",
    "            res.append(rev_text)\n",
    "\n",
    "        if rev_pros:\n",
    "            res.append(rev_pros)\n",
    "\n",
    "        if rev_cons:\n",
    "            res.append(rev_cons)\n",
    "            \n",
    "        if rev_neut:\n",
    "            res.append(rev_neut)\n",
    "\n",
    "    pos = [x for x in res if x['sens'] == 'pos']\n",
    "    neg = [x for x in res if x['sens'] == 'neg']\n",
    "    neut = [x for x in res if x['sens'] == 'neut']\n",
    "    print('>> pos', len(pos))\n",
    "    print('>> neg', len(neg))\n",
    "    print('>> neut', len(neut))\n",
    "\n",
    "    res_all = pos + neg + neut\n",
    "    shuffle(res_all)\n",
    "    return res_all\n",
    "\n",
    "\n",
    "def divide_data(data):\n",
    "    data_len = int(len(data) * 0.7)\n",
    "    return data[:data_len], data[data_len:]\n",
    "\n",
    "\n",
    "\"\"\" text processors START \"\"\"\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenize_uk.tokenize_uk.tokenize_words(text)\n",
    "\n",
    "\n",
    "def lowerize_text(text):\n",
    "    return [word.lower() for word in tokenize_text(text)]\n",
    "\n",
    "\n",
    "def _lemmatize(text):\n",
    "    res = []\n",
    "    for word in tokenize_text(text):\n",
    "        m_word = morph.parse(word)[0]\n",
    "        res.append((m_word.normal_form, m_word))\n",
    "    return res\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [x for x, _ in _lemmatize(text)]\n",
    "\n",
    "\n",
    "def filterize_text(text):\n",
    "    res = []\n",
    "    lemmatized = _lemmatize(text)\n",
    "\n",
    "    symbols = ['-', '+', ':', '<', '>', '&']\n",
    "    invalid_pos = ['CONJ', 'INTJ', 'PREP', 'NPRO']\n",
    "    invalid_non_oc_pos = ['NUMB,intg', 'NUMB,real', 'ROMN', 'PNCT', 'LATN']\n",
    "\n",
    "    for word, m_word in lemmatized:\n",
    "        if len(word) and str(m_word.tag) not in invalid_non_oc_pos and \\\n",
    "                m_word.tag.POS not in invalid_pos and \\\n",
    "                word not in symbols:\n",
    "            res.append(word)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def negatiaze_text(text):\n",
    "    res = []\n",
    "    words = filterize_text(text)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        p = morph.parse(word)[0]\n",
    "        if (p.tag.POS == 'ADJF' or p.tag.POS == 'VERB' or p.tag.POS == 'INFN') \\\n",
    "                and words[i-1] == 'не':\n",
    "            res.append(f'не_{word}')\n",
    "        else:\n",
    "            res.append(word)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def ngrammaze_text(text, additional_preproc=None):\n",
    "    if additional_preproc:\n",
    "        words = ' '.join(additional_preproc(text))\n",
    "    else:\n",
    "        words = text\n",
    "    return [words[i:i + 3] if i > 0 else '^' + words[i:i + 3] for i in range(0, len(words), 1)]\n",
    "\n",
    "\n",
    "\"\"\" text processors END \"\"\"\n",
    "\n",
    "\n",
    "def get_X(reviews):\n",
    "    return [x['text'] for x in reviews]\n",
    "\n",
    "\n",
    "def get_y(reviews):\n",
    "    return [x['sens'] for x in reviews]\n",
    "\n",
    "\n",
    "def get_classification_report(preprocess_fn, train_data, test_data, y_target):\n",
    "    cls = NaiveBayesClassifier(preprocess_fn)\n",
    "    cls.fit(train_data)\n",
    "    test_predict = cls.predict(test_data)\n",
    "    print(classification_report(y_target, test_predict))\n",
    "\n",
    "\n",
    "def get_cross_validation_report(preprocess_fn, X_train, y_train):\n",
    "    vect = CountVectorizer(tokenizer=preprocess_fn)\n",
    "    cls_1 = Pipeline([('vect', vect), ('cls', MultinomialNB())])\n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "               'precision_pos': make_scorer(precision_score, average=None, labels=['pos']),\n",
    "               'precision_neut': make_scorer(precision_score, average=None, labels=['neut']),\n",
    "               'precision_neg': make_scorer(precision_score, average=None, labels=['neg']),\n",
    "               'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "               'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "               'recall_pos': make_scorer(recall_score, average=None, labels=['pos']),\n",
    "               'recall_neut': make_scorer(recall_score, average=None, labels=['neut']),\n",
    "               'recall_neg': make_scorer(recall_score, average=None, labels=['neg']),\n",
    "               'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "               'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "               'f1_pos': make_scorer(f1_score, average=None, labels=['pos']),\n",
    "               'f1_neut': make_scorer(f1_score, average=None, labels=['neut']),\n",
    "               'f1_neg': make_scorer(f1_score, average=None, labels=['neg']),\n",
    "               'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "               'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "               }\n",
    "    res = cross_validate(cls_1, X_train, y_train, scoring=scoring)\n",
    "    \n",
    "    def get_score(field):\n",
    "        return round(res[field].mean(), 2)\n",
    "    \n",
    "    accuracy = get_score('test_accuracy')\n",
    "    precision_pos = get_score('test_precision_pos')\n",
    "    precision_neut = get_score('test_precision_neut')\n",
    "    precision_neg = get_score('test_precision_neg')\n",
    "    recall_pos = get_score('test_recall_pos')\n",
    "    recall_neut = get_score('test_recall_neut')\n",
    "    recall_neg = get_score('test_recall_neg')\n",
    "    f1_pos = get_score('test_f1_pos')\n",
    "    f1_neut = get_score('test_f1_neut')\n",
    "    f1_neg = get_score('test_f1_neg')\n",
    "    precision_macro = get_score('test_precision_macro')\n",
    "    precision_weighted = get_score('test_precision_weighted')\n",
    "    recall_macro = get_score('test_recall_macro')\n",
    "    recall_weighted = get_score('test_recall_weighted')\n",
    "    f1_macro = get_score('test_f1_macro')\n",
    "    f1_weighted = get_score('test_f1_weighted')\n",
    "\n",
    "    scores = ['precision', 'recall', 'f1-score']\n",
    "    labels = ['pos', 'neut', 'neg', '', 'accuracy', 'macro avg', 'weighted avg']\n",
    "\n",
    "    data = np.array([[precision_pos, recall_pos, f1_pos],\n",
    "                     [precision_neut, recall_neut, f1_neut],\n",
    "                     [precision_neg, recall_neg, f1_neg],\n",
    "                     ['', '', ''],\n",
    "                     ['', '', accuracy],\n",
    "                     [precision_macro, recall_macro, f1_macro],\n",
    "                     [precision_weighted, recall_weighted, f1_weighted],\n",
    "                     ])\n",
    "    print(pandas.DataFrame(data, labels, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> pos 7163\n",
      ">> neg 4115\n",
      ">> neut 1643\n"
     ]
    }
   ],
   "source": [
    "corpus = get_corpus(ukr_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = divide_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = get_X(train_data)\n",
    "X_test = get_X(test_data)\n",
    "y_train = get_y(train_data)\n",
    "y_target = get_y(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.34      0.49      1241\n",
      "        neut       0.72      0.06      0.12       489\n",
      "         pos       0.63      0.98      0.76      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.75      0.46      0.46      3877\n",
      "weighted avg       0.73      0.66      0.60      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(tokenize_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.74   0.95     0.83\n",
      "neut               0.5    0.2     0.29\n",
      "neg               0.81   0.61      0.7\n",
      "                                      \n",
      "accuracy                          0.75\n",
      "macro avg         0.68   0.59     0.61\n",
      "weighted avg      0.73   0.75     0.72\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(tokenize_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.35      0.50      1241\n",
      "        neut       0.62      0.04      0.07       489\n",
      "         pos       0.63      0.98      0.77      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.72      0.46      0.45      3877\n",
      "weighted avg       0.72      0.66      0.59      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lowerize_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.74   0.95     0.83\n",
      "neut               0.5    0.2     0.29\n",
      "neg               0.81   0.61      0.7\n",
      "                                      \n",
      "accuracy                          0.75\n",
      "macro avg         0.68   0.59     0.61\n",
      "weighted avg      0.73   0.75     0.72\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lowerize_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.34      0.49      1241\n",
      "        neut       0.47      0.01      0.03       489\n",
      "         pos       0.62      0.99      0.76      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.66      0.45      0.43      3877\n",
      "weighted avg       0.69      0.66      0.58      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lemmatize_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.77   0.94     0.84\n",
      "neut              0.49   0.23     0.31\n",
      "neg               0.79   0.66     0.72\n",
      "                                      \n",
      "accuracy                          0.76\n",
      "macro avg         0.68   0.61     0.63\n",
      "weighted avg      0.74   0.76     0.74\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lemmatize_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.40      0.54      1241\n",
      "        neut       0.17      0.01      0.01       489\n",
      "         pos       0.65      0.98      0.78      2147\n",
      "\n",
      "    accuracy                           0.67      3877\n",
      "   macro avg       0.54      0.46      0.44      3877\n",
      "weighted avg       0.64      0.67      0.60      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(filterize_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.77   0.93     0.84\n",
      "neut              0.41   0.15     0.22\n",
      "neg               0.76   0.67     0.71\n",
      "                                      \n",
      "accuracy                          0.75\n",
      "macro avg         0.65   0.58     0.59\n",
      "weighted avg      0.72   0.75     0.72\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(filterize_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.45      0.59      1241\n",
      "        neut       0.32      0.02      0.03       489\n",
      "         pos       0.66      0.98      0.79      2147\n",
      "\n",
      "    accuracy                           0.69      3877\n",
      "   macro avg       0.60      0.48      0.47      3877\n",
      "weighted avg       0.67      0.69      0.63      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(negatiaze_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.79   0.93     0.85\n",
      "neut              0.46   0.18     0.25\n",
      "neg               0.78   0.72     0.75\n",
      "                                      \n",
      "accuracy                          0.77\n",
      "macro avg         0.68   0.61     0.62\n",
      "weighted avg      0.74   0.77     0.74\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(negatiaze_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.31      0.46      1241\n",
      "        neut       1.00      0.02      0.04       489\n",
      "         pos       0.62      0.99      0.76      2147\n",
      "\n",
      "    accuracy                           0.65      3877\n",
      "   macro avg       0.83      0.44      0.42      3877\n",
      "weighted avg       0.75      0.65      0.57      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, lowerize_text), train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.82   0.85     0.84\n",
      "neut              0.41   0.39      0.4\n",
      "neg               0.76   0.72     0.74\n",
      "                                      \n",
      "accuracy                          0.75\n",
      "macro avg         0.66   0.66     0.66\n",
      "weighted avg      0.75   0.75     0.75\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, lowerize_text), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.33      0.48      1241\n",
      "        neut       0.00      0.00      0.00       489\n",
      "         pos       0.62      0.99      0.76      2147\n",
      "\n",
      "    accuracy                           0.65      3877\n",
      "   macro avg       0.50      0.44      0.41      3877\n",
      "weighted avg       0.62      0.65      0.57      3877\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, lemmatize_text), train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.82   0.85     0.84\n",
      "neut               0.4   0.36     0.38\n",
      "neg               0.75   0.73     0.74\n",
      "                                      \n",
      "accuracy                          0.75\n",
      "macro avg         0.66   0.65     0.65\n",
      "weighted avg      0.74   0.75     0.75\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, lemmatize_text), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.35      0.49      1241\n",
      "        neut       0.00      0.00      0.00       489\n",
      "         pos       0.63      0.98      0.77      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.48      0.44      0.42      3877\n",
      "weighted avg       0.61      0.66      0.58      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, filterize_text), train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.83   0.83     0.83\n",
      "neut              0.33   0.32     0.33\n",
      "neg               0.72   0.73     0.72\n",
      "                                      \n",
      "accuracy                          0.73\n",
      "macro avg         0.63   0.63     0.63\n",
      "weighted avg      0.73   0.73     0.73\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, filterize_text), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.35      0.49      1241\n",
      "        neut       0.00      0.00      0.00       489\n",
      "         pos       0.63      0.98      0.77      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.48      0.45      0.42      3877\n",
      "weighted avg       0.61      0.66      0.58      3877\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(lambda x: ngrammaze_text(x, negatiaze_text), train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.83   0.84     0.84\n",
      "neut              0.33   0.31     0.32\n",
      "neg               0.73   0.73     0.73\n",
      "                                      \n",
      "accuracy                          0.74\n",
      "macro avg         0.63   0.63     0.63\n",
      "weighted avg      0.73   0.74     0.73\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(lambda x: ngrammaze_text(x, negatiaze_text), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.34      0.49      1241\n",
      "        neut       1.00      0.02      0.05       489\n",
      "         pos       0.63      0.98      0.76      2147\n",
      "\n",
      "    accuracy                           0.66      3877\n",
      "   macro avg       0.83      0.45      0.43      3877\n",
      "weighted avg       0.75      0.66      0.59      3877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(ngrammaze_text, train_data, test_data, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision recall f1-score\n",
      "pos               0.81   0.87     0.84\n",
      "neut              0.43   0.35     0.38\n",
      "neg               0.76   0.72     0.74\n",
      "                                      \n",
      "accuracy                          0.76\n",
      "macro avg         0.67   0.65     0.66\n",
      "weighted avg      0.75   0.76     0.75\n"
     ]
    }
   ],
   "source": [
    "get_cross_validation_report(ngrammaze_text, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
