{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    result = []\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            l = line.strip()\n",
    "            if len(l) > 0:\n",
    "                result.append(l)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10804"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \n",
    "    END_OF_SENT_PUNCT = ['.', '!', '?']\n",
    "    \n",
    "    \n",
    "    def generate(self, data: List[str]):\n",
    "        result = []\n",
    "        for data_line in data:\n",
    "            result += self.generate_for_text(data_line)\n",
    "        return result\n",
    "\n",
    "    def generate_for_text(self, text: str):\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return []\n",
    "\n",
    "        return self.generate_for_tokenized_sentences(sentences)\n",
    "    \n",
    "    def generate_for_tokenized_sentences(self, tokenized_sentences, tokenized_words = False):\n",
    "        size = len(tokenized_sentences)\n",
    "        start = 0\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < size:\n",
    "            num_of_sents = self.get_random_num_of_sentences()\n",
    "            end = i + num_of_sents if i + num_of_sents < size else size\n",
    "\n",
    "            data = self.__generate_data_for_sentences(tokenized_sentences[i:end], tokenized_words)\n",
    "            i = end\n",
    "\n",
    "            if len(data) > 0:\n",
    "                result.append(data)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def __generate_data_for_sentences(self, sentences: List[str], tokenized_words = False):\n",
    "\n",
    "        result = []\n",
    "        sentences_size = len(sentences)\n",
    "        for i in range(0, sentences_size):\n",
    "\n",
    "            sent_data = []\n",
    "            tokens = sentences[i] if tokenized_words else word_tokenize(sentences[i])\n",
    "            tokens_size = len(tokens) \n",
    "            should_set_end_word = False\n",
    "            for j in range(tokens_size- 1, -1, -1):\n",
    "                token = tokens[j]\n",
    "                \n",
    "                if token in self.END_OF_SENT_PUNCT:\n",
    "                    if i < sentences_size - 1:\n",
    "                        if j == tokens_size - 1:\n",
    "                            should_set_end_word = True\n",
    "                        continue\n",
    "                   \n",
    "                word = token\n",
    "                if j == 0 and i > 0:\n",
    "                    word = self.__randomly_lowercase_word(token, word_idx=j, sent_idx=i)\n",
    "                \n",
    "                if should_set_end_word:\n",
    "                    sent_data.insert(0, [word, True])\n",
    "                    should_set_end_word = False\n",
    "                else:\n",
    "                    sent_data.insert(0, [word, False])\n",
    "            \n",
    "            if len(sent_data) > 0:\n",
    "                result += sent_data\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __randomly_lowercase_word(self, word: str, word_idx: int, sent_idx: int):\n",
    "        if sent_idx == 0 or word_idx > 0:\n",
    "            return word\n",
    "\n",
    "        should_be_lower = random.choice([True, False])\n",
    "        if should_be_lower:\n",
    "            return word.lower()\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __is_punctuation(self, token: str):\n",
    "        if token in string.punctuation:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __is_end_word_in_sent(self, word: str, word_idx: int, num_token_in_sents: int):\n",
    "        if self.word_pattern.match(word) is None:\n",
    "            return False\n",
    "\n",
    "        if word_idx == num_token_in_sents - 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def get_random_num_of_sentences(self):\n",
    "        num = random.choices([2, 3, 4], weights=[12, 4, 3], k=1)[0]\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Thanks', False],\n",
       "  ['for', False],\n",
       "  ['talking', False],\n",
       "  ['to', False],\n",
       "  ['me', True],\n",
       "  ['let', False],\n",
       "  [\"'s\", False],\n",
       "  ['meet', False],\n",
       "  ['again', False],\n",
       "  ['tomorrow', True],\n",
       "  ['Bye', False],\n",
       "  ['.', False]]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_generator.generate_for_text(\"Thanks for talking to me. Let's meet again tomorrow. Bye.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset for full data corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator.generate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4533"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"data/dataset.json\"\n",
    "write_to_file(data=json.dumps(dataset), file=dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/dbabenko/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = categories[:9]\n",
    "val_categories = categories[9:12]\n",
    "test_categories = categories[9:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_brown_sentences = brown.sents(categories=train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_brown_dataset = dataset_generator.generate_for_tokenized_sentences(train_brown_sentences, tokenized_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_brown_dataset_file = \"data/brown-dataset-train.json\"\n",
    "write_to_file(data=json.dumps(train_brown_dataset), file=train_brown_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_brown_sentences = brown.sents(categories=val_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_brown_dataset = dataset_generator.generate_for_tokenized_sentences(val_brown_sentences, tokenized_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_brown_dataset_file = \"data/brown-dataset-val.json\"\n",
    "write_to_file(data=json.dumps(val_brown_dataset), file=val_brown_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brown_sentences = brown.sents(categories=test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brown_dataset = dataset_generator.generate_for_tokenized_sentences(test_brown_sentences, tokenized_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brown_dataset_file = \"data/brown-dataset-test.json\"\n",
    "write_to_file(data=json.dumps(test_brown_dataset), file=test_brown_dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate n-grams probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_sents = nltk.pos_tag_sents(train_brown_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tuple_ngram_to_str(ngram):\n",
    "    ngram_str = f\"{ngram[0]}\"\n",
    "    for i in range(1, len(ngram)):\n",
    "        ngram_str += f\"_{ngram[i]}\"\n",
    "    return ngram_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_dict(pos_tag_sents, n):\n",
    "    ngram_dict = dict()\n",
    "    for pos_tag_sent in pos_tag_sents:\n",
    "        ngrams = nltk.ngrams(list(zip(*pos_tag_sent))[1], n)\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            ngram_str = convert_tuple_ngram_to_str(ngram)\n",
    "            if ngram_str not in ngram_dict:\n",
    "                ngram_dict[ngram_str] = 1\n",
    "            else:\n",
    "                ngram_dict[ngram_str] += 1\n",
    "                \n",
    "    size = len(ngram_dict)\n",
    "    for ngram in ngram_dict:\n",
    "        ngram_dict[ngram] /= size\n",
    "        \n",
    "    return ngram_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_dict = generate_ngram_dict(pos_tag_sents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(data=json.dumps(bi_gram_dict), file=\"data/pos-bigrams.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_gram_dict = generate_ngram_dict(pos_tag_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(data=json.dumps(three_gram_dict), file=\"data/pos-threegrams.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
