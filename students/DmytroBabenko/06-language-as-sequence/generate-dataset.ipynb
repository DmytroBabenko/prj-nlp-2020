{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reuters.sents(['training/9880']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.sents(['training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    result = []\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            l = line.strip()\n",
    "            if len(l) > 0:\n",
    "                result.append(l)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10804"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \n",
    "    END_OF_SENT_PUNCT = ['.', '!', '?']\n",
    "    \n",
    "    \n",
    "    def generate(self, data: List[str]):\n",
    "        result = []\n",
    "        for data_line in data:\n",
    "            result += self.generate_for_text(data_line)\n",
    "        return result\n",
    "\n",
    "    def generate_for_text(self, text: str):\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        size = len(sentences)\n",
    "        start = 0\n",
    "        result = []\n",
    "        i = 0\n",
    "        while i < size:\n",
    "            num_of_sents = self.get_random_num_of_sentences()\n",
    "            end = i + num_of_sents if i + num_of_sents < len(sentences) else size\n",
    "\n",
    "            data = self.__generate_data_for_sentences(sentences[i:end])\n",
    "            i = end\n",
    "\n",
    "            if len(data) > 0:\n",
    "                result.append(data)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __generate_data_for_sentences(self, sentences: List[str]):\n",
    "\n",
    "        result = []\n",
    "        sentences_size = len(sentences)\n",
    "        for i in range(0, sentences_size):\n",
    "\n",
    "            sent_data = []\n",
    "            tokens = word_tokenize(sentences[i])\n",
    "            tokens_size = len(tokens) \n",
    "            should_set_end_word = False\n",
    "            for j in range(tokens_size- 1, -1, -1):\n",
    "                token = tokens[j]\n",
    "                \n",
    "                if token in self.END_OF_SENT_PUNCT:\n",
    "                    if j == tokens_size - 1 and i < sentences_size - 1:\n",
    "                        should_set_end_word = True\n",
    "                    continue\n",
    "                \n",
    "                if should_set_end_word:\n",
    "                    word = self.__randomly_lowercase_word(token, word_idx=j, sent_idx=i)\n",
    "                    sent_data.insert(0, [word, True])\n",
    "                    should_set_end_word = False\n",
    "                else:\n",
    "                    sent_data.insert(0, [token, False])\n",
    "            \n",
    "            if len(sent_data) > 0:\n",
    "                result += sent_data\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __randomly_lowercase_word(self, word: str, word_idx: int, sent_idx: int):\n",
    "        if sent_idx == 0 or word_idx > 0:\n",
    "            return word\n",
    "\n",
    "        should_be_lower = random.choice([True, False])\n",
    "        if should_be_lower:\n",
    "            return word.lower()\n",
    "\n",
    "        return word\n",
    "\n",
    "    def __is_punctuation(self, token: str):\n",
    "        if token in string.punctuation:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __is_end_word_in_sent(self, word: str, word_idx: int, num_token_in_sents: int):\n",
    "        if self.word_pattern.match(word) is None:\n",
    "            return False\n",
    "\n",
    "        if word_idx == num_token_in_sents - 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def get_random_num_of_sentences(self):\n",
    "        num = random.choices([2, 3, 4], weights=[12, 4, 3], k=1)[0]\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Thanks', False],\n",
       "  ['for', False],\n",
       "  ['talking', False],\n",
       "  ['to', False],\n",
       "  ['me', True],\n",
       "  ['Let', False],\n",
       "  [\"'s\", False],\n",
       "  ['meet', False],\n",
       "  ['again', False],\n",
       "  ['tomorrow', False]],\n",
       " [['Bye', False]]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_generator.generate_for_text(\"Thanks for talking to me. Let's meet again tomorrow. Bye.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset for full data corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator.generate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(data: str, file: str):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"data/dataset.json\"\n",
    "write_to_file(data=json.dumps(dataset), file=dataset_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
