{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable = 'ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Збираю речення з корпусу. У якості тренувального корпусу я використала дві книги - Гру Престолів та Портрет Доріана Грея. Додатково відфільтровую речення капслоком (назви розділів), речення з прямою мовою, так як Spacy sentence tokenizer їх дуже погано парсить. Загальна кількість тренувального корупусу - 59282 речень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59282"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data (text):\n",
    "    result = []\n",
    "    pos_tags = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        text = sent.text\n",
    "        # process sentence tokenizer's mistakes - poor parsing of direct speech\n",
    "        # process names of chapters, dates etc\n",
    "        if not re.search(r'[\"“]', text) and not re.match(r'[a-z,-]', text) \\\n",
    "        and not re.search(r'\\b[A-Z]+\\b', text):\n",
    "            result.append(text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def gather_sentences (path):\n",
    "    sentences = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as f: \n",
    "            paragraphs = f.read().split('\\n\\n')\n",
    "            for paragraph in paragraphs:\n",
    "                sentences.extend(get_data(paragraph.replace('\\n', ' ').strip()))\n",
    "                \n",
    "    return sentences        \n",
    "\n",
    "sentences = gather_sentences('train_data/')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data/sentences.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### У кожному реченні видаляю кінцеву пунктуацію"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    sent = re.sub('[?!(...).][\"”]?$', '', sent)\n",
    "    clean_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Приводжу до нижнього регістру перше слово кожного третього речення (якщо воно не починається з \"I\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_sents (sentences):\n",
    "    lowercased_sent = []\n",
    "\n",
    "    for i, x in enumerate(sentences):\n",
    "        sent = clean_sentences[i]\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            first_letter = sent.split()[0]\n",
    "\n",
    "            if first_letter != \"I\": \n",
    "                sent = first_letter.lower() + \" \" + ' '.join(sent.split()[1:])\n",
    "\n",
    "        lowercased_sent.append(sent)\n",
    "    return lowercased_sent\n",
    "\n",
    "lowercased = lowercase_sents(clean_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Формую пари з 2/3/4 склеєних речень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['robb and Bran and Rickon were his father’s sons, and he loved them still, yet Jon knew that he had never truly been one of them',\n",
       "  'Catelyn Stark had seen to that'],\n",
       " ['The grey walls of Winterfell might still haunt his dreams, but Castle Black was his life now, and his brothers were Sam and Grenn and Halder and Pyp and the other cast-outs who wore the black of the Night’s Watch',\n",
       "  'he wondered if he would ever see Benjen Stark again, to tell him'],\n",
       " ['This cursed heat had half the city in a fever to start, and now with all these visitors… last night we had a drowning, a tavern riot, three knife fights, a rape, two fires, robberies beyond count, and a drunken horse race down the Street of the Sisters',\n",
       "  'The night before a woman’s head was found in the Great Sept, floating in the rainbow pool'],\n",
       " ['no one seems to know how it got there or who it belongs to',\n",
       "  'Lord Renly Baratheon was less sympathetic'],\n",
       " ['Stout, jowly Janos Slynt puffed himself up like an angry frog, his bald pate reddening',\n",
       "  'ned asked, leaning forward']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_pairs (data, pair_number):\n",
    "    return [data[k:k+pair_number] for k in range(0, len(data), pair_number)]\n",
    "\n",
    "\n",
    "def combine_sentences (sentences):\n",
    "    \n",
    "    result = []\n",
    "    # 70% of sentences -> tuples with 2 elements\n",
    "    len_batch_1 = round(len(sentences) * 0.7)\n",
    "    result.extend(form_pairs(sentences[:len_batch_1], 2))\n",
    "\n",
    "    # 20% of sentences -> tuples with 3 elements\n",
    "    len_batch_2 = round(len(sentences) * 0.2) + len_batch_1\n",
    "    result.extend(form_pairs(sentences[len_batch_1:len_batch_2],3))\n",
    "\n",
    "    # 10% of sentences -> tuples with 4 elements\n",
    "    result.extend(form_pairs(sentences[len_batch_2:],4))\n",
    "\n",
    "    return result\n",
    "\n",
    "sentence_combinations = combine_sentences(lowercased)\n",
    "sentence_combinations[4500:4505]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### До вже склеєних речень (фактично - список списків) додаю лейбли True - якщо це кінець речення, False - початок/середина речення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels (sentences):\n",
    "    data = []\n",
    "\n",
    "    for pair in sentences:\n",
    "        sentence_result = []\n",
    "        for sent in pair:\n",
    "            doc = nlp(sent)\n",
    "            for i, token in enumerate(doc):\n",
    "\n",
    "                #mark the last token\n",
    "                if i == len(doc) - 1:\n",
    "                    sentence_result.append((token.text, \"True\"))\n",
    "                else:\n",
    "                    sentence_result.append((token.text, \"False\"))\n",
    "        data.append(sentence_result)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = add_labels(sentence_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('An', 'False'),\n",
       "  ('ethical', 'False'),\n",
       "  ('sympathy', 'False'),\n",
       "  ('in', 'False'),\n",
       "  ('an', 'False'),\n",
       "  ('artist', 'False'),\n",
       "  ('is', 'False'),\n",
       "  ('an', 'False'),\n",
       "  ('unpardonable', 'False'),\n",
       "  ('mannerism', 'False'),\n",
       "  ('of', 'False'),\n",
       "  ('style', 'True'),\n",
       "  ('no', 'False'),\n",
       "  ('artist', 'False'),\n",
       "  ('is', 'False'),\n",
       "  ('ever', 'False'),\n",
       "  ('morbid', 'True')],\n",
       " [('The', 'False'),\n",
       "  ('artist', 'False'),\n",
       "  ('can', 'False'),\n",
       "  ('express', 'False'),\n",
       "  ('everything', 'True'),\n",
       "  ('Thought', 'False'),\n",
       "  ('and', 'False'),\n",
       "  ('language', 'False'),\n",
       "  ('are', 'False'),\n",
       "  ('to', 'False'),\n",
       "  ('the', 'False'),\n",
       "  ('artist', 'False'),\n",
       "  ('instruments', 'False'),\n",
       "  ('of', 'False'),\n",
       "  ('an', 'False'),\n",
       "  ('art', 'True')]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Створюю тренувальні токени та тренувальні лейбли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [[pair[0] for pair in sentence] for sentence in train_data]\n",
    "train_labels = [[pair[1] for pair in sentence] for sentence in train_data]\n",
    "\n",
    "with open('train_tokens.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('train_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Нграми збираю на Gutenberg корпусі (3035 файлів). Він більше за інші корпуси підходить під мої тренувальні дані (худ літ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Збираю параграфи з усіх файлів директорії. Функція gather_paragraphs - розбиває пафаграфи по '\\n\\n',а потім додатково замінює перехід на нову лінію на пробіл у межах кожного параграфу. Це потрібно для подальшого коректного парсингу речень, так як текст у файлах має обмеження по кількості символів у однії лінії і має багато додаткових переходнів на нову лінію."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Функція gather_sentences - розбиває параграфи на речення "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Не додаю результат роботи коду до ноутбуку, так як він виконувався на іншому більш потужному ком'ютері (більше 3 годин)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_paragraphs(inp):\n",
    "    paragraphs = []\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(inp)):\n",
    "        with open(os.path.join(inp, filename), 'r', errors='ignore') as f: \n",
    "            paragraphs.extend(paragraph.replace('\\n', ' ').strip() for paragraph in f.read().split('\\n\\n'))\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def gather_sentences(paragraphs):\n",
    "    result = []\n",
    "\n",
    "    for doc in nlp.pipe(paragraphs, disable=[\"tagger\", \"ner\", \"textcat\"]):\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text\n",
    "            if not re.search(r'[\"“]', text) and not re.match(r'[a-z,-]', text) \\\n",
    "            and not re.search(r'\\b[A-Z]+\\b', text):\n",
    "                result.append(text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(sentences, n):\n",
    "    bigrams = defaultdict(int)\n",
    "    lemma_bigrams = defaultdict(int)\n",
    "    pos_bigrams = defaultdict(int)\n",
    "    dep_bigrams = defaultdict(int)\n",
    "    tag_bigrams = defaultdict(int)\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for sent in nlp.pipe(sentences, disable=[\"ner\", \"textcat\"]):\n",
    "        token_list = ['<S>']\n",
    "        pos_list = ['<S>']\n",
    "        dep_list = ['<S>']\n",
    "        lemma_list = ['<S>']\n",
    "        tag_list = ['<S>']\n",
    "                \n",
    "        for token in sent:\n",
    "            if token.text.strip():\n",
    "                token_list.append(token.text)\n",
    "                pos_list.append(token.pos_)\n",
    "                dep_list.append(token.dep_)\n",
    "                lemma_list.append(token.lemma_)\n",
    "                tag_list.append(token.tag_)\n",
    "\n",
    "        for i in range(len(token_list) - n + 1):\n",
    "            bigrams[tuple(token_list[i:i + n])] += 1\n",
    "            pos_bigrams[tuple(pos_list[i:i + n])] += 1\n",
    "            dep_bigrams[tuple(dep_list[i:i + n])] += 1\n",
    "            lemma_bigrams[tuple(lemma_list[i:i + n])] += 1\n",
    "            tag_bigrams[tuple(tag_list[i:i + n])] += 1\n",
    "\n",
    "        if j % 20000 == 0 or j == len(sentences) - 1:\n",
    "            print(datetime.datetime.now(), j, len(sentences) - j, 'left')\n",
    "            with open('data/bigrams.pickle', 'wb') as handle:\n",
    "                pickle.dump(bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('data/lemma_bigrams.pickle', 'wb') as handle:\n",
    "                pickle.dump(lemma_bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)                \n",
    "            with open('data/pos_bigrams.pickle', 'wb') as handle:\n",
    "                pickle.dump(pos_bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('data/dep_bigrams.pickle', 'wb') as handle:\n",
    "                pickle.dump(dep_bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('data/tag_bigrams.pickle', 'wb') as handle:\n",
    "                pickle.dump(tag_bigrams, handle, protocol=pickle.HIGHEST_PROTOCOL)                        \n",
    "\n",
    "        j += 1\n",
    "    \n",
    "    return bigrams, pos_bigrams, dep_bigrams, lemma_bigrams, tag_bigrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
