{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tokenize_uk\n",
    "\n",
    "PATH = \"UD_Ukrainian-IU\"\n",
    "\n",
    "with open(PATH + \"/uk_iu-ud-train.conllu\", \"r\") as f:\n",
    "    train_trees = parse(f.read())\n",
    "\n",
    "with open(PATH + \"/uk_iu-ud-dev.conllu\", \"r\") as f:\n",
    "    test_trees = parse(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Завдання 1: Покращення парсера залежностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Додаю SWAP для обрабки непроективних дерев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Для імплементації SWAP я керувалася статтями \"Transition-Based Techniques for NonProjective Dependency Parsing\" (https://www.diva-portal.org/smash/get/diva2:661423/FULLTEXT01.pdf) та \"Non-Projective Dependency Parsing in Expected Linear Time\" (https://www.aclweb.org/anthology/P09-1040.pdf), умову обрання дії SWAP в Оракулі підібрала частково експерементним шляхом, так як найбільш очевидна частина умови описується у статтях (id другого елементу стеку має бути меншим за id топу стеку), але не згадується які токени перевіряти на проективні залежності. Тож я дивилася на непроективні дерева у даних, та під них намагалася підібрати такі умови для Swap в Оракулі, щоб проективні зв'язки проставлятися точніше. \n",
    "\n",
    "Так, мені треба було виявити які саме токени стеку та черги порівнювати на непроективні залежності. Одна з ідей була порівняти тільки елементи стеку - топ стеку та другий елемент з кінця, але така конфігурація давала 0,04 покриття + погіршилися покриття та точність інших дій (SHIFT, LEFT і тд). На жаль, я не змогла знайти пояснення помилкам, тому обрала надалі працювати з конфігурацією, де перевіряю на непроективність топ стеку та топ черги, так як це дає кращу точність та покриття. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Actions(str, Enum):\n",
    "    SHIFT = \"shift\"\n",
    "    REDUCE = \"reduce\"\n",
    "    RIGHT = \"right\"\n",
    "    LEFT = \"left\"\n",
    "    SWAP = \"swap\"\n",
    "\n",
    "def oracle(stack, top_queue, relations):\n",
    "    top_stack = stack[-1]\n",
    "    if len(stack) > 1:\n",
    "        prev_stack = stack[-2]\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    elif top_queue[\"head\"] == top_stack[\"id\"]:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack[\"head\"] == top_queue[\"id\"]:\n",
    "        return Actions.LEFT\n",
    "    elif top_stack[\"id\"] in [i[0] for i in relations] and \\\n",
    "         (top_queue[\"head\"] < top_stack[\"id\"] or \\\n",
    "          [s for s in stack if s[\"head\"] == top_queue[\"id\"]]):\n",
    "        return Actions.REDUCE\n",
    "    elif len(stack) > 1 and prev_stack[\"id\"] != 0 and (prev_stack[\"id\"] < top_stack[\"id\"]):\n",
    "        a,c = sorted([top_queue[\"head\"], top_queue[\"id\"]])\n",
    "        b,d = sorted([top_stack[\"head\"], top_stack[\"id\"]])\n",
    "        if a < b and b < c and c < d:\n",
    "            return Actions.SWAP\n",
    "        return Actions.SHIFT\n",
    "    else:\n",
    "        return Actions.SHIFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def extract_features(stack, queue, tree):\n",
    "    features = dict()\n",
    "    if len(stack) > 0:\n",
    "        stack_top = stack[-1]\n",
    "        features[\"s0-word\"] = stack_top[\"form\"]\n",
    "        features[\"s0-lemma\"] = stack_top[\"lemma\"]\n",
    "        features[\"s0-tag\"] = stack_top[\"upostag\"]\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-tag\"] = stack[-2][\"upostag\"]\n",
    "    if queue:\n",
    "        queue_top = queue[0]\n",
    "        features[\"q0-word\"] = queue_top[\"form\"]\n",
    "        features[\"q0-lemma\"] = queue_top[\"lemma\"]\n",
    "        features[\"q0-tag\"] = queue_top[\"upostag\"]\n",
    "    if len(queue) > 1:\n",
    "        queue_next = queue[1]\n",
    "        features[\"q1-word\"] = queue_next[\"form\"]\n",
    "        features[\"q1-tag\"] = queue_next[\"upostag\"]\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])  \n",
    "\n",
    "def get_data(tree, extractor):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "\n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                        queue[0] if len(queue) > 0 else None,\n",
    "                        relations)\n",
    "        features.append(extractor(stack, queue, tree))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.SWAP:\n",
    "            queue.insert(0, stack.pop(-2))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "#     print(relations)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Залежності без операції SWAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<Також, у, кліпі, некрасиві, люди, .>\n",
      "[(2, 3), (5, 4), (6, 4)]\n"
     ]
    }
   ],
   "source": [
    "test_tree = train_trees[5227]\n",
    "print(test_tree)\n",
    "result = get_data([t for t in test_tree if type(t[\"id\"])==int], extract_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Залежності зі SWAP  --> матчиться більше залежностей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<Також, у, кліпі, некрасиві, люди, .>\n",
      "[(2, 3), (1, 4), (5, 4), (6, 4)]\n"
     ]
    }
   ],
   "source": [
    "test_tree = train_trees[5227]\n",
    "print(test_tree)\n",
    "result = get_data([t for t in test_tree if type(t[\"id\"])==int], extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rels (tree, clf, extractor, vec):   \n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    while queue or stack:\n",
    "        \n",
    "        features = extractor(stack, queue, tree)\n",
    "        feats_vectorized = vec.transform([features])\n",
    "        action = clf.predict(feats_vectorized)\n",
    "        \n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.SWAP:\n",
    "            queue.insert(0, stack.pop(-2))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    return relations\n",
    "\n",
    "result = get_rels(parsed_sentence, model2, extract_features, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def prepare_data (trees, extractor):\n",
    "    features, labels = [], []\n",
    "    for tree in trees:\n",
    "        tree_features, tree_labels = get_data([t for t in tree if type(t[\"id\"])==int], extractor)\n",
    "        features += tree_features\n",
    "        labels += tree_labels\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = prepare_data(train_trees, extract_features)\n",
    "test_features, test_labels = prepare_data(test_trees, extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train_model (train_features, test_features, train_labels):\n",
    "    vectorizer = DictVectorizer()\n",
    "    vec = vectorizer.fit(train_features)\n",
    "    train_features_vectorized = vec.transform(train_features)\n",
    "    test_features_vectorized = vec.transform(test_features)\n",
    "    lrc = LogisticRegression(random_state=42, solver=\"saga\",\n",
    "                             multi_class=\"multinomial\", max_iter=600, \n",
    "                             verbose=1)\n",
    "    model = lrc.fit(train_features_vectorized, train_labels)\n",
    "    predicted = lrc.predict(test_features_vectorized)\n",
    "    return predicted, model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 580 epochs took 132 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.85      0.87      0.86      6409\n",
      "      reduce       0.85      0.78      0.81      6837\n",
      "       right       0.75      0.79      0.77      6022\n",
      "       shift       0.84      0.87      0.86      6625\n",
      "        swap       0.60      0.16      0.26        73\n",
      "\n",
      "    accuracy                           0.82     25966\n",
      "   macro avg       0.78      0.69      0.71     25966\n",
      "weighted avg       0.82      0.82      0.82     25966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "predicted, model, vec = train_model(train_features, test_features, train_labels)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Додаю нові фічі 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "З цими фічами є покращення, буду використовувати.\n",
    "\n",
    "\n",
    "Вже під час тестування моделі на нових даних, я зрозуміла,що не зможу використовувати ці фічі, тому що вони рахують кількість дітей у  HEAD, а інформації про батька та дитину у мене у нерозмічених даних немає. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def merge_features (features1, features2):\n",
    "    features = copy.deepcopy(features1)\n",
    "    for i, f in enumerate(features2):\n",
    "        for k,v in f.items():\n",
    "            features[i][k] = v\n",
    "    return features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def count_children (tree, node_id):\n",
    "    return len([node['head'] for node in tree if node['head'] == node_id])\n",
    "\n",
    "def extract_features_1 (stack, queue, tree):\n",
    "    features = dict()\n",
    "    if len(stack) > 0:\n",
    "        features[\"s0-child-num\"] = count_children(tree, stack[-1][\"id\"])\n",
    "    if queue:\n",
    "        features[\"q0-child-num\"] = count_children(tree, queue[0][\"id\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 142 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.88      0.91      0.90      6409\n",
      "      reduce       0.86      0.82      0.84      6837\n",
      "       right       0.82      0.83      0.82      6022\n",
      "       shift       0.88      0.90      0.89      6625\n",
      "        swap       0.54      0.18      0.27        73\n",
      "\n",
      "    accuracy                           0.86     25966\n",
      "   macro avg       0.80      0.73      0.74     25966\n",
      "weighted avg       0.86      0.86      0.86     25966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "train_features_1, _ = prepare_data(train_trees, extract_features_1)\n",
    "test_features_1, _ = prepare_data(test_trees, extract_features_1)\n",
    "\n",
    "train_features_1 = merge_features(train_features, train_features_1)\n",
    "test_features_1 = merge_features(test_features, test_features_1)\n",
    "\n",
    "predicted, model1, vec1 = train_model (train_features_1, test_features_1, train_labels)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Ще додаю трохи нових фіч - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def extract_features_2 (stack, queue, tree):\n",
    "    features = dict()\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-child-num\"] = count_children(tree, stack[-2][\"id\"])\n",
    "    if len(queue) > 1:\n",
    "        features[\"q1-child-num\"] = count_children(tree, queue[1][\"id\"])\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-child-num\"] = count_children(tree, queue[2][\"id\"])\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-child-num\"] = count_children(tree, queue[3][\"id\"])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 146 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.88      0.92      0.90      6409\n",
      "      reduce       0.87      0.82      0.84      6837\n",
      "       right       0.82      0.83      0.83      6022\n",
      "       shift       0.88      0.90      0.89      6625\n",
      "        swap       0.64      0.19      0.29        73\n",
      "\n",
      "    accuracy                           0.87     25966\n",
      "   macro avg       0.82      0.73      0.75     25966\n",
      "weighted avg       0.86      0.87      0.86     25966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "train_features_2, _ = prepare_data(train_trees, extract_features_2)\n",
    "test_features_2, _ = prepare_data(test_trees, extract_features_2)\n",
    "\n",
    "train_features_2 = merge_features(train_features_1, train_features_2)\n",
    "test_features_2 = merge_features(test_features_1, test_features_2)\n",
    "\n",
    "predicted, model2, vec2 = train_model (train_features_2, test_features_2, train_labels)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Ще додаю трохи нових фіч - 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ці фічі використовувати не буду, так як падає якість"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def extract_features_3 (stack, queue, tree):\n",
    "    features = dict()\n",
    "    if len(stack) > 0:\n",
    "        features[\"s0-id\"] = stack[-1][\"id\"]\n",
    "    if queue:\n",
    "        features[\"q0-id\"] = queue[0][\"id\"]\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-id\"] = stack[-2][\"id\"]\n",
    "    if len(queue) > 1:\n",
    "        features[\"q1-id\"] = queue[1][\"id\"]\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-id\"] = queue[2][\"id\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-id\"] = queue[3][\"id\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 174 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.86      0.91      0.88      6409\n",
      "      reduce       0.85      0.78      0.82      6837\n",
      "       right       0.79      0.79      0.79      6022\n",
      "       shift       0.85      0.88      0.87      6625\n",
      "        swap       0.00      0.00      0.00        73\n",
      "\n",
      "    accuracy                           0.84     25966\n",
      "   macro avg       0.67      0.67      0.67     25966\n",
      "weighted avg       0.84      0.84      0.84     25966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeria/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "train_features_3, _ = prepare_data(train_trees, extract_features_3)\n",
    "test_features_3, _ = prepare_data(test_trees, extract_features_3)\n",
    "\n",
    "train_features_3 = merge_features(train_features_2, train_features_3)\n",
    "test_features_3 = merge_features(test_features_2, test_features_3)\n",
    "\n",
    "predicted, model3, vec3 = train_model(train_features_3, test_features_3, train_labels)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Пошук гіперпараметрів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Вирішила також спробувати зробити гіперпошук параметів, так як буду його робити також і для курсової роботи\n",
    "\n",
    "Як видно - у мене не було покращення якості після пошуку гіперпараметрів. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 8524 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        left       0.89      0.92      0.90      6409\n",
      "      reduce       0.87      0.83      0.85      6837\n",
      "       right       0.83      0.84      0.83      6022\n",
      "       shift       0.89      0.90      0.90      6625\n",
      "        swap       0.52      0.19      0.28        73\n",
      "\n",
      "    accuracy                           0.87     25966\n",
      "   macro avg       0.80      0.74      0.75     25966\n",
      "weighted avg       0.87      0.87      0.87     25966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 142.1min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features_2)\n",
    "train_features_vectorized = vec.transform(train_features_2)\n",
    "test_features_vectorized = vec.transform(test_features_2)\n",
    "\n",
    "logistic = LogisticRegression(random_state=42,\n",
    "                             multi_class=\"multinomial\", max_iter=600, \n",
    "                             verbose=1)\n",
    "hyperparameters = {\n",
    "    'penalty': ['l1', 'l2'],  \n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'solver':  ['sag', 'saga']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(logistic, hyperparameters, random_state=1,\n",
    "                         cv=3, verbose=0, n_jobs=-1)\n",
    "model = clf.fit(train_features_vectorized, train_labels)\n",
    "predicted = model.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'saga', 'penalty': 'l1', 'C': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Завдання 2: Використання парсера на нових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
    "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
    "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
    "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
    "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
    "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
    "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
    "\n",
    "PREP = [\"до\", \"на\"]\n",
    "\n",
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if \"coord\" in word.tag:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    elif \"PNCT\" in word.tag:\n",
    "        return \"PUNCT\"\n",
    "    elif word.normal_form in PREP:\n",
    "        return \"PREP\"\n",
    "    elif word.normal_form in DET:\n",
    "        return \"DET\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# приводжу речення до формату Conllu\n",
    "\n",
    "def parse_sent (sent):\n",
    "    i = 1\n",
    "    parsed_sentence = []\n",
    "    for token in tokenize_uk.tokenize_words(sent):\n",
    "        token = morph.parse(token)[0]\n",
    "        parsed_token =OrderedDict([(\"id\", i),(\"form\", token.word),(\"lemma\", token.normal_form),\n",
    "                                   (\"upostag\",  normalize_pos(token))])\n",
    "        i += 1\n",
    "        parsed_sentence.append(parsed_token)\n",
    "        \n",
    "    return parsed_sentence\n",
    "\n",
    "def get_rels (sent, clf, extractor, vec): \n",
    "    tree = parse_sent(sent)\n",
    "    stack, queue, relations,features = [ROOT], tree[:], [], []\n",
    "    i = 0\n",
    "    while queue or stack:\n",
    "        \n",
    "        features.append(extractor(stack, queue, tree))\n",
    "        feats_vectorized = vec.transform(features)\n",
    "\n",
    "        action = clf.predict(feats_vectorized)\n",
    "        if action[i] == 'shift':\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action[i] == 'reduce':\n",
    "            stack.pop()\n",
    "        elif action[i] == 'left':\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action[i] == 'right':\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action[i] == 'swap':\n",
    "            queue.insert(0, stack.pop(-2))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "            \n",
    "        i += 1\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2),\n",
       " (3, 2),\n",
       " (4, 5),\n",
       " (5, 3),\n",
       " (6, 2),\n",
       " (7, 8),\n",
       " (9, 10),\n",
       " (11, 12),\n",
       " (10, 12),\n",
       " (12, 8),\n",
       " (14, 13),\n",
       " (15, 8)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rels(\"У парку Кіото (Київ) зацвіла одна з найдовших у світі алея сакур.\", model2, extract_features, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 4), (2, 4), (1, 4), (9, 8), (10, 9)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rels(\"Я ніколи не думала, що зможу написати парсер залежностей!\", model2, extract_features, vec2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(зможу написати), (думала що) -> не проставило залежність "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
