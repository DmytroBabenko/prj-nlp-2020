{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nM_GpU3n7X57"
   },
   "source": [
    "# Infrastructure setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6otNhXODAyb"
   },
   "source": [
    "## Initilizing the runtime (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oCAEQDxSaiPK",
    "outputId": "3f1b70ec-4406-4020-a678-bc10c0216ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: local_data_path=/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/\n",
      "data path: /Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/\n",
      "Progress: |--------------------------------------------------| 1.0% Completee"
     ]
    }
   ],
   "source": [
    "%env local_data_path = /Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/\n",
    "!echo \"data path: ${local_data_path}\"\n",
    "\n",
    "# 1551 Dataset has lots of small files. Reading them from mounted Google Drive\n",
    "# sometimes takes significant time, so I have decided to checkout it to VM\n",
    "# DATA_1551_PATH = '/content/drive/My  Drive/Colab/homework-10/1551.gov.ua/raw/'\n",
    "DATA_1551_PATH = '/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/1551.gov.ua/raw/'\n",
    "\n",
    "\n",
    "#\n",
    "# Working with word2vec models\n",
    "#\n",
    "\n",
    "models_path = '/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/vec_models/'\n",
    "ubercorpus_cased_tokenized_word2vec_300d_path = models_path + 'ubercorpus.cased.tokenized.word2vec.300d'\n",
    "\n",
    "#\n",
    "# An amazing progress bar function\n",
    "#\n",
    "\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "printProgressBar(1, 10, prefix = 'Progress:', suffix = 'Complete', length = 50)        \n",
    "printProgressBar(1, 100, prefix = 'Progress:', suffix = 'Complete', length = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynSz3En5969y"
   },
   "source": [
    "## Downloading the data (once)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ego73YOhhNGv"
   },
   "source": [
    "### Donwloading 1551 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "AAC4Q8UM4dLQ",
    "outputId": "11052cc2-a857-4d90-cff1-53f723714498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/1551.gov.ua'...\r\n",
      "remote: Enumerating objects: 127370, done.\u001b[K\r\n",
      "remote: Total 127370 (delta 0), reused 0 (delta 0), pack-reused 127370\u001b[Kbjects:   6% (7643/127370), 3.89 MiB | 3.87 MiB/sReceiving objects:   8% (10190/127370), 6.82 MiB | 4.52 MiB/sReceiving objects:  10% (12737/127370), 10.04 MiB | 5.00 MiB/sReceiving objects:  12% (15285/127370), 10.04 MiB | 5.00 MiB/sReceiving objects:  14% (17832/127370), 13.23 MiB | 5.27 MiB/sReceiving objects:  15% (19460/127370), 13.23 MiB | 5.27 MiB/sReceiving objects:  17% (21653/127370), 16.96 MiB | 5.63 MiB/sReceiving objects:  19% (24201/127370), 20.60 MiB | 5.86 MiB/sReceiving objects:  21% (26748/127370), 20.60 MiB | 5.86 MiB/sReceiving objects:  22% (28022/127370), 23.75 MiB | 5.92 MiB/sReceiving objects:  24% (30569/127370), 23.75 MiB | 5.92 MiB/sReceiving objects:  26% (33117/127370), 27.11 MiB | 6.00 MiB/sReceiving objects:  27% (35373/127370), 27.11 MiB | 6.00 MiB/sReceiving objects:  29% (36938/127370), 30.48 MiB | 6.58 MiB/sReceiving objects:  31% (39485/127370), 33.54 MiB | 6.57 MiB/sReceiving objects:  33% (42033/127370), 33.54 MiB | 6.57 MiB/sReceiving objects:  34% (43306/127370), 36.72 MiB | 6.63 MiB/sReceiving objects:  36% (45854/127370), 36.72 MiB | 6.63 MiB/sReceiving objects:  38% (48401/127370), 39.72 MiB | 6.57 MiB/sReceiving objects:  40% (50948/127370), 42.85 MiB | 6.56 MiB/sReceiving objects:  42% (53496/127370), 42.85 MiB | 6.56 MiB/sReceiving objects:  44% (56043/127370), 45.88 MiB | 6.41 MiB/sReceiving objects:  45% (57317/127370), 45.88 MiB | 6.41 MiB/sReceiving objects:  47% (59864/127370), 48.86 MiB | 6.25 MiB/sReceiving objects:  49% (62412/127370), 51.81 MiB | 6.21 MiB/sReceiving objects:  50% (64243/127370), 51.81 MiB | 6.21 MiB/sReceiving objects:  52% (66233/127370), 55.15 MiB | 6.21 MiB/sReceiving objects:  54% (68780/127370), 58.14 MiB | 6.12 MiB/sReceiving objects:  56% (71328/127370), 58.14 MiB | 6.12 MiB/sReceiving objects:  57% (72601/127370), 60.96 MiB | 6.07 MiB/sReceiving objects:  59% (75149/127370), 63.78 MiB | 5.99 MiB/sReceiving objects:  61% (77696/127370), 63.78 MiB | 5.99 MiB/sReceiving objects:  62% (78970/127370), 67.00 MiB | 6.04 MiB/sReceiving objects:  64% (81517/127370), 67.00 MiB | 6.04 MiB/sReceiving objects:  66% (84065/127370), 70.26 MiB | 6.07 MiB/sReceiving objects:  67% (85997/127370), 70.26 MiB | 6.07 MiB/sReceiving objects:  69% (87886/127370), 73.43 MiB | 6.09 MiB/sReceiving objects:  71% (90433/127370), 76.20 MiB | 6.05 MiB/sReceiving objects:  72% (92801/127370), 76.20 MiB | 6.05 MiB/sReceiving objects:  74% (94254/127370), 79.39 MiB | 6.10 MiB/sReceiving objects:  76% (96802/127370), 82.19 MiB | 5.98 MiB/sReceiving objects:  78% (99349/127370), 82.19 MiB | 5.98 MiB/sReceiving objects:  79% (100623/127370), 82.19 MiB | 5.98 MiB/sReceiving objects:  81% (103170/127370), 85.47 MiB | 6.04 MiB/sReceiving objects:  83% (105718/127370), 88.60 MiB | 6.10 MiB/sReceiving objects:  84% (107680/127370), 88.60 MiB | 6.10 MiB/sReceiving objects:  86% (109539/127370), 91.98 MiB | 6.23 MiB/sReceiving objects:  88% (112086/127370), 94.84 MiB | 6.15 MiB/sReceiving objects:  89% (114617/127370), 94.84 MiB | 6.15 MiB/sReceiving objects:  91% (115907/127370), 98.04 MiB | 6.14 MiB/sReceiving objects:  93% (118455/127370), 98.04 MiB | 6.14 MiB/sReceiving objects:  95% (121002/127370), 101.16 MiB | 6.14 MiB/sReceiving objects:  97% (123549/127370), 104.46 MiB | 6.27 MiB/sReceiving objects:  99% (126097/127370), 104.46 MiB | 6.27 MiB/s\r\n",
      "Receiving objects: 100% (127370/127370), 108.40 MiB | 6.13 MiB/s, done.\r\n",
      "Resolving deltas: 100% (43/43), done.\r\n",
      "Updating files: 100% (127332/127332), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lang-uk/1551.gov.ua ${local_data_path}1551.gov.ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PP_s5xVgzlc"
   },
   "source": [
    "### Downloading **ubercorpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "ErzD1O8uiXuj",
    "outputId": "2ef68ac2-695c-4fb6-8b14-0a751ddc8975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-23 20:28:04--  https://lang.org.ua/static/downloads/models/ubercorpus.cased.tokenized.word2vec.300d.bz2\n",
      "Resolving lang.org.ua (lang.org.ua)... 95.216.74.77\n",
      "Connecting to lang.org.ua (lang.org.ua)|95.216.74.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 555203899 (529M) [application/octet-stream]\n",
      "Saving to: ‘/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/ubercorpus.cased.tokenized.word2vec.300d.bz2’\n",
      "\n",
      "ubercorpus.cased.to 100%[===================>] 529,48M  8,36MB/s    in 80s     \n",
      "\n",
      "2020-05-23 20:29:24 (6,62 MB/s) - ‘/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/ubercorpus.cased.tokenized.word2vec.300d.bz2’ saved [555203899/555203899]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P ${local_data_path} https://lang.org.ua/static/downloads/models/ubercorpus.cased.tokenized.word2vec.300d.bz2\n",
    "!bzip2 -d ${local_data_path}ubercorpus.cased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcVgFQPqle7c"
   },
   "source": [
    "## Installing pymorphy (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "I-gigYGcjufa",
    "outputId": "b6229eaa-6476-424c-e900-3c97e6a719b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pymorphy2-dicts-uk 2.4.1.1.1460299261\n",
      "Uninstalling pymorphy2-dicts-uk-2.4.1.1.1460299261:\n",
      "  Successfully uninstalled pymorphy2-dicts-uk-2.4.1.1.1460299261\n",
      "Found existing installation: pymorphy2 0.8\n",
      "Uninstalling pymorphy2-0.8:\n",
      "  Successfully uninstalled pymorphy2-0.8\n",
      "Collecting spacy\n",
      "  Using cached spacy-2.2.4-cp38-cp38-macosx_10_9_x86_64.whl (10.6 MB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (183 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc==7.4.0\n",
      "  Using cached thinc-7.4.0-cp38-cp38-macosx_10_9_x86_64.whl (2.2 MB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.2-cp38-cp38-macosx_10_9_x86_64.whl (112 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy) (46.4.0)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp38-cp38-macosx_10_9_x86_64.whl (3.7 MB)\n",
      "Collecting numpy>=1.15.0\n",
      "  Using cached numpy-1.18.4-cp38-cp38-macosx_10_9_x86_64.whl (15.2 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.2-cp38-cp38-macosx_10_9_x86_64.whl (19 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Installing collected packages: srsly, wasabi, plac, numpy, catalogue, tqdm, cymem, blis, murmurhash, preshed, thinc, urllib3, certifi, idna, chardet, requests, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 certifi-2020.4.5.1 chardet-3.0.4 cymem-2.0.3 idna-2.9 murmurhash-1.0.2 numpy-1.18.4 plac-1.1.3 preshed-3.0.2 requests-2.23.0 spacy-2.2.4 srsly-1.0.2 thinc-7.4.0 tqdm-4.46.0 urllib3-1.25.9 wasabi-0.6.0\n",
      "Collecting git+https://github.com/kmike/pymorphy2.git\n",
      "  Cloning https://github.com/kmike/pymorphy2.git to /private/var/folders/9s/pv07x6217fldw0yk1sy3m2x40000gn/T/pip-req-build-07x6w8g0\n",
      "  Running command git clone -q https://github.com/kmike/pymorphy2.git /private/var/folders/9s/pv07x6217fldw0yk1sy3m2x40000gn/T/pip-req-build-07x6w8g0\n",
      "Collecting pymorphy2-dicts-uk\n",
      "  Using cached pymorphy2_dicts_uk-2.4.1.1.1460299261-py2.py3-none-any.whl (5.0 MB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in ./venv/lib/python3.8/site-packages (from pymorphy2==0.8) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in ./venv/lib/python3.8/site-packages (from pymorphy2==0.8) (2.4.404381.4453942)\n",
      "Requirement already satisfied: docopt>=0.6 in ./venv/lib/python3.8/site-packages (from pymorphy2==0.8) (0.6.2)\n",
      "Using legacy setup.py install for pymorphy2, since package 'wheel' is not installed.\n",
      "Installing collected packages: pymorphy2-dicts-uk, pymorphy2\n",
      "    Running setup.py install for pymorphy2 ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pymorphy2-0.8 pymorphy2-dicts-uk-2.4.1.1.1460299261\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pymorphy2-dicts-uk -y\n",
    "!pip uninstall pymorphy2 -y\n",
    "!pip install spacy\n",
    "!pip install git+https://github.com/kmike/pymorphy2.git pymorphy2-dicts-uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5qAC84Om1Hs"
   },
   "source": [
    "## Initializing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "EYh7LjC6nIvW",
    "outputId": "758f9767-6d7b-4fc5-9bd2-bdf72d132fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Keating model...\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "595119it [00:55, 10683.68it/s]dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/ubercorpus.cased.tokenized.word2vec.300d\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from\n",
      "/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/data/ubercorpus.cased.tokenized.word2vec.300d\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "595290 entries, 595119 vectors\n"
     ]
    }
   ],
   "source": [
    "!rm -r ${local_data_path}vec_models\n",
    "!mkdir ${local_data_path}vec_models\n",
    "!\n",
    "# ubercorpus.cased.tokenized.word2vec.300d\n",
    "!python -m spacy init-model uk ${local_data_path}vec_models/ubercorpus.cased.tokenized.word2vec.300d --vectors-loc ${local_data_path}ubercorpus.cased.tokenized.word2vec.300d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULpS27_EtHxY"
   },
   "source": [
    "### Testing if pymorphy models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIa7JDFCteoD"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "ubercorpus_cased_tokenized_model = spacy.load(ubercorpus_cased_tokenized_word2vec_300d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lfPu6JG6zXCd",
    "outputId": "13d67a5e-7bee-474e-e3be-7b6786b79def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707450064131281\n"
     ]
    }
   ],
   "source": [
    "w1 = ubercorpus_cased_tokenized_model('кіт')\n",
    "w2 = ubercorpus_cased_tokenized_model('собака')\n",
    "print(w1.similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zOXEkIeWJXY"
   },
   "source": [
    "## Language detector (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Jp7LB2gyWINl",
    "outputId": "78591bca-23be-4455-d187-4f6ed2520be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycld2\n",
      "  Using cached pycld2-0.41.tar.gz (41.4 MB)\n",
      "Using legacy setup.py install for pycld2, since package 'wheel' is not installed.\n",
      "Installing collected packages: pycld2\n",
      "    Running setup.py install for pycld2 ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pycld2-0.41\n",
      "uk\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pycld2\n",
    "\n",
    "import pycld2 as cld2\n",
    "\n",
    "isReliable, textBytesFound, details = cld2.detect(\n",
    "    \"Відкриття спортивних залів не підлягає нинішнім карантинним пом'якшенням.\"\n",
    ")\n",
    "\n",
    "print (details[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iKDhKIfyT6p"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5-TR8rGvSCJ"
   },
   "source": [
    "## Doc2Vec (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agHZDsININwl"
   },
   "outputs": [],
   "source": [
    "def vectorize_without_stopwords(text, model):\n",
    "    words = model(text)\n",
    "    filtered_words = []\n",
    "\n",
    "    for w in words:\n",
    "      if w not in spacy.lang.uk.stop_words.STOP_WORDS:\n",
    "        filtered_words.append(w.text)\n",
    "\n",
    "    words = model(' '.join(filtered_words))\n",
    "    return words.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvgadytcuOZ6"
   },
   "source": [
    "## Loading the data in memory (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WXHXyDfNzl71",
    "outputId": "ea22973c-8204-4b2f-fde5-73d97d962407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files count 127329\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "Data loaded: 127329\n",
      "Time elapsed: 98.35836696624756 seconds\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import time\n",
    "\n",
    "def read_file(f):\n",
    "    with gzip.open(f, 'rt', encoding='utf-8') as inf:\n",
    "        j = json.load(inf)\n",
    "        return j[0]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = []\n",
    "files = []\n",
    "\n",
    "for f in glob.glob(DATA_1551_PATH+'*/*'):\n",
    "  files.append(f)\n",
    "\n",
    "print('Files count', len(files))\n",
    "counter = 0\n",
    "for file in files:\n",
    "  data.append(read_file(file))\n",
    "  counter+=1\n",
    "  if counter%100 ==0:\n",
    "    #print (len(files) - counter,'/',len(files))\n",
    "    printProgressBar(counter, len(files), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n",
    "printProgressBar(counter, len(files), prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "print()\n",
    "print ('Data loaded:',len(data))\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed:', end - start, 'seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Eg2gzBYxc3m"
   },
   "source": [
    "## Selecting a subset of data for fast processing (runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OKXPw4sxtRO"
   },
   "outputs": [],
   "source": [
    "import pycld2 as cld2\n",
    "category_id = {}\n",
    "\n",
    "def get_top_categories (n_categories, filter_ukrainian = False):\n",
    "  categories_count = {}\n",
    "  counter = 0\n",
    "\n",
    "  for data_item in data:\n",
    "    if data_item['CallZType'] not in categories_count:\n",
    "      categories_count[data_item['CallZType']] = 0\n",
    "      category_id[data_item['CallZType']] = counter\n",
    "      counter+=1\n",
    "    categories_count[data_item['CallZType']]+=1\n",
    "\n",
    "\n",
    "  def takeSecond(elem):\n",
    "      return elem[1]\n",
    "\n",
    "\n",
    "  category_statistics = []\n",
    "\n",
    "  for category in categories_count:\n",
    "    category_statistics.append((category, categories_count[category]))\n",
    "\n",
    "  category_statistics.sort(key=takeSecond, reverse=True)\n",
    "\n",
    "  selected_categories = {}\n",
    "\n",
    "  for category in category_statistics[:n_categories]:\n",
    "      selected_categories[category[0]] = []\n",
    "\n",
    "  for data_item in data:\n",
    "\n",
    "      if filter_ukrainian:\n",
    "        printable_str = ''.join(x for x in data_item['CallZText'] if x.isprintable())\n",
    "        isReliable, textBytesFound, details = cld2.detect(printable_str)\n",
    "        if details[0][1]=='uk' and data_item['CallZType'] in selected_categories:\n",
    "          selected_categories[data_item['CallZType']].append(data_item['CallZText'])\n",
    "\n",
    "      elif data_item['CallZType'] in selected_categories:\n",
    "        selected_categories[data_item['CallZType']].append(data_item['CallZText'])\n",
    "  \n",
    "  return selected_categories\n",
    "\n",
    "\n",
    "def get_vectorized_data(source_data, vectorizer, model):\n",
    "\n",
    "  features = []\n",
    "  labels = []\n",
    "\n",
    "  for category in source_data:\n",
    "    for data_item in source_data[category]:\n",
    "      vectorized_data = vectorizer(data_item, model)\n",
    "      if vectorized_data is not None:\n",
    "        features.append(vectorized_data)\n",
    "        labels.append(category_id[category])\n",
    "\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JF3OmmLpfjx6"
   },
   "source": [
    "# Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9-UQu_lu4s4"
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.23.1-cp38-cp38-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-0.15.1-py3-none-any.whl (298 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached scipy-1.4.1-cp38-cp38-macosx_10_9_x86_64.whl (28.8 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.0.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.18.4)\n",
      "Using legacy setup.py install for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: joblib, scipy, threadpoolctl, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed joblib-0.15.1 scikit-learn-0.23.1 scipy-1.4.1 sklearn-0.0 threadpoolctl-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "wsskq1fXKIGA",
    "outputId": "b3954556-623e-4904-ebb5-2d8c160a023f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44895 44895\n",
      "Accuracy: 0.5713417926565875\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.83      0.68      4450\n",
      "           4       0.53      0.54      0.54      1071\n",
      "          10       0.58      0.51      0.54       857\n",
      "          13       0.39      0.41      0.40       761\n",
      "          14       0.59      0.45      0.51      1946\n",
      "          20       0.36      0.35      0.36       820\n",
      "          25       0.69      0.45      0.54       852\n",
      "          35       0.61      0.47      0.53      2155\n",
      "          46       0.68      0.61      0.64      1190\n",
      "          91       0.54      0.26      0.35       714\n",
      "\n",
      "    accuracy                           0.57     14816\n",
      "   macro avg       0.56      0.49      0.51     14816\n",
      "weighted avg       0.58      0.57      0.56     14816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "filtered_data = get_top_categories(10)\n",
    "\n",
    "X, y = get_vectorized_data(filtered_data, vectorizer = vectorize_without_stopwords, model = ubercorpus_cased_tokenized_model)\n",
    "\n",
    "print (len(X), len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "neighbors = KNeighborsClassifier(n_neighbors=3, n_jobs = -1)\n",
    "neighbors.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "y_pred = neighbors.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "print()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cn9qwNJtBP1"
   },
   "source": [
    "## Baseline on MLPClassifier (from homework 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "o9BVWwBvtHOW",
    "outputId": "3cb68891-67ef-4434-d01a-b33ff89c1ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23789 23789\n",
      "Accuracy: 0.811743726913769\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.88      0.88      2159\n",
      "           4       0.84      0.85      0.84       651\n",
      "          10       0.88      0.90      0.89       411\n",
      "          13       0.68      0.70      0.69       362\n",
      "          14       0.87      0.85      0.86      1048\n",
      "          20       0.60      0.58      0.59       441\n",
      "          25       0.92      0.92      0.92       404\n",
      "          35       0.72      0.74      0.73      1200\n",
      "          46       0.87      0.90      0.88       731\n",
      "          91       0.61      0.52      0.57       444\n",
      "\n",
      "    accuracy                           0.81      7851\n",
      "   macro avg       0.79      0.78      0.78      7851\n",
      "weighted avg       0.81      0.81      0.81      7851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/11-nn/venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate (language_model):\n",
    "  filtered_data = get_top_categories(10, filter_ukrainian=True)\n",
    "\n",
    "  X, y = get_vectorized_data(filtered_data, vectorizer = vectorize_without_stopwords, model = language_model)\n",
    "\n",
    "  print (len(X), len(y))\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "  # {'alpha': 0.1, 'hidden_layer_sizes': 14, 'max_iter': 400, 'solver': 'lbfgs'}\n",
    "  clf = MLPClassifier(alpha = 0.1, hidden_layer_sizes = 14, max_iter = 400, solver = 'lbfgs', verbose=100, random_state=42)\n",
    "  clf.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "  y_pred = clf.predict(X_test)\n",
    "\n",
    "  print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "  print()\n",
    "\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "model = spacy.load(ubercorpus_cased_tokenized_word2vec_300d_path)\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-macosx_10_11_x86_64.whl (175.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.4 MB 4.9 MB/s eta 0:00:017\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.18.4)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.1-cp38-cp38-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.29.0-cp38-cp38-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in ./venv/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow) (46.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.15.0-py2.py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "\u001b[K     |████████████████████████████████| 777 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<4.1,>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy setup.py install for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for wrapt, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for absl-py, since package 'wheel' is not installed.\n",
      "Installing collected packages: opt-einsum, protobuf, termcolor, wheel, grpcio, gast, google-pasta, tensorflow-estimator, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, absl-py, tensorboard-plugin-wit, werkzeug, markdown, tensorboard, h5py, astunparse, keras-preprocessing, wrapt, tensorflow\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.0 gast-0.3.3 google-auth-1.15.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.29.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.2.1 protobuf-3.12.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.2.1 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in ./venv/lib/python3.8/site-packages (from keras) (1.1.2)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in ./venv/lib/python3.8/site-packages (from keras) (1.4.1)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in ./venv/lib/python3.8/site-packages (from keras) (1.18.4)\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.8/site-packages (from keras) (1.15.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp38-cp38-macosx_10_9_x86_64.whl size=44624 sha256=193ca6038ef6e09ad578e05bd01e0334ccba97bd044feebbc3756a9225829b0d\n",
      "  Stored in directory: /Users/dmytromindra/Library/Caches/pip/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 pyyaml-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23789 23789\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "language_model = spacy.load(ubercorpus_cased_tokenized_word2vec_300d_path)\n",
    "filtered_data = get_top_categories(10, filter_ukrainian=True)\n",
    "raw_X, raw_y = get_vectorized_data(filtered_data, vectorizer = vectorize_without_stopwords, model = language_model)\n",
    "print (len(raw_X), len(raw_X))\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23789, 300)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 200)               2200      \n",
      "=================================================================\n",
      "Total params: 5,210\n",
      "Trainable params: 5,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.2078 - accuracy: 0.6203\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6630 - accuracy: 0.7702\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.5712 - accuracy: 0.7989\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.5266 - accuracy: 0.8088\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5018 - accuracy: 0.8194\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.4833 - accuracy: 0.8238\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.4690 - accuracy: 0.8295\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4580 - accuracy: 0.8341\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4488 - accuracy: 0.8345\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4388 - accuracy: 0.8399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x196c1aaf0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, LSTM, Embedding, Bidirectional, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "X = np.array(raw_X)\n",
    "y = np.array(raw_y)\n",
    "\n",
    "print (X.shape)\n",
    "\n",
    "input_dim = np.array(X_train).shape[1]\n",
    "\n",
    "def get_model():\n",
    "    global input_dim\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(200, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=get_model, nb_epoch=10, batch_size=5, verbose=0)\n",
    "estimator.fit(np.array(X_train), np.array(y_train), epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8216787670360464\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.92      0.89      2159\n",
      "           4       0.79      0.86      0.83       651\n",
      "          10       0.95      0.86      0.90       411\n",
      "          13       0.72      0.69      0.71       362\n",
      "          14       0.90      0.86      0.88      1048\n",
      "          20       0.77      0.40      0.53       441\n",
      "          25       0.93      0.92      0.93       404\n",
      "          35       0.66      0.88      0.76      1200\n",
      "          46       0.94      0.86      0.90       731\n",
      "          91       0.76      0.39      0.51       444\n",
      "\n",
      "    accuracy                           0.82      7851\n",
      "   macro avg       0.83      0.76      0.78      7851\n",
      "weighted avg       0.83      0.82      0.82      7851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = estimator.predict(np.array(X_test))\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 0.81 accuracy with Logistic Regression. Simple NN showed accuracy 0.82 on the same data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Homework-10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
