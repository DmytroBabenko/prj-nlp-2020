{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-5QD4Cdgj5m"
   },
   "source": [
    "# Домашняя работа #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**:\n",
    "1. Google Colab\n",
    "\n",
    "    1.1. Есть возможность сохранять файлы на Google Drive и подгружать их оттуда.\n",
    "    \n",
    "    1.2. Google без предупреждения делает recycle для инстансов как по таймауту, так и по потреблению памяти и процессора.\n",
    "    \n",
    "    1.3. Google Colab Pro никак не помог решить проблемы из 1.2\n",
    "    \n",
    "    1.4. Делать работу в Google Colab очень сложно. Долелывал ее на локальной машине.\n",
    "    \n",
    "    \n",
    "2. Библиотека stanza работала значительно медленнее чем spacy. Пришлось переписать извлечение фич и переключить его на spacy.\n",
    "\n",
    "3. Я попробовал собрать n-grams на своем корпусе, но их оказалось очень мало и добавление n-grams ухудшило результат на логистической регрессии. Код сбора n-grams остался в репозитории, но результаты их применения я не включил.\n",
    "\n",
    "4. Я поработал с векторизацией данных и логистической регрессией.\n",
    "\n",
    "5. Я попробовал Conditional random fields (но не смог добиться приемлемого результата с ним)\n",
    "\n",
    "6. Попробовал подбор параметров при помощи RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyaifoTXj2XI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "# I plan to have quite a number of pre-processed data that would be stored in 'data' folder\n",
    "DATA_PATH = '/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/06-language-as-sequence/'\n",
    "\n",
    "RAW_DATA_FILENAME = DATA_PATH + 'data/masc_sentences.tsv'\n",
    "RAW_SENTENCES_FILENAME = DATA_PATH + 'data/stripped_masc_sentences.json'\n",
    "BASELINE_FEATURES_FILENAME = DATA_PATH + 'data/baseline_features_dataset.json'\n",
    "BASELINE_LABELS_FILENAME = DATA_PATH + 'data/baseline_labels_dataset.json'\n",
    "IMPROVED_FEATURES_FILENAME = DATA_PATH + 'data/improved_features_dataset.json'\n",
    "IMPROVED_LABELS_FILENAME = DATA_PATH + 'data/improved_labels_dataset.json'\n",
    "RUN_ON_TEST_RAW_FILENAME = DATA_PATH + 'data/run-on-test.json'\n",
    "RUN_ON_TEST_TOKENS_FILENAME = DATA_PATH + 'data/run_on_test_tokens_dataset.json'\n",
    "RUN_ON_TEST_LABELS_FILENAME = DATA_PATH + 'data/run_on_test_labels_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ySFAiRBiwvH"
   },
   "outputs": [],
   "source": [
    "def file_exists(filename):\n",
    "  if os.path.isfile(filename):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BQjHnPI8dXIb",
    "outputId": "3d0848d7-fe44-4aa1-90b9-4dfdd0d274b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 85887\n"
     ]
    }
   ],
   "source": [
    "def read_raw_data():\n",
    "    \"\"\"\n",
    "    Reading the original file\n",
    "    Stripping it from the meta information and saving in json\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(RAW_DATA_FILENAME) as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "        for row in reader:\n",
    "            data.append(list(row.items())[6][1])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data = read_raw_data()\n",
    "\n",
    "with open(RAW_SENTENCES_FILENAME, 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "\n",
    "print ('Total sentences in dataset:', len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "azAnWNEye0AJ",
    "outputId": "8af70421-fafd-4b53-da0a-e9f5cdd8bf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences loaded: 85887\n"
     ]
    }
   ],
   "source": [
    "def read_raw_data_from_json():\n",
    "    \"\"\"\n",
    "    Reading the original file\n",
    "    Stripping it from the meta information and saving in json\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(RAW_DATA_FILENAME) as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
    "        for row in reader:\n",
    "            data.append(list(row.items())[6][1])\n",
    "\n",
    "    return data\n",
    "\n",
    "masc_dataset = read_raw_data_from_json()\n",
    "print ('Total sentences loaded:',len(masc_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "f-Mpiv76fiej",
    "outputId": "4bbabba0-32af-4c7c-beb4-b1a4ee0fb28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./venv/lib/python3.8/site-packages (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.18.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy) (46.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy) (4.46.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глобальная переменная **BASELINE** переключает набор фич, которые **extract_token_features** извлекает из текста. Это не элегантное решение и в продакшн коде так делать не стоит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TALLeFIPfze2"
   },
   "outputs": [],
   "source": [
    "from random import random, randrange\n",
    "\n",
    "# a function for run-on sentence simulation\n",
    "def decision(probability):\n",
    "    return random() < probability\n",
    "\n",
    "# a distance to the sentence root\n",
    "def get_root_distance(word_no, sentence):\n",
    "    max_distance = 8\n",
    "    word = sentence.words[word_no]\n",
    "    distance = 0\n",
    "\n",
    "    while distance <= max_distance and word.head != 0:\n",
    "        word = sentence.words[word.head - 1]\n",
    "        distance += 1\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "BASELINE  = False \n",
    "\n",
    "def extract_token_features(doc, token_no):\n",
    "    token = doc[token_no]\n",
    "    text = token.text\n",
    "    \n",
    "    is_upper = is_upper_text(text)\n",
    "        \n",
    "    left_token = ''\n",
    "    left_pos = ''\n",
    "    right_token = ''\n",
    "    right_pos = ''\n",
    "    is_right_upper = False\n",
    "    is_sentence_end = False\n",
    "    if token_no > 0:\n",
    "                left_token = doc[token_no - 1].text\n",
    "                left_pos = doc[token_no - 1].pos_\n",
    "    if token_no <= len(doc) - 2:\n",
    "                right_token = doc[token_no + 1].text\n",
    "                right_pos = doc[token_no + 1].pos_\n",
    "\n",
    "                if len(doc[token_no + 1].text) > 0 and \\\n",
    "                        doc[token_no + 1].text[0].isupper():\n",
    "                    is_right_upper = True\n",
    "    if token_no == len(doc) - 2 and right_token =='.':\n",
    "                is_sentence_end = True\n",
    "    if token_no == len(doc) - 1 and text != '.':\n",
    "                is_sentence_end = True\n",
    "    \n",
    "    global BASELINE\n",
    "    if BASELINE:\n",
    "        features = {\n",
    "                        'text': token.text,\n",
    "                        'pos': token.pos_,\n",
    "                        'lemma': token.lemma_,\n",
    "                        'deprel': token.dep_\n",
    "                    }\n",
    "    else:\n",
    "        features = {'text': token.text,\n",
    "                        'pos': token.pos_,\n",
    "                        'lemma': token.lemma_,\n",
    "                        'deprel': token.dep_,\n",
    "                        'head': token.head.text,\n",
    "                        'is_upper': is_upper,\n",
    "                        'left_token': left_token,\n",
    "                        'left_pos': left_pos,\n",
    "                        'right_token': right_token,\n",
    "                        'right_pos': right_pos,\n",
    "                        'is_right_upper': is_right_upper,\n",
    "                        'is_sentence_end' : is_sentence_end\n",
    "                        }\n",
    "    return features\n",
    "\n",
    " \n",
    "\n",
    "def is_upper_text(text):\n",
    "    if len(text) > 0 and text[0].isupper():\n",
    "        is_upper = True\n",
    "    else:\n",
    "        is_upper = False\n",
    "    return is_upper\n",
    "\n",
    "def extract_features(sentences):\n",
    "    sentence_count = len(sentences)\n",
    "    current_shift = 0\n",
    "    sentence_end_positions = []\n",
    "    \n",
    "    text = concatenate_sentences(current_shift,sentence_count,sentence_end_positions,sentences)\n",
    "    \n",
    "    doc = nlp(text)    \n",
    "    labels, tokens = calculate_text_labels_and_features(doc,sentence_count,sentence_end_positions)\n",
    "\n",
    "    return (tokens, labels)\n",
    "\n",
    "\n",
    "def concatenate_sentences(current_shift, sentence_count, sentence_end_positions, sentences):\n",
    "    text = ''\n",
    "    for sentence_no in range(sentence_count):\n",
    "        sentence = sentences[sentence_no]\n",
    "        if sentence_no != sentence_count - 1:\n",
    "                if sentence.endswith('.'):\n",
    "                    sentence = sentence[:-1]\n",
    "                sentence_end_positions.append(current_shift + len(sentence) - 1)\n",
    "                current_shift += len(sentence)\n",
    "        if sentence_no > 0:\n",
    "                lower_case_decision = decision(0.5)\n",
    "                if lower_case_decision:\n",
    "                    sentence = sentence[:1].lower() + sentence[1:]\n",
    "        if text == '':\n",
    "            text = sentence\n",
    "        else:\n",
    "            text = text + ' ' + sentence\n",
    "    return text\n",
    "\n",
    "def calculate_text_labels_and_features(doc, sentence_count, sentence_end_positions):\n",
    "    current_sentnse = 0\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for token_no in range(len(doc)):\n",
    "        token = doc[token_no]\n",
    "        features = extract_token_features(doc,token_no)\n",
    "\n",
    "        label = False\n",
    "        \n",
    "        end_char = token.idx + len(token.text)\n",
    "\n",
    "        if sentence_count > 1 and \\\n",
    "                current_sentnse < sentence_count - 1 and \\\n",
    "                end_char > sentence_end_positions[current_sentnse]:\n",
    "            current_sentnse += 1\n",
    "            label = True\n",
    "\n",
    "        tokens.append(features)\n",
    "        labels.append(label)\n",
    "    return labels, tokens\n",
    "\n",
    " \n",
    "\n",
    "def prepare_dataset(data):\n",
    "    print('Processing data ...')\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    counter = 0\n",
    "    while len(data) > 0:\n",
    "\n",
    "        batch_size = randrange(4) + 1\n",
    "        batch = data[:batch_size]\n",
    "        data = data[batch_size:]\n",
    "        dataset = extract_features(batch)\n",
    "        tokens.append(dataset[0])\n",
    "        labels.append(dataset[1])\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print('to go', len(data))\n",
    "\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'text': 'If',\n",
       "   'pos': 'SCONJ',\n",
       "   'lemma': 'if',\n",
       "   'deprel': 'mark',\n",
       "   'head': 'need',\n",
       "   'is_upper': True,\n",
       "   'left_token': '',\n",
       "   'left_pos': '',\n",
       "   'right_token': 'you',\n",
       "   'right_pos': 'PRON',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'you',\n",
       "   'pos': 'PRON',\n",
       "   'lemma': '-PRON-',\n",
       "   'deprel': 'nsubj',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'If',\n",
       "   'left_pos': 'SCONJ',\n",
       "   'right_token': 'do',\n",
       "   'right_pos': 'AUX',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'do',\n",
       "   'pos': 'AUX',\n",
       "   'lemma': 'do',\n",
       "   'deprel': 'aux',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'you',\n",
       "   'left_pos': 'PRON',\n",
       "   'right_token': 'n’t',\n",
       "   'right_pos': 'PART',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'n’t',\n",
       "   'pos': 'PART',\n",
       "   'lemma': 'not',\n",
       "   'deprel': 'neg',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'do',\n",
       "   'left_pos': 'AUX',\n",
       "   'right_token': 'need',\n",
       "   'right_pos': 'VERB',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'need',\n",
       "   'pos': 'VERB',\n",
       "   'lemma': 'need',\n",
       "   'deprel': 'advcl',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'n’t',\n",
       "   'left_pos': 'PART',\n",
       "   'right_token': 'a',\n",
       "   'right_pos': 'DET',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'a',\n",
       "   'pos': 'DET',\n",
       "   'lemma': 'a',\n",
       "   'deprel': 'det',\n",
       "   'head': 'component',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'need',\n",
       "   'left_pos': 'VERB',\n",
       "   'right_token': 'particular',\n",
       "   'right_pos': 'ADJ',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'particular',\n",
       "   'pos': 'ADJ',\n",
       "   'lemma': 'particular',\n",
       "   'deprel': 'amod',\n",
       "   'head': 'component',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'a',\n",
       "   'left_pos': 'DET',\n",
       "   'right_token': 'component',\n",
       "   'right_pos': 'NOUN',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'component',\n",
       "   'pos': 'NOUN',\n",
       "   'lemma': 'component',\n",
       "   'deprel': 'dobj',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'particular',\n",
       "   'left_pos': 'ADJ',\n",
       "   'right_token': 'of',\n",
       "   'right_pos': 'ADP',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'of',\n",
       "   'pos': 'ADP',\n",
       "   'lemma': 'of',\n",
       "   'deprel': 'prep',\n",
       "   'head': 'component',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'component',\n",
       "   'left_pos': 'NOUN',\n",
       "   'right_token': 'the',\n",
       "   'right_pos': 'DET',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'the',\n",
       "   'pos': 'DET',\n",
       "   'lemma': 'the',\n",
       "   'deprel': 'det',\n",
       "   'head': 'pipeline',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'of',\n",
       "   'left_pos': 'ADP',\n",
       "   'right_token': 'pipeline',\n",
       "   'right_pos': 'NOUN',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'pipeline',\n",
       "   'pos': 'NOUN',\n",
       "   'lemma': 'pipeline',\n",
       "   'deprel': 'pobj',\n",
       "   'head': 'of',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'the',\n",
       "   'left_pos': 'DET',\n",
       "   'right_token': '–',\n",
       "   'right_pos': 'PUNCT',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': '–',\n",
       "   'pos': 'PUNCT',\n",
       "   'lemma': '–',\n",
       "   'deprel': 'punct',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'pipeline',\n",
       "   'left_pos': 'NOUN',\n",
       "   'right_token': 'for',\n",
       "   'right_pos': 'ADP',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'for',\n",
       "   'pos': 'ADP',\n",
       "   'lemma': 'for',\n",
       "   'deprel': 'prep',\n",
       "   'head': 'tagger',\n",
       "   'is_upper': False,\n",
       "   'left_token': '–',\n",
       "   'left_pos': 'PUNCT',\n",
       "   'right_token': 'example',\n",
       "   'right_pos': 'NOUN',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'example',\n",
       "   'pos': 'NOUN',\n",
       "   'lemma': 'example',\n",
       "   'deprel': 'pobj',\n",
       "   'head': 'for',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'for',\n",
       "   'left_pos': 'ADP',\n",
       "   'right_token': ',',\n",
       "   'right_pos': 'PUNCT',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': ',',\n",
       "   'pos': 'PUNCT',\n",
       "   'lemma': ',',\n",
       "   'deprel': 'punct',\n",
       "   'head': 'tagger',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'example',\n",
       "   'left_pos': 'NOUN',\n",
       "   'right_token': 'the',\n",
       "   'right_pos': 'DET',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'the',\n",
       "   'pos': 'DET',\n",
       "   'lemma': 'the',\n",
       "   'deprel': 'det',\n",
       "   'head': 'tagger',\n",
       "   'is_upper': False,\n",
       "   'left_token': ',',\n",
       "   'left_pos': 'PUNCT',\n",
       "   'right_token': 'tagger',\n",
       "   'right_pos': 'NOUN',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'tagger',\n",
       "   'pos': 'NOUN',\n",
       "   'lemma': 'tagger',\n",
       "   'deprel': 'dep',\n",
       "   'head': 'need',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'the',\n",
       "   'left_pos': 'DET',\n",
       "   'right_token': 'or',\n",
       "   'right_pos': 'CCONJ',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'or',\n",
       "   'pos': 'CCONJ',\n",
       "   'lemma': 'or',\n",
       "   'deprel': 'cc',\n",
       "   'head': 'tagger',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'tagger',\n",
       "   'left_pos': 'NOUN',\n",
       "   'right_token': 'the',\n",
       "   'right_pos': 'DET',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'the',\n",
       "   'pos': 'DET',\n",
       "   'lemma': 'the',\n",
       "   'deprel': 'det',\n",
       "   'head': 'parser',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'or',\n",
       "   'left_pos': 'CCONJ',\n",
       "   'right_token': 'parser',\n",
       "   'right_pos': 'NOUN',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'parser',\n",
       "   'pos': 'NOUN',\n",
       "   'lemma': 'parser',\n",
       "   'deprel': 'conj',\n",
       "   'head': 'tagger',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'the',\n",
       "   'left_pos': 'DET',\n",
       "   'right_token': ',',\n",
       "   'right_pos': 'PUNCT',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': ',',\n",
       "   'pos': 'PUNCT',\n",
       "   'lemma': ',',\n",
       "   'deprel': 'punct',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'parser',\n",
       "   'left_pos': 'NOUN',\n",
       "   'right_token': 'you',\n",
       "   'right_pos': 'PRON',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'you',\n",
       "   'pos': 'PRON',\n",
       "   'lemma': '-PRON-',\n",
       "   'deprel': 'nsubj',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': ',',\n",
       "   'left_pos': 'PUNCT',\n",
       "   'right_token': 'can',\n",
       "   'right_pos': 'VERB',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'can',\n",
       "   'pos': 'VERB',\n",
       "   'lemma': 'can',\n",
       "   'deprel': 'aux',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'you',\n",
       "   'left_pos': 'PRON',\n",
       "   'right_token': 'disable',\n",
       "   'right_pos': 'VERB',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'disable',\n",
       "   'pos': 'VERB',\n",
       "   'lemma': 'disable',\n",
       "   'deprel': 'ROOT',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'can',\n",
       "   'left_pos': 'VERB',\n",
       "   'right_token': 'loading',\n",
       "   'right_pos': 'VERB',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'loading',\n",
       "   'pos': 'VERB',\n",
       "   'lemma': 'load',\n",
       "   'deprel': 'xcomp',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'disable',\n",
       "   'left_pos': 'VERB',\n",
       "   'right_token': 'it',\n",
       "   'right_pos': 'PRON',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False},\n",
       "  {'text': 'it',\n",
       "   'pos': 'PRON',\n",
       "   'lemma': '-PRON-',\n",
       "   'deprel': 'dobj',\n",
       "   'head': 'loading',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'loading',\n",
       "   'left_pos': 'VERB',\n",
       "   'right_token': '.',\n",
       "   'right_pos': 'PUNCT',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': True},\n",
       "  {'text': '.',\n",
       "   'pos': 'PUNCT',\n",
       "   'lemma': '.',\n",
       "   'deprel': 'punct',\n",
       "   'head': 'disable',\n",
       "   'is_upper': False,\n",
       "   'left_token': 'it',\n",
       "   'left_pos': 'PRON',\n",
       "   'right_token': '',\n",
       "   'right_pos': '',\n",
       "   'is_right_upper': False,\n",
       "   'is_sentence_end': False}],\n",
       " [False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False,\n",
       "  False])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "extract_features([\"If you don’t need a particular component of the pipeline – for example, the tagger or the parser, you can disable loading it.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dcaNpxOoy5bi",
    "outputId": "ef061696-777b-4a9e-d2ab-6a199220a2ea",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data ...\n",
      "to go 85648\n",
      "to go 85380\n",
      "to go 85128\n",
      "to go 84872\n",
      "to go 84625\n",
      "to go 84378\n",
      "to go 84137\n",
      "to go 83875\n",
      "to go 83638\n",
      "to go 83400\n",
      "to go 83160\n",
      "to go 82903\n",
      "to go 82654\n",
      "to go 82438\n",
      "to go 82192\n",
      "to go 81939\n",
      "to go 81701\n",
      "to go 81447\n",
      "to go 81187\n",
      "to go 80941\n",
      "to go 80678\n",
      "to go 80414\n",
      "to go 80161\n",
      "to go 79922\n",
      "to go 79683\n",
      "to go 79427\n",
      "to go 79178\n",
      "to go 78938\n",
      "to go 78692\n",
      "to go 78446\n",
      "to go 78184\n",
      "to go 77923\n",
      "to go 77667\n",
      "to go 77430\n",
      "to go 77184\n",
      "to go 76931\n",
      "to go 76663\n",
      "to go 76418\n",
      "to go 76177\n",
      "to go 75922\n",
      "to go 75683\n",
      "to go 75437\n",
      "to go 75213\n",
      "to go 74976\n",
      "to go 74715\n",
      "to go 74453\n",
      "to go 74186\n",
      "to go 73934\n",
      "to go 73666\n",
      "to go 73423\n",
      "to go 73179\n",
      "to go 72924\n",
      "to go 72694\n",
      "to go 72437\n",
      "to go 72171\n",
      "to go 71917\n",
      "to go 71668\n",
      "to go 71422\n",
      "to go 71153\n",
      "to go 70896\n",
      "to go 70658\n",
      "to go 70409\n",
      "to go 70162\n",
      "to go 69901\n",
      "to go 69649\n",
      "to go 69396\n",
      "to go 69161\n",
      "to go 68901\n",
      "to go 68660\n",
      "to go 68418\n",
      "to go 68172\n",
      "to go 67910\n",
      "to go 67673\n",
      "to go 67422\n",
      "to go 67185\n",
      "to go 66941\n",
      "to go 66683\n",
      "to go 66436\n",
      "to go 66186\n",
      "to go 65940\n",
      "to go 65686\n",
      "to go 65453\n",
      "to go 65193\n",
      "to go 64954\n",
      "to go 64707\n",
      "to go 64452\n",
      "to go 64206\n",
      "to go 63978\n",
      "to go 63736\n",
      "to go 63500\n",
      "to go 63250\n",
      "to go 63007\n",
      "to go 62755\n",
      "to go 62499\n",
      "to go 62251\n",
      "to go 62012\n",
      "to go 61754\n",
      "to go 61502\n",
      "to go 61234\n",
      "to go 60981\n",
      "to go 60759\n",
      "to go 60523\n",
      "to go 60269\n",
      "to go 60025\n",
      "to go 59769\n",
      "to go 59517\n",
      "to go 59271\n",
      "to go 59027\n",
      "to go 58796\n",
      "to go 58554\n",
      "to go 58310\n",
      "to go 58047\n",
      "to go 57802\n",
      "to go 57550\n",
      "to go 57314\n",
      "to go 57055\n",
      "to go 56787\n",
      "to go 56540\n",
      "to go 56282\n",
      "to go 56022\n",
      "to go 55796\n",
      "to go 55545\n",
      "to go 55281\n",
      "to go 55029\n",
      "to go 54765\n",
      "to go 54519\n",
      "to go 54269\n",
      "to go 54023\n",
      "to go 53778\n",
      "to go 53537\n",
      "to go 53295\n",
      "to go 53012\n",
      "to go 52756\n",
      "to go 52498\n",
      "to go 52269\n",
      "to go 52001\n",
      "to go 51737\n",
      "to go 51495\n",
      "to go 51251\n",
      "to go 50990\n",
      "to go 50741\n",
      "to go 50476\n",
      "to go 50218\n",
      "to go 49969\n",
      "to go 49716\n",
      "to go 49460\n",
      "to go 49204\n",
      "to go 48960\n",
      "to go 48706\n",
      "to go 48447\n",
      "to go 48186\n",
      "to go 47949\n",
      "to go 47690\n",
      "to go 47446\n",
      "to go 47189\n",
      "to go 46957\n",
      "to go 46701\n",
      "to go 46463\n",
      "to go 46216\n",
      "to go 45975\n",
      "to go 45735\n",
      "to go 45482\n",
      "to go 45257\n",
      "to go 45030\n",
      "to go 44794\n",
      "to go 44559\n",
      "to go 44296\n",
      "to go 44043\n",
      "to go 43793\n",
      "to go 43533\n",
      "to go 43275\n",
      "to go 43006\n",
      "to go 42758\n",
      "to go 42511\n",
      "to go 42264\n",
      "to go 42001\n",
      "to go 41760\n",
      "to go 41506\n",
      "to go 41244\n",
      "to go 40985\n",
      "to go 40734\n",
      "to go 40490\n",
      "to go 40235\n",
      "to go 39991\n",
      "to go 39733\n",
      "to go 39474\n",
      "to go 39237\n",
      "to go 38982\n",
      "to go 38745\n",
      "to go 38510\n",
      "to go 38251\n",
      "to go 37983\n",
      "to go 37736\n",
      "to go 37497\n",
      "to go 37246\n",
      "to go 37003\n",
      "to go 36756\n",
      "to go 36509\n",
      "to go 36275\n",
      "to go 36043\n",
      "to go 35777\n",
      "to go 35531\n",
      "to go 35297\n",
      "to go 35043\n",
      "to go 34818\n",
      "to go 34558\n",
      "to go 34325\n",
      "to go 34061\n",
      "to go 33821\n",
      "to go 33567\n",
      "to go 33335\n",
      "to go 33097\n",
      "to go 32858\n",
      "to go 32593\n",
      "to go 32352\n",
      "to go 32106\n",
      "to go 31857\n",
      "to go 31597\n",
      "to go 31340\n",
      "to go 31053\n",
      "to go 30823\n",
      "to go 30575\n",
      "to go 30317\n",
      "to go 30043\n",
      "to go 29771\n",
      "to go 29519\n",
      "to go 29281\n",
      "to go 29045\n",
      "to go 28796\n",
      "to go 28571\n",
      "to go 28309\n",
      "to go 28048\n",
      "to go 27795\n",
      "to go 27552\n",
      "to go 27300\n",
      "to go 27064\n",
      "to go 26817\n",
      "to go 26553\n",
      "to go 26309\n",
      "to go 26055\n",
      "to go 25804\n",
      "to go 25551\n",
      "to go 25290\n",
      "to go 25051\n",
      "to go 24786\n",
      "to go 24550\n",
      "to go 24297\n",
      "to go 24046\n",
      "to go 23787\n",
      "to go 23556\n",
      "to go 23293\n",
      "to go 23041\n",
      "to go 22773\n",
      "to go 22522\n",
      "to go 22260\n",
      "to go 22010\n",
      "to go 21764\n",
      "to go 21512\n",
      "to go 21252\n",
      "to go 21011\n",
      "to go 20754\n",
      "to go 20532\n",
      "to go 20284\n",
      "to go 20033\n",
      "to go 19785\n",
      "to go 19566\n",
      "to go 19315\n",
      "to go 19066\n",
      "to go 18808\n",
      "to go 18542\n",
      "to go 18276\n",
      "to go 18038\n",
      "to go 17787\n",
      "to go 17541\n",
      "to go 17328\n",
      "to go 17073\n",
      "to go 16821\n",
      "to go 16590\n",
      "to go 16330\n",
      "to go 16070\n",
      "to go 15826\n",
      "to go 15571\n",
      "to go 15333\n",
      "to go 15093\n",
      "to go 14846\n",
      "to go 14590\n",
      "to go 14344\n",
      "to go 14103\n",
      "to go 13843\n",
      "to go 13621\n",
      "to go 13356\n",
      "to go 13102\n",
      "to go 12844\n",
      "to go 12573\n",
      "to go 12308\n",
      "to go 12060\n",
      "to go 11831\n",
      "to go 11582\n",
      "to go 11319\n",
      "to go 11076\n",
      "to go 10819\n",
      "to go 10552\n",
      "to go 10276\n",
      "to go 10033\n",
      "to go 9770\n",
      "to go 9513\n",
      "to go 9269\n",
      "to go 9024\n",
      "to go 8777\n",
      "to go 8534\n",
      "to go 8308\n",
      "to go 8046\n",
      "to go 7792\n",
      "to go 7549\n",
      "to go 7307\n",
      "to go 7065\n",
      "to go 6806\n",
      "to go 6581\n",
      "to go 6347\n",
      "to go 6095\n",
      "to go 5860\n",
      "to go 5600\n",
      "to go 5343\n",
      "to go 5092\n",
      "to go 4839\n",
      "to go 4592\n",
      "to go 4336\n",
      "to go 4085\n",
      "to go 3837\n",
      "to go 3583\n",
      "to go 3342\n",
      "to go 3080\n",
      "to go 2844\n",
      "to go 2593\n",
      "to go 2361\n",
      "to go 2097\n",
      "to go 1843\n",
      "to go 1586\n",
      "to go 1352\n",
      "to go 1112\n",
      "to go 838\n",
      "to go 588\n",
      "to go 336\n",
      "to go 70\n",
      "Dumped baseline_features and baseline_labels\n",
      "Total features loaded: 34425\n"
     ]
    }
   ],
   "source": [
    "BASELINE = True\n",
    "\n",
    "baseline_features = []\n",
    "baseline_labels = []\n",
    "\n",
    "if not file_exists(BASELINE_FEATURES_FILENAME) and not file_exists(BASELINE_LABELS_FILENAME):\n",
    "  baseline_features, baseline_labels = prepare_dataset(masc_dataset)\n",
    "\n",
    "  with open(BASELINE_FEATURES_FILENAME, 'w') as outfile:\n",
    "    json.dump(baseline_features, outfile)\n",
    "    \n",
    "  with open(BASELINE_LABELS_FILENAME, 'w') as outfile:\n",
    "      json.dump(baseline_labels, outfile)\n",
    "\n",
    "  print ('Dumped baseline_features and baseline_labels')\n",
    "else:\n",
    "  print ('Using existing files:')\n",
    "\n",
    "  with open(BASELINE_FEATURES_FILENAME) as json_file:\n",
    "      for sentence in json.load(json_file):\n",
    "          for token in sentence:\n",
    "              baseline_features.append(token)\n",
    "\n",
    "\n",
    "  with open(BASELINE_LABELS_FILENAME) as json_file:\n",
    "      for sentence in json.load(json_file):\n",
    "          for label in sentence:\n",
    "              baseline_labels.append(label)\n",
    "\n",
    "print ('Total features loaded:', len(baseline_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: LogisticRegression on a small set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "mZTkyLi0pZHH",
    "outputId": "31f05a02-e02e-4f46-caea-1e823a287890",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size 1973926\n",
      "Evalset size 845968\n",
      "Total number of features:  152092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 74 epochs took 142 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', random_state=42, solver='sag',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "features_data = []\n",
    "labels_data = []\n",
    "\n",
    "\n",
    "for sentence in baseline_features:\n",
    "    features_data.extend(sentence)\n",
    "\n",
    "for sentence in baseline_labels:\n",
    "    labels_data.extend(sentence)\n",
    "\n",
    "total_size = len(features_data)\n",
    "trainset_size = round(total_size * 0.7)\n",
    "testset_size = total_size - trainset_size\n",
    "\n",
    "train_features = features_data[:trainset_size]\n",
    "test_features = features_data[trainset_size:]\n",
    "\n",
    "print ('Trainset size', len(train_features))\n",
    "print('Evalset size', len(test_features))\n",
    "\n",
    "train_labels = labels_data[:trainset_size]\n",
    "test_labels = labels_data[trainset_size:]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"Total number of features: \", len(vec.get_feature_names()))\n",
    "\n",
    "train_features_vectorized = vec.transform(train_features)\n",
    "test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "lrc = LogisticRegression(random_state=42, solver=\"sag\", multi_class=\"multinomial\",\n",
    "                         max_iter=100, verbose=1, n_jobs = -1)\n",
    "lrc.fit(train_features_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "4W_SBKNrqLZ_",
    "outputId": "10b0d912-338a-421d-852b-0ac559c583f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99    830306\n",
      "        True       0.56      0.01      0.02     15662\n",
      "\n",
      "    accuracy                           0.98    845968\n",
      "   macro avg       0.77      0.51      0.51    845968\n",
      "weighted avg       0.97      0.98      0.97    845968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = lrc.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "EM9plFDBqnan",
    "outputId": "2ca867c1-6998-4130-c61f-74d39a8535fb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 75 epochs took 86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 76 epochs took 85 seconds\n",
      "Cross validation: [0.98208103 0.98203843 0.98209105 0.98203593 0.98207602]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lrc, train_features_vectorized, train_labels, cv=5)\n",
    "print ('Cross validation:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dZ-mh-uzuJxg",
    "outputId": "e50e64e8-8a2e-4dfb-f68c-f9dd8451b5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files:\n",
      "Total features loaded: 2819882\n"
     ]
    }
   ],
   "source": [
    "BASELINE = False\n",
    "\n",
    "improved_features = []\n",
    "improved_labels = []\n",
    "\n",
    "if not file_exists(IMPROVED_FEATURES_FILENAME) and not file_exists(IMPROVED_LABELS_FILENAME):\n",
    "  improved_features, improved_labels = prepare_dataset(masc_dataset)\n",
    "  with open(IMPROVED_FEATURES_FILENAME, 'w') as outfile:\n",
    "      json.dump(improved_features, outfile)\n",
    "      \n",
    "  with open(IMPROVED_LABELS_FILENAME, 'w') as outfile:\n",
    "      json.dump(improved_labels, outfile)\n",
    "\n",
    "  print ('Dumped improved_features and improved_labels')\n",
    "  print ('Total features calculated:', len(improved_features))\n",
    "else:\n",
    "  print ('Using existing files:')\n",
    "\n",
    "  with open(IMPROVED_FEATURES_FILENAME) as json_file:\n",
    "      for sentence in json.load(json_file):\n",
    "          for token in sentence:\n",
    "              improved_features.append(token)\n",
    "\n",
    "\n",
    "  with open(IMPROVED_LABELS_FILENAME) as json_file:\n",
    "      for sentence in json.load(json_file):\n",
    "          for label in sentence:\n",
    "              improved_labels.append(label)\n",
    "\n",
    "  print ('Total features loaded:', len(improved_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression on an extended set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uwy1v8rqLWbm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size 1973917\n",
      "Evalset size 845965\n",
      "Total number of features:  368779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 254 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/06-language-as-sequence/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  4.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation: [0.98635203 0.98615445 0.98656477 0.98665089 0.98639759]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      1.00      0.99    830380\n",
      "        True       0.79      0.37      0.50     15585\n",
      "\n",
      "    accuracy                           0.99    845965\n",
      "   macro avg       0.89      0.68      0.75    845965\n",
      "weighted avg       0.98      0.99      0.98    845965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "features_data = improved_features\n",
    "labels_data = improved_labels\n",
    "\n",
    "\n",
    "total_size = len(features_data)\n",
    "trainset_size = round(total_size * 0.7)\n",
    "testset_size = total_size - trainset_size\n",
    "\n",
    "train_features = features_data[:trainset_size]\n",
    "test_features = features_data[trainset_size:]\n",
    "\n",
    "print ('Trainset size', len(train_features))\n",
    "print('Evalset size', len(test_features))\n",
    "\n",
    "train_labels = labels_data[:trainset_size]\n",
    "test_labels = labels_data[trainset_size:]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"Total number of features: \", len(vec.get_feature_names()))\n",
    "\n",
    "train_features_vectorized = vec.transform(train_features)\n",
    "test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "lrc = LogisticRegression(random_state=42, solver=\"sag\", multi_class=\"multinomial\",\n",
    "                         max_iter=100, verbose=1, n_jobs = -1)\n",
    "lrc.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "scores = cross_val_score(lrc, train_features_vectorized, train_labels, cv=5)\n",
    "print ('Cross validation:', scores)\n",
    "\n",
    "predicted_labels = lrc.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression trained on synthetic data and validated on run_on_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bP6f7SmjLwN_",
    "outputId": "3e40120c-6826-4cb1-bcf9-2d15cdfb717e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 17\n",
      "labels: 17\n",
      "retokenized: 18\n",
      "to go 190\n",
      "tokens: 16\n",
      "labels: 16\n",
      "retokenized: 20\n",
      "tokens: 36\n",
      "labels: 36\n",
      "retokenized: 38\n",
      "tokens: 33\n",
      "labels: 33\n",
      "retokenized: 35\n",
      "tokens: 25\n",
      "labels: 25\n",
      "retokenized: 27\n",
      "tokens: 17\n",
      "labels: 17\n",
      "retokenized: 18\n",
      "to go 180\n",
      "tokens: 12\n",
      "labels: 12\n",
      "retokenized: 14\n",
      "to go 170\n",
      "tokens: 29\n",
      "labels: 29\n",
      "retokenized: 31\n",
      "to go 160\n",
      "to go 150\n",
      "tokens: 20\n",
      "labels: 20\n",
      "retokenized: 21\n",
      "to go 140\n",
      "to go 130\n",
      "to go 120\n",
      "tokens: 33\n",
      "labels: 33\n",
      "retokenized: 37\n",
      "to go 110\n",
      "tokens: 17\n",
      "labels: 17\n",
      "retokenized: 19\n",
      "to go 100\n",
      "to go 90\n",
      "tokens: 17\n",
      "labels: 17\n",
      "retokenized: 19\n",
      "tokens: 56\n",
      "labels: 56\n",
      "retokenized: 58\n",
      "to go 80\n",
      "tokens: 18\n",
      "labels: 18\n",
      "retokenized: 20\n",
      "tokens: 36\n",
      "labels: 36\n",
      "retokenized: 38\n",
      "to go 70\n",
      "tokens: 34\n",
      "labels: 34\n",
      "retokenized: 36\n",
      "to go 60\n",
      "to go 50\n",
      "tokens: 20\n",
      "labels: 20\n",
      "retokenized: 21\n",
      "tokens: 23\n",
      "labels: 23\n",
      "retokenized: 24\n",
      "to go 40\n",
      "to go 30\n",
      "tokens: 20\n",
      "labels: 20\n",
      "retokenized: 22\n",
      "to go 20\n",
      "tokens: 48\n",
      "labels: 48\n",
      "retokenized: 49\n",
      "to go 10\n",
      "to go 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def prepare_run_on_test():\n",
    "    with open(RUN_ON_TEST_RAW_FILENAME) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    counter = 0\n",
    "    for sentence in data:\n",
    "\n",
    "        temp_tokens = []\n",
    "        temp_labels = []\n",
    "        for token in sentence:\n",
    "            temp_tokens.append(token[0])\n",
    "            temp_labels.append(token[1])\n",
    "\n",
    "        dataset = extract_features([' '.join(temp_tokens)])\n",
    "\n",
    "        if len(temp_tokens) != len(dataset[0]) or len(dataset[0])!=len(temp_labels):\n",
    "            print('tokens:',len(temp_tokens))\n",
    "            print('labels:',len(temp_labels))\n",
    "            print('retokenized:',len(dataset[0]))\n",
    "\n",
    "        if len(temp_tokens) == len(dataset[0]) and len(dataset[0])==len(temp_labels):\n",
    "            tokens.append(dataset[0])\n",
    "            labels.append(temp_labels)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            print('to go', len(data)-counter)\n",
    "\n",
    "    with open(RUN_ON_TEST_TOKENS_FILENAME, 'w') as outfile:\n",
    "        json.dump(tokens, outfile)\n",
    "    with open(RUN_ON_TEST_LABELS_FILENAME, 'w') as outfile:\n",
    "        json.dump(labels, outfile)\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "BASELINE = False\n",
    "run_on_features, run_on_labels = prepare_run_on_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrAoZKYzuEll"
   },
   "outputs": [],
   "source": [
    "improved_features = []\n",
    "improved_labels = []\n",
    "\n",
    "with open(IMPROVED_FEATURES_FILENAME) as json_file:\n",
    "    for sentence in json.load(json_file):\n",
    "        for token in sentence:\n",
    "            improved_features.append(token)\n",
    "\n",
    "\n",
    "with open(IMPROVED_LABELS_FILENAME) as json_file:\n",
    "    for sentence in json.load(json_file):\n",
    "        for label in sentence:\n",
    "            improved_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "l1B-nM3oTb2H",
    "outputId": "f503506d-73bc-4e58-cfc0-7e1dca928fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  441235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 406 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytromindra/Projects/prj-nlp-2020/students/DmytroMindra/06-language-as-sequence/venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      4027\n",
      "        True       0.86      0.48      0.61       143\n",
      "\n",
      "    accuracy                           0.98      4170\n",
      "   macro avg       0.92      0.74      0.80      4170\n",
      "weighted avg       0.98      0.98      0.98      4170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "features_data = improved_features\n",
    "labels_data = improved_labels\n",
    "\n",
    "run_on_features_data = []\n",
    "run_on_labels_data = []\n",
    "\n",
    "for sentence in run_on_features:\n",
    "    for token in sentence:\n",
    "        run_on_features_data.append(token)\n",
    "\n",
    "for sentence in run_on_labels:\n",
    "    for label in sentence:\n",
    "        run_on_labels_data.append(label)\n",
    "\n",
    "train_features = features_data\n",
    "test_features = run_on_features_data\n",
    "\n",
    "train_labels = labels_data\n",
    "test_labels = run_on_labels_data\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"Total number of features: \", len(vec.get_feature_names()))\n",
    "\n",
    "train_features_vectorized = vec.transform(train_features)\n",
    "test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "lrc = LogisticRegression(random_state=42, solver=\"sag\", multi_class=\"multinomial\",\n",
    "                         max_iter=100, verbose=1, n_jobs =-1)\n",
    "\n",
    "lrc.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "predicted_labels = lrc.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF and RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size 1973917\n",
      "Evalset size 845965\n",
      "Total number of features:  368779\n",
      "Cross validation: [0.98201548 0.98192683 0.98172667 0.98220795 0.98138724]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    830380\n",
      "           1       0.00      0.00      0.00     15585\n",
      "\n",
      "    accuracy                           0.98    845965\n",
      "   macro avg       0.49      0.50      0.50    845965\n",
      "weighted avg       0.96      0.98      0.97    845965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn_crfsuite\n",
    "\n",
    "features_data = improved_features\n",
    "labels_data = improved_labels\n",
    "\n",
    "\n",
    "total_size = len(features_data)\n",
    "trainset_size = round(total_size * 0.7)\n",
    "testset_size = total_size - trainset_size\n",
    "\n",
    "train_features = features_data[:trainset_size]\n",
    "test_features = features_data[trainset_size:]\n",
    "\n",
    "print ('Trainset size', len(train_features))\n",
    "print('Evalset size', len(test_features))\n",
    "\n",
    "train_labels = labels_data[:trainset_size]\n",
    "test_labels = labels_data[trainset_size:]\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"Total number of features: \", len(vec.get_feature_names()))\n",
    "\n",
    "train_features_vectorized = vec.transform(train_features)\n",
    "test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "train_labels_vectorized = []\n",
    "for label in train_labels:\n",
    "  if label:\n",
    "    train_labels_vectorized.append([\"1\"])\n",
    "  else:\n",
    "    train_labels_vectorized.append([\"0\"])\n",
    "\n",
    "crf.fit(train_features_vectorized, train_labels_vectorized)\n",
    "\n",
    "test_labels_vectorized = []\n",
    "for label in test_labels:\n",
    "  if label:\n",
    "    test_labels_vectorized.append([\"1\"])\n",
    "  else:\n",
    "    test_labels_vectorized.append([\"0\"])\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(train_features_vectorized, train_labels_vectorized)\n",
    "\n",
    "scores = cross_val_score(crf, train_features_vectorized, train_labels_vectorized, cv=5)\n",
    "print ('Cross validation:', scores)\n",
    "\n",
    "predicted_labels = crf.predict(test_features_vectorized)\n",
    "print(classification_report(test_labels_vectorized, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "siBHASrlV-w5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "colab_type": "code",
    "id": "MN-6uArTV95J",
    "outputId": "7c3a4d2e-4dd2-48da-d839-867986bb9f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in ./venv/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.8/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.18.4)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./venv/lib/python3.8/site-packages (0.3.6)\n",
      "Requirement already satisfied: tqdm>=2.0 in ./venv/lib/python3.8/site-packages (from sklearn-crfsuite) (4.46.0)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from sklearn-crfsuite) (1.14.0)\n",
      "Requirement already satisfied: tabulate in ./venv/lib/python3.8/site-packages (from sklearn-crfsuite) (0.8.7)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in ./venv/lib/python3.8/site-packages (from sklearn-crfsuite) (0.9.7)\n",
      "4170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      4027\n",
      "           1       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.97      4170\n",
      "   macro avg       0.48      0.50      0.49      4170\n",
      "weighted avg       0.93      0.97      0.95      4170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install sklearn-crfsuite\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import scipy.stats\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=3,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=5,\n",
    "                        scoring = scorer)\n",
    "\n",
    "#train_features_vectorized = vec.transform(train_features)\n",
    "#test_features_vectorized = vec.transform(test_features)\n",
    "\n",
    "train_labels_vectorized = []\n",
    "for label in train_labels:\n",
    "  if label:\n",
    "    train_labels_vectorized.append([\"1\"])\n",
    "  else:\n",
    "    train_labels_vectorized.append([\"0\"])\n",
    "\n",
    "crf.fit(train_features_vectorized, train_labels_vectorized)\n",
    "\n",
    "test_labels_vectorized = []\n",
    "for label in test_labels:\n",
    "  if label:\n",
    "    test_labels_vectorized.append([\"1\"])\n",
    "  else:\n",
    "    test_labels_vectorized.append([\"0\"])\n",
    "print (len(test_labels_vectorized))\n",
    "\n",
    "\n",
    "\n",
    "y_pred = crf.predict(test_features_vectorized)\n",
    "\n",
    "#print('best params:', rs.best_params_)\n",
    "#print('best CV score:', rs.best_score_)\n",
    "#print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "print(classification_report(test_labels_vectorized, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework-6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}