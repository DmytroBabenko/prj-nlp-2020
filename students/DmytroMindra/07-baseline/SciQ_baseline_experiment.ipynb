{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline #\n",
    "\n",
    "Первое решение я решил строить на основе датасета \"SciQ Dataset\"\n",
    "https://allenai.org/data/sciq"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загружаем данные:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "test_data: 1000\n",
      "train_data: 11679\n",
      "validation_data: 1000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "PATH_TO_SCIQ_DATASET = 'data/SciQ dataset/'\n",
    "TEST_FILENAME = 'test.json'\n",
    "TRAIN_FILENAME = 'train.json'\n",
    "VALIDATION_FILENAME = 'valid.json'\n",
    "\n",
    "with open(PATH_TO_SCIQ_DATASET+TEST_FILENAME,'r') as json_file:\n",
    "    test_data =  json.load(json_file)\n",
    "\n",
    "with open(PATH_TO_SCIQ_DATASET+TRAIN_FILENAME,'r') as json_file:\n",
    "    train_data =  json.load(json_file)\n",
    "\n",
    "with open(PATH_TO_SCIQ_DATASET+VALIDATION_FILENAME,'r') as json_file:\n",
    "    validation_data =  json.load(json_file)\n",
    "\n",
    "    \n",
    "print('test_data:',len(test_data))\n",
    "print('train_data:',len(train_data))\n",
    "print('validation_data:',len(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```json5\n",
    "{\n",
    "    \"question\": \"Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\",\n",
    "    \"distractor3\": \"residues\",\n",
    "    \"distractor1\": \"antioxidants\",\n",
    "    \"distractor2\": \"Oxygen\",\n",
    "    \"correct_answer\": \"oxidants\",\n",
    "    \"support\": \"Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F2, are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.\"\n",
    "}\n",
    "```\n",
    "Я преобразую данные в два вектора: в первом будут варианты ответов (distractor1, distractor2, distractor3, correct_answer) и фичи, которые я извлеку из support. Например, встречается ли каждый из этих вариантов ответа в тексте?\n",
    "Вторым вектором будет вектор метор, где `True` означает правильный ответ, а `False` - дистрактор: [False False False True]\n",
    "Также перед добавлением вариантов ответа в общий вектор, я буду их перемешивать."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "([{'present_in_support': False},\n  {'present_in_support': False},\n  {'present_in_support': True},\n  {'present_in_support': True}],\n [False, False, True, False])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "\n",
    "def get_answer_features(answer, support):\n",
    "    features = {}\n",
    "    if support.lower().find(answer.lower())>-1:\n",
    "        features['present_in_support'] = True\n",
    "    else:\n",
    "        features['present_in_support'] = False\n",
    "    return features\n",
    "\n",
    "def get_question_features(question_data):\n",
    "    \n",
    "    question = question_data[\"question\"]\n",
    "    distractors = [question_data[\"distractor1\"], question_data[\"distractor2\"],question_data[\"distractor3\"]]\n",
    "    correct_answer = question_data[\"correct_answer\"]\n",
    "    support = question_data[\"support\"]\n",
    "    answers = distractors + [correct_answer]\n",
    "    random.shuffle(answers)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for answer in answers:\n",
    "        features.append(get_answer_features(answer, support))\n",
    "        if answer == correct_answer:\n",
    "            labels.append(True)\n",
    "        else:\n",
    "            labels.append(False)\n",
    "        \n",
    "    return (features,labels)\n",
    "\n",
    "get_question_features(test_data[105])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "convergence after 17 epochs took 0 seconds\n",
      "convergence after 17 epochs took 0 seconds\n",
      "convergence after 19 epochs took 1 seconds\n",
      "convergence after 15 epochs took 0 seconds\n",
      "convergence after 16 epochs took 0 seconds\n",
      "convergence after 17 epochs took 0 seconds\n",
      "Cross validation:\n",
      "[0.92583476 0.92550573 0.92614792 0.92122445 0.92518463]\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.95      0.95      2998\n",
      "        True       0.86      0.82      0.84      1002\n",
      "\n",
      "    accuracy                           0.92      4000\n",
      "   macro avg       0.90      0.89      0.89      4000\n",
      "weighted avg       0.92      0.92      0.92      4000\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_data_features = []\n",
    "train_data_labels = []\n",
    "test_data_features = []\n",
    "test_data_labels = []\n",
    "\n",
    "for question in train_data:\n",
    "    local_data = get_question_features(question)\n",
    "    train_data_features.extend(local_data[0])\n",
    "    train_data_labels.extend(local_data[1])\n",
    "\n",
    "for question in test_data:\n",
    "    local_data = get_question_features(question)\n",
    "    test_data_features.extend(local_data[0])\n",
    "    test_data_labels.extend(local_data[1])\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_data_features)\n",
    "#print(\"Total number of features: \", len(vec.get_feature_names()))\n",
    "\n",
    "train_features_vectorized = vec.transform(train_data_features)\n",
    "test_features_vectorized = vec.transform(test_data_features)\n",
    "\n",
    "lrc = LogisticRegression(random_state=42, solver=\"sag\", multi_class=\"multinomial\",\n",
    "                             max_iter=50, verbose=1)\n",
    "\n",
    "lrc.fit(train_features_vectorized, train_data_labels)\n",
    "\n",
    "scores = cross_val_score(lrc, train_features_vectorized, train_data_labels, cv=5)\n",
    "\n",
    "print ('Cross validation:')\n",
    "print (scores)\n",
    "\n",
    "print ('classification_report:')\n",
    "predicted_labels = lrc.predict(test_features_vectorized)\n",
    "print(classification_report(test_data_labels, predicted_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вывод ##\n",
    "В датасете SciQ в большинстве \"support\" предложений дистракторы отсутвуют, а правильный вариант ответа записан абсолютно так же, как он указан в вопросах.\n",
    "Поэтому простой фичи \"есть ли вариант ответа в \"support\" предложении\" достаочно для получения довольно высокой точности.\n",
    "\n",
    "## Следующие шаги ##\n",
    "\n",
    "1. Написать парсер, который найдет тексты, из которых взяты \"support\" предложения и дополнить ими датасет.\n",
    "2. Натренировать модель находить support sentence для вопросов \"What is/are ...\"  "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}