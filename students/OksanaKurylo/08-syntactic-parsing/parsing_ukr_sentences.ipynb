{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "import stanza\n",
    "import pymorphy2\n",
    "import tokenize_uk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-12 17:37:13 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| mwt       | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2020-05-12 17:37:13 INFO: Use device: cpu\n",
      "2020-05-12 17:37:13 INFO: Loading: tokenize\n",
      "2020-05-12 17:37:13 INFO: Loading: mwt\n",
      "2020-05-12 17:37:13 INFO: Loading: pos\n",
      "2020-05-12 17:37:14 INFO: Loading: lemma\n",
      "2020-05-12 17:37:14 INFO: Loading: depparse\n",
      "2020-05-12 17:37:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = stanza.Pipeline(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.path.abspath('__file__') \n",
    "path_list = script_path.split(os.sep)\n",
    "script_directory = path_list[0:len(path_list)-5]\n",
    "rel_path = \"UD_Ukrainian-IU\"\n",
    "PATH = \"/\".join(script_directory[:4]) + \"/\" + rel_path\n",
    " \n",
    "with open(PATH + \"/uk_iu-ud-train.conllu\", \"r\") as f:\n",
    "    train_trees = parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Цінова лихоманка у царині енергоресурсів засвідчує, що світова економіка переживає не найкращі часи.\", \n",
    "          \"Проте усвідомлення цього факту не завадило верховному представнику ЄС у закордонних справах та політиці безпеки Жозепу Боррелю під час спілкування з Дмитром Кулебою наголосити на зацікавленості Європейського Союзу у співпраці з Україною та готовності здійснити відкладений візит до України тільки-но для цього з’явиться можливість.\",\n",
    "          \"Нагадаю, що пан Боррель мав намір відвідати Донбас, що протягом останніх років стало ледь не обов’язковою частиною для перебування в Україні поважних іноземних гостей.\",\n",
    "          \"Міністри закордонних справ країн ЄС також продемонстрували готовність до розвитку співпраці з Україною, а Європейська Комісія оголосила про готовність виділити для України 1,2 мільярди для боротьби з наслідками коронавірусу.\",  \n",
    "          \"Російська Федерація також демонструє активність щодо України, проте дещо іншого порядку.\", \n",
    "          \"У Москві свідомо ігнорують очевидне: словосполучення «російський гуманітарний конвой» з 2014 року стало синонімом незаконного втручання та підтримки бойовиків.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be taken for a golden parse\n",
    "def golden_tree(sentence):\n",
    "    transformed_tree = []\n",
    "    doc = stanza_nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            transformed_tree.append(OrderedDict([('id', int(word.id)), ('form',  word.text), ('lemma',word.lemma),\n",
    "                                     ('upostag', word.upos), ('xpostag', None),\n",
    "                                     ('feats', word.feats), ('head', int(word.head)), ('deprel', None),\n",
    "                                     ('deps', None), ('misc', None)]))\n",
    "    return transformed_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version I. \n",
    "- golden - залежності, які проставив парсер stanz-и\n",
    "- Порівнюю роботу свого парсера на реченнях, які привела до необхідного вигляду з допомогою pymorphy2\n",
    "- Граматичні ознаки зліплюю (бо набір з conllu - 'feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Nom'), ('Gender', 'Neut'), ('Number', 'Sing')]), а в pymorhpy - 'feats', OrderedDict([('NOUN,', 'inan femn,nomn')]), просто кодувати по одній не допоможе, їх потрібно привести до одного вигляду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "\n",
    "DET = ['будь-який', 'ваш', 'ввесь', 'весь', 'все', 'всенький', 'всякий',\n",
    "       'всілякий', 'деякий', 'другий', 'жадний', 'жодний', 'ин.', 'ін.',\n",
    "       'інакший', 'інш.', 'інший', 'їх', 'їхній', 'її', 'його', 'кожний',\n",
    "       'кожній', 'котрий', 'котрийсь', 'кілька', 'мій', 'наш', 'небагато',\n",
    "       'ніякий', 'отакий', 'отой', 'оцей', 'сам', 'самий', 'свій', 'сей',\n",
    "       'скільки', 'такий', 'тамтой', 'твій', 'те', 'той', 'увесь', 'усякий',\n",
    "       'усілякий', 'це', 'цей', 'чий', 'чийсь', 'який', 'якийсь']\n",
    "\n",
    "PREP = [\"до\", \"на\"]\n",
    "\n",
    "mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "           \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "           \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "def normalize_pos(word):\n",
    "    if word.tag.POS == \"CONJ\":\n",
    "        if \"coord\" in word.tag:\n",
    "            return \"CCONJ\"\n",
    "        else:\n",
    "            return \"SCONJ\"\n",
    "    elif \"PNCT\" in word.tag:\n",
    "        return \"PUNCT\"\n",
    "    elif word.normal_form in PREP:\n",
    "        return \"PREP\"\n",
    "    elif word.normal_form in DET:\n",
    "        return \"DET\"\n",
    "    else:\n",
    "        return mapping.get(word.tag.POS, word.tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test sentences\n",
    "def convert_sentence_with_pymorphy_feats(sentence):\n",
    "    tokenized_sent = tokenize_uk.tokenize_words(sentence)\n",
    "    morphs = [morph.parse(word) for word in tokenized_sent]\n",
    "    converted_tree = []\n",
    "    \n",
    "    for idx, mrph in enumerate(morphs):\n",
    "        w = mrph[0]\n",
    "        tagset = str(w.tag)\n",
    "        pos = tagset[:5]\n",
    "        # an empty feature set: tag=OpencorporaTag('PRCL')\n",
    "        if len(tagset) <= 4:\n",
    "            feats = str(False)\n",
    "        else:\n",
    "            feats = tagset[5:]\n",
    "        features = OrderedDict([(pos, feats)])\n",
    "        converted_tree.append(OrderedDict([('id', idx + 1), ('form', w.word), ('lemma', w.normal_form),\n",
    "                                         ('upostag', normalize_pos(w)), ('xpostag', None),\n",
    "                                         ('feats', features), ('head', None), ('deprel', None),\n",
    "                                         ('deps', None), ('misc', None)]))\n",
    "        \n",
    "    return converted_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(str, Enum):\n",
    "    SHIFT = \"shift\"\n",
    "    REDUCE = \"reduce\"\n",
    "    RIGHT = \"right\"\n",
    "    LEFT = \"left\"\n",
    "    \n",
    "def oracle(stack, top_queue, relations):\n",
    "    \"\"\"\n",
    "    Make a decision on the right action to do.\n",
    "    \"\"\"\n",
    "    top_stack = stack[-1]\n",
    "    # check if both stack and queue are non-empty\n",
    "    if top_stack and not top_queue:\n",
    "        return Actions.REDUCE\n",
    "    # check if there are any clear dependencies\n",
    "    elif top_queue[\"head\"] == top_stack[\"id\"]:\n",
    "        return Actions.RIGHT\n",
    "    elif top_stack[\"head\"] == top_queue[\"id\"]:\n",
    "        return Actions.LEFT\n",
    "    # check if we can reduce the top of the stack\n",
    "    elif top_stack[\"id\"] in [i[0] for i in relations] and \\\n",
    "         (top_queue[\"head\"] < top_stack[\"id\"] or \\\n",
    "          [s for s in stack if s[\"head\"] == top_queue[\"id\"]]):\n",
    "        return Actions.REDUCE\n",
    "    # default option\n",
    "    else:\n",
    "        return Actions.SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(stack, queue):\n",
    "    features = dict()\n",
    "\n",
    "    if len(stack) > 0:\n",
    "        stack_top = stack[-1]\n",
    "        features[\"s0-word\"] = stack_top[\"form\"]\n",
    "        features[\"s0-lemma\"] = stack_top[\"lemma\"]\n",
    "        features[\"s0-tag\"] = stack_top[\"upostag\"]\n",
    "         # фічі для слова в стеці\n",
    "        if stack_top[\"feats\"] != None:\n",
    "            features[\"s0-feats\"] = \"_\".join([f for f in stack_top[\"feats\"].values()])\n",
    "        else:\n",
    "            features[\"s0-feats\"] = False\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-tag\"] = stack[-2][\"upostag\"]\n",
    "        if stack[-2][\"feats\"] != None:\n",
    "            features[\"s1-feats\"] = \"_\".join([f for f in stack[-2][\"feats\"].values()])\n",
    "        else:\n",
    "            features[\"s1-feats\"] = False\n",
    "    if queue:\n",
    "        queue_top = queue[0]\n",
    "        features[\"q0-word\"] = queue_top[\"form\"]\n",
    "        features[\"q0-lemma\"] = queue_top[\"lemma\"]\n",
    "        features[\"q0-tag\"] = queue_top[\"upostag\"]\n",
    "        # фічі для 1-го слова в черзі\n",
    "        if queue_top[\"feats\"] != None:\n",
    "            features[\"q0-feats\"] = \"_\".join([f for f in queue_top[\"feats\"].values()])\n",
    "        else:\n",
    "            features[\"q0-feats\"] = False\n",
    "    if len(queue) > 1:\n",
    "        queue_next = queue[1]\n",
    "        features[\"q1-word\"] = queue_next[\"form\"]\n",
    "        features[\"q1-tag\"] = queue_next[\"upostag\"]\n",
    "        # фічі для 2-го слова в черзі\n",
    "        if queue_next[\"feats\"] != None:\n",
    "            features[\"q1-feats\"] = \"_\".join([f for f in queue_next[\"feats\"].values()])\n",
    "        else:\n",
    "            features[\"q1-feats\"] = False\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tree):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    \n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                       queue[0] if len(queue) > 0 else None,\n",
    "                       relations)\n",
    "        features.append(extract_features(stack, queue))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190298 190298\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = [], []\n",
    "for tree in train_trees:\n",
    "    tree_features, tree_labels = get_data([t for t in tree if type(t[\"id\"])==int])\n",
    "    train_features += tree_features\n",
    "    train_labels += tree_labels\n",
    "\n",
    "print(len(train_features), len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features: 115303\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"\\nTotal number of features:\", len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190298\n"
     ]
    }
   ],
   "source": [
    "train_features_vectorized = vec.transform(train_features)\n",
    "print(len(train_features_vectorized.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 644 epochs took 188 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='saga', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = LogisticRegression(random_state=42, solver=\"saga\", multi_class=\"multinomial\", max_iter=1000, verbose=1)\n",
    "lrc.fit(train_features_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parse(sent, oracle, vectorizer):\n",
    "    stack, queue, relations = [ROOT], sent[:], []\n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = extract_features(stack, queue)\n",
    "            action = oracle.predict(vectorizer.transform([features]))[0]\n",
    "            if action == Actions.SHIFT:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == Actions.REDUCE:\n",
    "                stack.pop()\n",
    "            elif action == Actions.LEFT:\n",
    "                relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "                stack.pop()\n",
    "            elif action == Actions.RIGHT:\n",
    "                relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "                stack.append(queue.pop(0))\n",
    "            else:\n",
    "                print(\"Unknown action.\")\n",
    "    return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['цінова_1', 'лихоманка_2', 'у_3', 'царині_4', 'енергоресурсів_5', 'засвідчує_6', ',_7', 'що_8', 'світова_9', 'економіка_10', 'переживає_11', 'не_12', 'найкращі_13', 'часи_14', '._15']\n",
      "Golden: [(1, 2), (2, 6), (3, 4), (4, 2), (5, 4), (6, 0), (7, 11), (8, 11), (9, 10), (10, 11), (11, 6), (12, 13), (13, 14), (14, 11), (15, 6)]\n",
      "Predicted: [(1, 2), (2, 0), (2, 6), (3, 4), (4, 2), (5, 4), (6, 0), (7, 11), (8, 11), (9, 10), (10, 11), (11, 6), (12, 14), (13, 14), (14, 11), (15, 6)]\n",
      "=================================================================\n",
      "Sentence: ['проте_1', 'усвідомлення_2', 'цього_3', 'факту_4', 'не_5', 'завадило_6', 'верховному_7', 'представнику_8', 'єс_9', 'у_10', 'закордонних_11', 'справах_12', 'та_13', 'політиці_14', 'безпеки_15', 'жозепу_16', 'боррелю_17', 'під_18', 'час_19', 'спілкування_20', 'з_21', 'дмитром_22', 'кулебою_23', 'наголосити_24', 'на_25', 'зацікавленості_26', 'європейського_27', 'союзу_28', 'у_29', 'співпраці_30', 'з_31', 'україною_32', 'та_33', 'готовності_34', 'здійснити_35', 'відкладений_36', 'візит_37', 'до_38', 'україни_39', 'тільки_40', '-_41', 'но_42', 'для_43', 'цього_44', 'з’явиться_45', 'можливість_46', '._47']\n",
      "Golden: [(1, 6), (2, 6), (3, 4), (4, 2), (5, 6), (6, 0), (7, 8), (8, 6), (9, 8), (10, 12), (11, 12), (12, 8), (13, 14), (14, 12), (15, 14), (16, 15), (17, 16), (18, 19), (19, 24), (20, 19), (21, 22), (22, 20), (23, 22), (24, 6), (25, 26), (26, 24), (27, 28), (28, 26), (29, 30), (30, 26), (31, 32), (32, 30), (33, 34), (34, 32), (35, 34), (36, 37), (37, 35), (38, 39), (39, 37), (40, 45), (41, 45), (42, 45), (43, 44), (44, 45), (45, 6), (46, 45), (47, 45)]\n",
      "Predicted: [(1, 6), (2, 6), (3, 4), (4, 2), (5, 6), (6, 0), (7, 9), (8, 7), (9, 6), (10, 12), (11, 12), (12, 9), (13, 14), (14, 12), (15, 14), (16, 14), (17, 6), (18, 19), (19, 6), (20, 6), (21, 22), (22, 20), (23, 6), (24, 6), (25, 35), (27, 26), (27, 28), (28, 26), (29, 30), (30, 28), (31, 32), (32, 28), (33, 34), (34, 26), (35, 6), (36, 35), (37, 6), (40, 39), (40, 42), (41, 42), (42, 39), (43, 45), (44, 45), (45, 39), (46, 45), (47, 6)]\n",
      "=================================================================\n",
      "Sentence: ['нагадаю_1', ',_2', 'що_3', 'пан_4', 'боррель_5', 'мав_6', 'намір_7', 'відвідати_8', 'донбас_9', ',_10', 'що_11', 'протягом_12', 'останніх_13', 'років_14', 'стало_15', 'ледь_16', 'не_17', 'обов’язковою_18', 'частиною_19', 'для_20', 'перебування_21', 'в_22', 'україні_23', 'поважних_24', 'іноземних_25', 'гостей_26', '._27']\n",
      "Golden: [(1, 0), (2, 6), (3, 6), (4, 6), (5, 4), (6, 1), (7, 6), (8, 7), (9, 8), (10, 15), (11, 15), (12, 14), (13, 14), (14, 15), (15, 9), (16, 18), (17, 18), (18, 19), (19, 15), (20, 21), (21, 19), (22, 23), (23, 21), (24, 26), (25, 26), (26, 21), (27, 1)]\n",
      "Predicted: [(1, 0), (5, 4), (6, 4), (7, 6), (8, 6), (9, 8), (10, 19), (11, 19), (12, 19), (13, 14), (14, 19), (15, 19), (16, 19), (17, 18), (18, 19), (19, 8), (20, 21), (21, 8), (22, 23), (23, 21), (24, 26), (25, 26), (26, 8)]\n",
      "=================================================================\n",
      "Sentence: ['міністри_1', 'закордонних_2', 'справ_3', 'країн_4', 'єс_5', 'також_6', 'продемонстрували_7', 'готовність_8', 'до_9', 'розвитку_10', 'співпраці_11', 'з_12', 'україною_13', ',_14', 'а_15', 'європейська_16', 'комісія_17', 'оголосила_18', 'про_19', 'готовність_20', 'виділити_21', 'для_22', 'україни_23', '1,2_24', 'мільярди_25', 'для_26', 'боротьби_27', 'з_28', 'наслідками_29', 'коронавірусу_30', '._31']\n",
      "Golden: [(1, 7), (2, 3), (3, 1), (4, 3), (5, 4), (6, 7), (7, 0), (8, 7), (9, 10), (10, 8), (11, 10), (12, 13), (13, 11), (14, 18), (15, 18), (16, 17), (17, 18), (18, 7), (19, 20), (20, 18), (21, 20), (22, 23), (23, 21), (24, 27), (25, 26), (26, 24), (27, 21), (28, 29), (29, 21), (30, 31), (31, 29), (32, 31), (33, 7)]\n",
      "Predicted: [(1, 7), (2, 3), (3, 1), (4, 3), (5, 3), (7, 0), (8, 7), (10, 9), (11, 10), (12, 13), (13, 10), (14, 18), (15, 18), (16, 17), (17, 18), (18, 7), (19, 18), (20, 18), (21, 7), (22, 23), (23, 7), (25, 7), (26, 27), (27, 7), (28, 29), (29, 7), (30, 29), (31, 7)]\n",
      "=================================================================\n",
      "Sentence: ['російська_1', 'федерація_2', 'також_3', 'демонструє_4', 'активність_5', 'щодо_6', 'україни_7', ',_8', 'проте_9', 'дещо_10', 'іншого_11', 'порядку_12', '._13']\n",
      "Golden: [(1, 2), (2, 4), (3, 4), (4, 0), (5, 4), (6, 7), (7, 5), (8, 12), (9, 12), (10, 12), (11, 12), (12, 5), (13, 4)]\n",
      "Predicted: [(1, 2), (4, 2), (5, 4), (6, 7), (7, 4), (8, 12), (9, 12), (10, 12), (11, 12), (12, 4), (13, 2)]\n",
      "=================================================================\n",
      "Sentence: ['у_1', 'москві_2', 'свідомо_3', 'ігнорують_4', 'очевидне_5', ':_6', 'словосполучення_7', '«_8', 'російський_9', 'гуманітарний_10', 'конвой_11', '»_12', 'з_13', '2014_14', 'року_15', 'стало_16', 'синонімом_17', 'незаконного_18', 'втручання_19', 'та_20', 'підтримки_21', 'бойовиків_22', '._23']\n",
      "Golden: [(1, 2), (2, 4), (3, 4), (4, 0), (5, 4), (6, 16), (7, 16), (8, 11), (9, 11), (10, 11), (11, 7), (12, 11), (13, 15), (14, 15), (15, 16), (16, 4), (17, 16), (18, 19), (19, 17), (20, 21), (21, 19), (22, 21), (23, 4)]\n",
      "Predicted: [(1, 2), (3, 4), (4, 2), (5, 4), (6, 7), (7, 5), (8, 11), (9, 10), (10, 11), (11, 5), (12, 4), (17, 16), (18, 19), (19, 17), (20, 21), (21, 19), (22, 21), (23, 2)]\n",
      "=================================================================\n",
      "Total: 156\n",
      "Correctly defined: 82\n",
      "UAS: 0.53\n",
      "Full match: 0.0\n"
     ]
    }
   ],
   "source": [
    "total, tp, full_match = 0, 0, 0\n",
    "for sentence in corpus:\n",
    "    transf_tree = convert_sentence_with_pymorphy_feats(sentence)\n",
    "    g_tree = golden_tree(sentence)\n",
    "    tree = [t for t in transf_tree if type(t[\"id\"])==int]\n",
    "    golden = [(node[\"id\"], node[\"head\"]) for node in g_tree]\n",
    "    predicted = dep_parse(tree, lrc, vec)\n",
    "    total += len(tree)\n",
    "    print(\"Sentence:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in transf_tree])\n",
    "    print(\"Golden:\", golden)\n",
    "    print(\"Predicted:\", predicted)\n",
    "    print(\"=================================================================\")\n",
    "    tp += len(set(golden).intersection(set(predicted)))\n",
    "    if set(golden) == set(predicted):\n",
    "        full_match += 1\n",
    "\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp/total, 2))\n",
    "print(\"Full match:\", round(full_match/len(corpus), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version II\n",
    "-  Golden - залежності, які проставив парсер stanz-и\n",
    "- Порівнюю роботу свого парсера на реченнях, які привела до необхідного вигляду з допомогою stanza (('head', None))\n",
    "- В цій версії фічі кодую по одній, оскільки в conllu - 'feats', OrderedDict([('Animacy', 'Inan'), ('Case', 'Nom'), ('Gender', 'Masc'), ('Number', 'Sing')]), а в stanza - \"Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stanza to transform test sentenses\n",
    "def stanza_sentence(sentence):\n",
    "    transformed_tree = []\n",
    "    doc = stanza_nlp(sentence)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            transformed_tree.append(OrderedDict([('id', int(word.id)), ('form',  word.text), ('lemma',word.lemma),\n",
    "                                     ('upostag', word.upos), ('xpostag', None),\n",
    "                                     ('feats', word.feats), ('head', None), ('deprel', None),\n",
    "                                     ('deps', None), ('misc', None)]))\n",
    "    return transformed_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza feats format is string (\"Case=Nom|Gender=Fem|Number=Sing\")\n",
    "def extract_stanza_features(stack, queue):\n",
    "    features = dict()\n",
    "\n",
    "    if len(stack) > 0:\n",
    "        stack_top = stack[-1]\n",
    "        features[\"s0-word\"] = stack_top[\"form\"]\n",
    "        features[\"s0-lemma\"] = stack_top[\"lemma\"]\n",
    "        features[\"s0-tag\"] = stack_top[\"upostag\"]\n",
    "         # фічі для ост-го слова в стеці\n",
    "        if stack_top[\"feats\"] != None:\n",
    "            fts = stack_top[\"feats\"].split(\"|\")\n",
    "            for i in fts:\n",
    "                idx = i.find(\"=\")\n",
    "                features[\"s0-\"+ i[:idx]] = i[idx+1:]\n",
    "        else:\n",
    "            features[\"s0-feats\"] = False\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-tag\"] = stack[-2][\"upostag\"]\n",
    "        if stack[-2][\"feats\"] != None:\n",
    "            st_fts = stack[-2][\"feats\"].split(\"|\")\n",
    "            for i in st_fts:\n",
    "                idx = i.find(\"=\")\n",
    "                features[\"s1-\"+ i[:idx]] = i[idx+1:]\n",
    "        else:\n",
    "            features[\"s1-feats\"] = False\n",
    "    if queue:\n",
    "        queue_top = queue[0]\n",
    "        features[\"q0-word\"] = queue_top[\"form\"]\n",
    "        features[\"q0-lemma\"] = queue_top[\"lemma\"]\n",
    "        features[\"q0-tag\"] = queue_top[\"upostag\"]\n",
    "        # фічі для 1-го слова в черзі\n",
    "        if queue_top[\"feats\"] != None:\n",
    "            qt_fts = queue_top[\"feats\"].split(\"|\")\n",
    "            for i in qt_fts:\n",
    "                idx = i.find(\"=\")\n",
    "                features[\"q0-\"+ i[:idx]] = i[idx+1:]\n",
    "        else:\n",
    "            features[\"q0-feats\"] = False\n",
    "    if len(queue) > 1:\n",
    "        queue_next = queue[1]\n",
    "        features[\"q1-word\"] = queue_next[\"form\"]\n",
    "        features[\"q1-tag\"] = queue_next[\"upostag\"]\n",
    "        # фічі для 2-го слова в черзі\n",
    "        if queue_next[\"feats\"] != None:\n",
    "            q_fts = queue_next[\"feats\"].split(\"|\")\n",
    "            for i in q_fts:\n",
    "                idx = i.find(\"=\")\n",
    "                features[\"q1-\"+ i[:idx]] = i[idx+1:]\n",
    "        else:\n",
    "            features[\"q1-feats\"] = False\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_parse_with_stanza_feats(sent, oracle, vectorizer):\n",
    "    stack, queue, relations = [ROOT], sent[:], []\n",
    "    while queue or stack:\n",
    "        if stack and not queue:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            features = extract_stanza_features(stack, queue)\n",
    "            action = oracle.predict(vectorizer.transform([features]))[0]\n",
    "            if action == Actions.SHIFT:\n",
    "                stack.append(queue.pop(0))\n",
    "            elif action == Actions.REDUCE:\n",
    "                stack.pop()\n",
    "            elif action == Actions.LEFT:\n",
    "                relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "                stack.pop()\n",
    "            elif action == Actions.RIGHT:\n",
    "                relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "                stack.append(queue.pop(0))\n",
    "            else:\n",
    "                print(\"Unknown action.\")\n",
    "    return sorted(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_v2(stack, queue):\n",
    "    features = dict()\n",
    "\n",
    "    if len(stack) > 0:\n",
    "        stack_top = stack[-1]\n",
    "        features[\"s0-word\"] = stack_top[\"form\"]\n",
    "        features[\"s0-lemma\"] = stack_top[\"lemma\"]\n",
    "        features[\"s0-tag\"] = stack_top[\"upostag\"]\n",
    "         # фічі для ост-го слова в стеці\n",
    "        if stack_top[\"feats\"] != None:\n",
    "            for k, v in stack_top[\"feats\"].items():\n",
    "                features[\"s0-\"+ k] = v\n",
    "        else:\n",
    "            features[\"s0-feats\"] = False\n",
    "    if len(stack) > 1:\n",
    "        features[\"s1-tag\"] = stack[-2][\"upostag\"]\n",
    "        if stack[-2][\"feats\"] != None:\n",
    "            for k, v in stack[-2][\"feats\"].items():\n",
    "                features[\"s1-\"+ k] = v\n",
    "        else:\n",
    "            features[\"s1-feats\"] = False\n",
    "    if queue:\n",
    "        queue_top = queue[0]\n",
    "        features[\"q0-word\"] = queue_top[\"form\"]\n",
    "        features[\"q0-lemma\"] = queue_top[\"lemma\"]\n",
    "        features[\"q0-tag\"] = queue_top[\"upostag\"]\n",
    "        # фічі для 1-го слова в черзі\n",
    "        if queue_top[\"feats\"] != None:\n",
    "            for k, v in queue_top[\"feats\"].items():\n",
    "                features[\"q0-\"+ k] = v\n",
    "        else:\n",
    "            features[\"q0-feats\"] = False\n",
    "    if len(queue) > 1:\n",
    "        queue_next = queue[1]\n",
    "        features[\"q1-word\"] = queue_next[\"form\"]\n",
    "        features[\"q1-tag\"] = queue_next[\"upostag\"]\n",
    "        # фічі для 2-го слова в черзі\n",
    "        if queue_next[\"feats\"] != None:\n",
    "            for k, v in queue_next[\"feats\"].items():\n",
    "                features[\"q1-\"+ k] = v\n",
    "        else:\n",
    "            features[\"q1-feats\"] = False\n",
    "    if len(queue) > 2:\n",
    "        features[\"q2-tag\"] = queue[2][\"upostag\"]\n",
    "    if len(queue) > 3:\n",
    "        features[\"q3-tag\"] = queue[3][\"upostag\"]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_v2(tree):\n",
    "    features, labels = [], []\n",
    "    stack, queue, relations = [ROOT], tree[:], []\n",
    "    \n",
    "    while queue or stack:\n",
    "        action = oracle(stack if len(stack) > 0 else None,\n",
    "                       queue[0] if len(queue) > 0 else None,\n",
    "                       relations)\n",
    "        features.append(extract_features_v2(stack, queue))\n",
    "        labels.append(action.value)\n",
    "        if action == Actions.SHIFT:\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == Actions.REDUCE:\n",
    "            stack.pop()\n",
    "        elif action == Actions.LEFT:\n",
    "            relations.append((stack[-1][\"id\"], queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == Actions.RIGHT:\n",
    "            relations.append((queue[0][\"id\"], stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190298 190298\n"
     ]
    }
   ],
   "source": [
    "train_feats, train_lbls = [], []\n",
    "for tree in train_trees:\n",
    "    tree_feats, tree_lbls = get_data_v2([t for t in tree if type(t[\"id\"])==int])\n",
    "    train_feats += tree_feats\n",
    "    train_lbls += tree_lbls\n",
    "\n",
    "print(len(train_feats), len(train_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of features: 111397\n"
     ]
    }
   ],
   "source": [
    "vec_v2 = vectorizer.fit(train_feats)\n",
    "print(\"\\nTotal number of features:\", len(vec_v2.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190298\n"
     ]
    }
   ],
   "source": [
    "train_feats_vectorized = vec_v2.transform(train_feats)\n",
    "print(len(train_feats_vectorized.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 404 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/escapistdreamer/anaconda3/envs/spacy_nlp/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='saga', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc_v2 = LogisticRegression(random_state=42, solver=\"saga\", multi_class=\"multinomial\", max_iter=1000, verbose=1)\n",
    "lrc_v2.fit(train_feats_vectorized, train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Цінова_1', 'лихоманка_2', 'у_3', 'царині_4', 'енергоресурсів_5', 'засвідчує_6', ',_7', 'що_8', 'світова_9', 'економіка_10', 'переживає_11', 'не_12', 'найкращі_13', 'часи_14', '._15']\n",
      "Golden: [(1, 2), (2, 6), (3, 4), (4, 2), (5, 4), (6, 0), (7, 11), (8, 11), (9, 10), (10, 11), (11, 6), (12, 13), (13, 14), (14, 11), (15, 6)]\n",
      "Predicted: [(1, 2), (2, 0), (2, 6), (3, 4), (4, 2), (4, 6), (5, 4), (6, 0), (7, 11), (8, 11), (9, 10), (10, 11), (11, 6), (12, 14), (13, 14), (14, 11), (15, 6)]\n",
      "========================\n",
      "Sentence: ['Проте_1', 'усвідомлення_2', 'цього_3', 'факту_4', 'не_5', 'завадило_6', 'верховному_7', 'представнику_8', 'ЄС_9', 'у_10', 'закордонних_11', 'справах_12', 'та_13', 'політиці_14', 'безпеки_15', 'Жозепу_16', 'Боррелю_17', 'під_18', 'час_19', 'спілкування_20', 'з_21', 'Дмитром_22', 'Кулебою_23', 'наголосити_24', 'на_25', 'зацікавленості_26', 'Європейського_27', 'Союзу_28', 'у_29', 'співпраці_30', 'з_31', 'Україною_32', 'та_33', 'готовності_34', 'здійснити_35', 'відкладений_36', 'візит_37', 'до_38', 'України_39', 'тільки_40', '-_41', 'но_42', 'для_43', 'цього_44', 'з’явиться_45', 'можливість_46', '._47']\n",
      "Golden: [(1, 6), (2, 6), (3, 4), (4, 2), (5, 6), (6, 0), (7, 8), (8, 6), (9, 8), (10, 12), (11, 12), (12, 8), (13, 14), (14, 12), (15, 14), (16, 15), (17, 16), (18, 19), (19, 24), (20, 19), (21, 22), (22, 20), (23, 22), (24, 6), (25, 26), (26, 24), (27, 28), (28, 26), (29, 30), (30, 26), (31, 32), (32, 30), (33, 34), (34, 32), (35, 34), (36, 37), (37, 35), (38, 39), (39, 37), (40, 45), (41, 45), (42, 45), (43, 44), (44, 45), (45, 6), (46, 45), (47, 45)]\n",
      "Predicted: [(1, 6), (2, 6), (3, 4), (4, 2), (5, 6), (6, 0), (7, 8), (8, 6), (9, 8), (10, 12), (11, 12), (12, 9), (13, 14), (14, 12), (15, 14), (16, 15), (17, 16), (18, 19), (19, 24), (20, 19), (21, 22), (22, 19), (23, 22), (24, 6), (25, 26), (26, 24), (27, 28), (28, 26), (29, 30), (30, 28), (31, 32), (32, 30), (33, 34), (34, 30), (35, 24), (36, 35), (36, 45), (37, 36), (38, 39), (39, 37), (40, 45), (41, 45), (42, 45), (43, 44), (44, 45), (45, 35), (46, 45), (47, 6)]\n",
      "========================\n",
      "Sentence: ['Нагадаю_1', ',_2', 'що_3', 'пан_4', 'Боррель_5', 'мав_6', 'намір_7', 'відвідати_8', 'Донбас_9', ',_10', 'що_11', 'протягом_12', 'останніх_13', 'років_14', 'стало_15', 'ледь_16', 'не_17', 'обов’язковою_18', 'частиною_19', 'для_20', 'перебування_21', 'в_22', 'Україні_23', 'поважних_24', 'іноземних_25', 'гостей_26', '._27']\n",
      "Golden: [(1, 0), (2, 6), (3, 6), (4, 6), (5, 4), (6, 1), (7, 6), (8, 7), (9, 8), (10, 15), (11, 15), (12, 14), (13, 14), (14, 15), (15, 9), (16, 18), (17, 18), (18, 19), (19, 15), (20, 21), (21, 19), (22, 23), (23, 21), (24, 26), (25, 26), (26, 21), (27, 1)]\n",
      "Predicted: [(1, 0), (4, 6), (5, 4), (7, 6), (8, 7), (9, 8), (10, 15), (11, 15), (12, 14), (13, 14), (14, 15), (15, 6), (16, 19), (17, 18), (18, 19), (19, 15), (20, 21), (21, 19), (22, 23), (23, 21), (24, 26), (25, 26), (26, 21), (27, 6)]\n",
      "========================\n",
      "Sentence: ['Міністри_1', 'закордонних_2', 'справ_3', 'країн_4', 'ЄС_5', 'також_6', 'продемонстрували_7', 'готовність_8', 'до_9', 'розвитку_10', 'співпраці_11', 'з_12', 'Україною_13', ',_14', 'а_15', 'Європейська_16', 'Комісія_17', 'оголосила_18', 'про_19', 'готовність_20', 'виділити_21', 'для_22', 'України_23', '1_24', ',_25', '2_26', 'мільярди_27', 'для_28', 'боротьби_29', 'з_30', 'наслідками_31', 'коронавірусу_32', '._33']\n",
      "Golden: [(1, 7), (2, 3), (3, 1), (4, 3), (5, 4), (6, 7), (7, 0), (8, 7), (9, 10), (10, 8), (11, 10), (12, 13), (13, 11), (14, 18), (15, 18), (16, 17), (17, 18), (18, 7), (19, 20), (20, 18), (21, 20), (22, 23), (23, 21), (24, 27), (25, 26), (26, 24), (27, 21), (28, 29), (29, 21), (30, 31), (31, 29), (32, 31), (33, 7)]\n",
      "Predicted: [(1, 7), (2, 3), (3, 1), (4, 3), (5, 4), (6, 5), (7, 0), (8, 7), (9, 10), (10, 8), (11, 10), (12, 13), (13, 11), (14, 18), (15, 18), (16, 17), (17, 18), (18, 7), (19, 20), (20, 18), (21, 18), (22, 23), (23, 21), (24, 21), (25, 27), (26, 27), (27, 21), (28, 29), (29, 27), (30, 31), (31, 21), (32, 31), (33, 7)]\n",
      "========================\n",
      "Sentence: ['Російська_1', 'Федерація_2', 'також_3', 'демонструє_4', 'активність_5', 'щодо_6', 'України_7', ',_8', 'проте_9', 'дещо_10', 'іншого_11', 'порядку_12', '._13']\n",
      "Golden: [(1, 2), (2, 4), (3, 4), (4, 0), (5, 4), (6, 7), (7, 5), (8, 12), (9, 12), (10, 12), (11, 12), (12, 5), (13, 4)]\n",
      "Predicted: [(1, 2), (3, 2), (4, 2), (5, 4), (6, 7), (7, 5), (8, 12), (9, 12), (10, 12), (11, 12), (12, 4), (13, 2)]\n",
      "========================\n",
      "Sentence: ['У_1', 'Москві_2', 'свідомо_3', 'ігнорують_4', 'очевидне_5', ':_6', 'словосполучення_7', '«_8', 'російський_9', 'гуманітарний_10', 'конвой_11', '»_12', 'з_13', '2014_14', 'року_15', 'стало_16', 'синонімом_17', 'незаконного_18', 'втручання_19', 'та_20', 'підтримки_21', 'бойовиків_22', '._23']\n",
      "Golden: [(1, 2), (2, 4), (3, 4), (4, 0), (5, 4), (6, 16), (7, 16), (8, 11), (9, 11), (10, 11), (11, 7), (12, 11), (13, 15), (14, 15), (15, 16), (16, 4), (17, 16), (18, 19), (19, 17), (20, 21), (21, 19), (22, 21), (23, 4)]\n",
      "Predicted: [(1, 2), (2, 0), (2, 4), (3, 4), (4, 0), (5, 4), (5, 11), (6, 7), (7, 5), (8, 11), (9, 10), (10, 11), (11, 4), (12, 11), (13, 15), (14, 15), (15, 16), (16, 4), (17, 16), (18, 19), (19, 17), (20, 21), (21, 19), (22, 21), (23, 4)]\n",
      "========================\n",
      "Total: 158\n",
      "Correctly defined: 126\n",
      "UAS: 0.8\n",
      "Full match: 0.0\n"
     ]
    }
   ],
   "source": [
    "# version 2:\n",
    "total, tp, full_match = 0, 0, 0\n",
    "for sentence in corpus:\n",
    "    transformed_tree = stanza_sentence(sentence)\n",
    "    stanza_golden = golden_tree(sentence)\n",
    "    tree = [t for t in transformed_tree if type(t[\"id\"])==int]\n",
    "    golden = [(node[\"id\"], node[\"head\"]) for node in stanza_golden]\n",
    "    predicted = dep_parse_with_stanza_feats(tree, lrc_v2, vec_v2)\n",
    "    total += len(tree)\n",
    "    print(\"Sentence:\", [i[\"form\"]+\"_\"+str(i[\"id\"]) for i in transformed_tree])\n",
    "    print(\"Golden:\", golden)\n",
    "    print(\"Predicted:\", predicted)\n",
    "    print(\"========================\")\n",
    "    tp += len(set(golden).intersection(set(predicted)))\n",
    "    if set(golden) == set(predicted):\n",
    "        full_match += 1\n",
    "\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"UAS:\", round(tp/total, 2))\n",
    "print(\"Full match:\", round(full_match/len(corpus), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "- З 2-х результатів можна зробити такі висновки:\n",
    "- 1) Додавання граматичних ознак покращує роботу парсера. Це видно з Total: 158, Correctly defined: 126, UAS: 0.8, де кожна граматична ознака була закодована окремо, а сам набір граматичних ознак був одинаковим для тренувальних та тестових даних.\n",
    "- 2) Хоча ці результати теж відносні, оскільки за golden взято залежності, які проставив парсер stanz-и."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
