## Logistic Regression:

```
              precision    recall  f1-score   support

        left       0.86      0.87      0.86      6371
      reduce       0.85      0.78      0.81      6875
       right       0.75      0.79      0.77      5996
       shift       0.85      0.87      0.86      6578

    accuracy                           0.83     25820
   macro avg       0.83      0.83      0.83     25820
weighted avg       0.83      0.83      0.83     25820
```

-> correct: 8717 out of 12574

## Feature Extraction Improvement & Logistic Rergression:

```
        left       0.94      0.95      0.94      6371
      reduce       0.90      0.85      0.88      6875
       right       0.80      0.83      0.82      5996
       shift       0.87      0.87      0.87      6578

    accuracy                           0.88     25820
   macro avg       0.88      0.88      0.88     25820
weighted avg       0.88      0.88      0.88     25820
```

-> correct: 9735 out of 12574

## Feature Extraction Improvement & XGBoost:

```
              precision    recall  f1-score   support

        left       0.94      0.96      0.95      6371
      reduce       0.93      0.85      0.89      6875
       right       0.82      0.88      0.85      5996
       shift       0.89      0.89      0.89      6578

    accuracy                           0.89     25820
   macro avg       0.89      0.89      0.89     25820
weighted avg       0.90      0.89      0.89     25820
```

-> І хоча XGBoost використовує всі ядра на відміну від логістичної регресії, але predict займає цілу вічність, тож результат не є адекватним.

## Adhoc Oracle & Logistic Regression:

```
              precision    recall  f1-score   support

        left       0.93      0.95      0.94      6371
      reduce       0.90      0.86      0.88      6875
       right       0.80      0.84      0.82      5996
      right2       0.81      0.84      0.82      1265
       shift       0.83      0.81      0.82      5313

    accuracy                           0.86     25820
   macro avg       0.85      0.86      0.86     25820
weighted avg       0.87      0.86      0.86     25820
```

-> correct: 9771 out of 12574
LAS: 77.7%
Не зважаючи на нижчий f1-score модель краще вгадує зв'язки.

## Використання на нових даних:

```
Не працюй неохоче або без турбот про загальне добро, без належної уважливости або в розсіяності.
працюй <- Не
ROOT <- працюй
працюй <- неохоче
загальне <- добро
належної <- уважливости
розсіяності <- або
розсіяності <- в
уважливости <- розсіяності
працюй <- .
Зробивши кілька ковтків, я поглянув на обриси хмар.
ROOT <- Зробивши
кілька <- ковтків
Зробивши <- поглянув
обриси <- хмар
Зробивши <- .
На хвилину маємо перервати історію про студентів і лотерею, тому що, коли ми заговорили про адитивність очікуваної цінності, я не можу не розповісти про одне з найкрасивіших з відомих мені доведень.
маємо <- На
ROOT <- маємо
маємо <- перервати
перервати <- історію
історію <- про
про <- студентів
тому <- що
ми <- заговорили
заговорили <- про
про <- адитивність
очікуваної <- цінності
можу <- не
я <- можу
розповісти <- не
можу <- розповісти
розповісти <- про
розповісти <- одне
відомих <- мені
мені <- доведень
можу <- .
```

Я якщо чесно очікував кращих результатів, бо коли фразу "загальне добро" парсить як "загальне <- добро", то це щонайменше дивує.
Ще у мене є сумніви, що в реченні "Зробивши кілька ковтків, я поглянув на обриси хмар." саме "зробивши" посилається на ROOT, а не "поглянув"