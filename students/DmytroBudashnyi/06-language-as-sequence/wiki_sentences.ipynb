{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset from Google Research\n",
    "One million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia edits.\n",
    "https://github.com/google-research-datasets/wiki-split.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data_dir = '../../../../wiki-split/'\n",
    "train_filename = data_dir + 'train.tsv.zip'\n",
    "valid_filename = data_dir + 'validation.tsv'\n",
    "test_filename = data_dir + 'test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    out_data = []\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    for index, row in data.iterrows():\n",
    "        out_data.append([s.strip() for s in row[1].split('<::::>')])\n",
    "\n",
    "    return pd.DataFrame(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>' '' BDSM is solely based on consensual activi...</td>\n",
       "      <td>The concepts presented by de Sade are not in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>' '' Critics criticized the use of the dispute...</td>\n",
       "      <td>For example , The Traditional Values Coalition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>' '' Do Re Mi '' ' is a song by Kurt Cobain , ...</td>\n",
       "      <td>It is believed to be one of the final songs he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>' '' For Robert Price '' docetism '' , togethe...</td>\n",
       "      <td>In one version , as in Marcionism , Christ was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>' '' He was the fourth of the nine children of...</td>\n",
       "      <td>His father was a Presbyterian minister who rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989938</td>\n",
       "      <td>` Arta is a village in Djibouti .</td>\n",
       "      <td>It is located in the Arta Region .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989939</td>\n",
       "      <td>` Assa Gaila is a town in Djibouti .</td>\n",
       "      <td>It is located in the Tadjoura region .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989940</td>\n",
       "      <td>` Jackson was linked with actor Gary Pendergas...</td>\n",
       "      <td>They formed Shoot The Moon Productions togethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989941</td>\n",
       "      <td>` Umar appointed him to be the judge of Kufah .</td>\n",
       "      <td>He was very young at the time .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989942</td>\n",
       "      <td>` You 've Bbeen here six years already and hav...</td>\n",
       "      <td>But Denise , bound and determined as ever , se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989943 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  \\\n",
       "0       ' '' BDSM is solely based on consensual activi...   \n",
       "1       ' '' Critics criticized the use of the dispute...   \n",
       "2       ' '' Do Re Mi '' ' is a song by Kurt Cobain , ...   \n",
       "3       ' '' For Robert Price '' docetism '' , togethe...   \n",
       "4       ' '' He was the fourth of the nine children of...   \n",
       "...                                                   ...   \n",
       "989938                  ` Arta is a village in Djibouti .   \n",
       "989939               ` Assa Gaila is a town in Djibouti .   \n",
       "989940  ` Jackson was linked with actor Gary Pendergas...   \n",
       "989941    ` Umar appointed him to be the judge of Kufah .   \n",
       "989942  ` You 've Bbeen here six years already and hav...   \n",
       "\n",
       "                                                        1  \n",
       "0       The concepts presented by de Sade are not in a...  \n",
       "1       For example , The Traditional Values Coalition...  \n",
       "2       It is believed to be one of the final songs he...  \n",
       "3       In one version , as in Marcionism , Christ was...  \n",
       "4       His father was a Presbyterian minister who rai...  \n",
       "...                                                   ...  \n",
       "989938                 It is located in the Arta Region .  \n",
       "989939             It is located in the Tadjoura region .  \n",
       "989940  They formed Shoot The Moon Productions togethe...  \n",
       "989941                    He was very young at the time .  \n",
       "989942  But Denise , bound and determined as ever , se...  \n",
       "\n",
       "[989943 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = read_data(train_filename)\n",
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>'' A Living Library '' was Sherk 's work that ...</td>\n",
       "      <td>She transformed these spaces for to build educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>'' All Singing , All Dancing '' is the elevent...</td>\n",
       "      <td>It originally aired on the Fox network in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'' Already Gone '' is a mid-tempo ballad set i...</td>\n",
       "      <td>The female narrator describes her life and alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'' Bafana Bafana '' is a nickname given to the...</td>\n",
       "      <td>It is Zulu and translates literally as '' the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>'' Blah Blah Blah '' is a song by American pop...</td>\n",
       "      <td>It is the second single from her debut album ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4994</td>\n",
       "      <td>Zahm Hall , a male dormitory at Notre Dame , i...</td>\n",
       "      <td>The dorm 's chapel is dedicated to St. Albert ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4995</td>\n",
       "      <td>Zahn was first diagnosed in the late 1990s .</td>\n",
       "      <td>Thereafter , he became a vocal supporter of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4996</td>\n",
       "      <td>Zeinab Elobeid Yousif ( 1952 -- 19 March 2016 ...</td>\n",
       "      <td>She was the first Sudanese female to be licens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>Zen Peacemakers have a 34 - acre campus , the ...</td>\n",
       "      <td>In the Untied States affiliates include includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>Zenica ( Cyrillic : '' Зеница '' ) is an indus...</td>\n",
       "      <td>It is the capital of the Zenica - Doboj Canton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     '' A Living Library '' was Sherk 's work that ...   \n",
       "1     '' All Singing , All Dancing '' is the elevent...   \n",
       "2     '' Already Gone '' is a mid-tempo ballad set i...   \n",
       "3     '' Bafana Bafana '' is a nickname given to the...   \n",
       "4     '' Blah Blah Blah '' is a song by American pop...   \n",
       "...                                                 ...   \n",
       "4994  Zahm Hall , a male dormitory at Notre Dame , i...   \n",
       "4995       Zahn was first diagnosed in the late 1990s .   \n",
       "4996  Zeinab Elobeid Yousif ( 1952 -- 19 March 2016 ...   \n",
       "4997  Zen Peacemakers have a 34 - acre campus , the ...   \n",
       "4998  Zenica ( Cyrillic : '' Зеница '' ) is an indus...   \n",
       "\n",
       "                                                      1  \n",
       "0     She transformed these spaces for to build educ...  \n",
       "1     It originally aired on the Fox network in the ...  \n",
       "2     The female narrator describes her life and alw...  \n",
       "3     It is Zulu and translates literally as '' the ...  \n",
       "4     It is the second single from her debut album ,...  \n",
       "...                                                 ...  \n",
       "4994  The dorm 's chapel is dedicated to St. Albert ...  \n",
       "4995  Thereafter , he became a vocal supporter of th...  \n",
       "4996  She was the first Sudanese female to be licens...  \n",
       "4997  In the Untied States affiliates include includ...  \n",
       "4998  It is the capital of the Zenica - Doboj Canton...  \n",
       "\n",
       "[4999 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_df = read_data(valid_filename)\n",
    "valid_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>' Eden Black ' was grown from seed in the late...</td>\n",
       "      <td>Under his conditions it produces pitchers that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>' Wilson should extend his stint on The Voice ...</td>\n",
       "      <td>Given that they 're pulling out all the stops ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>'' '' New York Mining Disaster 1941 '' '' was ...</td>\n",
       "      <td>It was their second EP and , like their first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>'' ADAPTOGENS : Herbs for Strength , Stamina ,...</td>\n",
       "      <td>Contains a detailed monograph on Schisandra ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>'' Aerodynamic '' is an song by Daft Punk .</td>\n",
       "      <td>It is a instrumental particularly well - known...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4994</td>\n",
       "      <td>Zhang 's grandfather , convinced that Renjie s...</td>\n",
       "      <td>The family arranged a marriage for him with Ya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4995</td>\n",
       "      <td>Zhu De became the commander of the Eighth Rout...</td>\n",
       "      <td>Agents working under Zhou Enlai set up a headq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4996</td>\n",
       "      <td>Zile Huma was born into a filmi and musical fa...</td>\n",
       "      <td>She was the youngest of the three children of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>Zion 's Hill , also known by its former name H...</td>\n",
       "      <td>It is the first town one reaches after leaving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>Ziryab also introduced bleached white clothing...</td>\n",
       "      <td>He introduced the Tablecloth and created a new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "0     ' Eden Black ' was grown from seed in the late...   \n",
       "1     ' Wilson should extend his stint on The Voice ...   \n",
       "2     '' '' New York Mining Disaster 1941 '' '' was ...   \n",
       "3     '' ADAPTOGENS : Herbs for Strength , Stamina ,...   \n",
       "4           '' Aerodynamic '' is an song by Daft Punk .   \n",
       "...                                                 ...   \n",
       "4994  Zhang 's grandfather , convinced that Renjie s...   \n",
       "4995  Zhu De became the commander of the Eighth Rout...   \n",
       "4996  Zile Huma was born into a filmi and musical fa...   \n",
       "4997  Zion 's Hill , also known by its former name H...   \n",
       "4998  Ziryab also introduced bleached white clothing...   \n",
       "\n",
       "                                                      1  \n",
       "0     Under his conditions it produces pitchers that...  \n",
       "1     Given that they 're pulling out all the stops ...  \n",
       "2     It was their second EP and , like their first ...  \n",
       "3     Contains a detailed monograph on Schisandra ch...  \n",
       "4     It is a instrumental particularly well - known...  \n",
       "...                                                 ...  \n",
       "4994  The family arranged a marriage for him with Ya...  \n",
       "4995  Agents working under Zhou Enlai set up a headq...  \n",
       "4996  She was the youngest of the three children of ...  \n",
       "4997  It is the first town one reaches after leaving...  \n",
       "4998  He introduced the Tablecloth and created a new...  \n",
       "\n",
       "[4999 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = read_data(test_filename)\n",
    "test_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = '../../../tasks/06-language-as-sequence/run-on-test.json'\n",
    "with open(test_filename) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_tokens = []\n",
    "test_classes = []\n",
    "for sentence in test_data:\n",
    "    for word in sentence:\n",
    "        test_tokens.append(word[0])\n",
    "        test_classes.append(word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think the magnitude of a benefit and error rates that were chosen were reasonable They were standard from our learning . Economists on both the left and right broadly agree that the need for stimulative government spending is necessary to prevent a further collapse of the global economic system'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False False False False False False False False False False False False False False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([str(c) for c in test_classes[:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just predict everything False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.98      4542\n",
      "        True       0.00      0.00      0.00       155\n",
      "\n",
      "    accuracy                           0.97      4697\n",
      "   macro avg       0.48      0.50      0.49      4697\n",
      "weighted avg       0.94      0.97      0.95      4697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmytro/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_classes, [False] * len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare validation data from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_list(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        result.append([word, False])\n",
    "\n",
    "    if word not in ('.', '?', '!'):\n",
    "        result[-1][1] = True\n",
    "\n",
    "    return result\n",
    "\n",
    "val_data_list = []\n",
    "for i, row in test_data_df.iterrows():\n",
    "    sentence = row[0].split()\n",
    "    del sentence[-1]\n",
    "    sentence1 = row[1].split()\n",
    "    sentence1[0].lower()\n",
    "\n",
    "    result = make_val_list(sentence)\n",
    "    result.extend(make_val_list(sentence1))\n",
    "    val_data_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'\", False],\n",
       " ['Eden', False],\n",
       " ['Black', False],\n",
       " [\"'\", False],\n",
       " ['was', False],\n",
       " ['grown', False],\n",
       " ['from', False],\n",
       " ['seed', False],\n",
       " ['in', False],\n",
       " ['the', False],\n",
       " ['late', False],\n",
       " ['1980s', False],\n",
       " ['by', False],\n",
       " ['Stephen', False],\n",
       " ['Morley', True],\n",
       " ['Under', False],\n",
       " ['his', False],\n",
       " ['conditions', False],\n",
       " ['it', False],\n",
       " ['produces', False],\n",
       " ['pitchers', False],\n",
       " ['that', False],\n",
       " ['are', False],\n",
       " ['almost', False],\n",
       " ['completley', False],\n",
       " ['black', False],\n",
       " ['.', False]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in valid_data_df.iterrows():\n",
    "    sentence = row[0].split()\n",
    "    del sentence[-1]\n",
    "    sentence1 = row[1].split()\n",
    "\n",
    "    result = make_val_list(sentence)\n",
    "    result.extend(make_val_list(sentence1))\n",
    "    val_data_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Zenica', False],\n",
       " ['(', False],\n",
       " ['Cyrillic', False],\n",
       " [':', False],\n",
       " [\"''\", False],\n",
       " ['Зеница', False],\n",
       " [\"''\", False],\n",
       " [')', False],\n",
       " ['is', False],\n",
       " ['an', False],\n",
       " ['industrial', False],\n",
       " ['city', False],\n",
       " ['(', False],\n",
       " ['the', False],\n",
       " ['third', False],\n",
       " ['largest', False],\n",
       " [',', False],\n",
       " ['after', False],\n",
       " ['Sarajevo', False],\n",
       " ['and', False],\n",
       " ['Banja', False],\n",
       " ['Luka', False],\n",
       " [')', False],\n",
       " ['and', False],\n",
       " ['municipality', False],\n",
       " ['in', False],\n",
       " ['Bosnia', False],\n",
       " ['and', False],\n",
       " ['Herzegovina', True],\n",
       " ['It', False],\n",
       " ['is', False],\n",
       " ['the', False],\n",
       " ['capital', False],\n",
       " ['of', False],\n",
       " ['the', False],\n",
       " ['Zenica', False],\n",
       " ['-', False],\n",
       " ['Doboj', False],\n",
       " ['Canton', False],\n",
       " ['of', False],\n",
       " ['the', False],\n",
       " ['Federation', False],\n",
       " ['of', False],\n",
       " ['Bosnia', False],\n",
       " ['and', False],\n",
       " ['Herzegovina', False],\n",
       " ['entity', False],\n",
       " ['.', False]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "random.shuffle(val_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', False],\n",
       " ['is', False],\n",
       " ['a', False],\n",
       " ['popular', False],\n",
       " ['misconception', False],\n",
       " ['that', False],\n",
       " ['these', False],\n",
       " ['lakes', False],\n",
       " ['are', False],\n",
       " ['filled', False],\n",
       " ['via', False],\n",
       " ['the', False],\n",
       " ['Nepean', False],\n",
       " ['River', True],\n",
       " ['They', False],\n",
       " ['are', False],\n",
       " ['not', False],\n",
       " [',', False],\n",
       " ['they', False],\n",
       " ['are', False],\n",
       " ['filled', False],\n",
       " ['via', False],\n",
       " ['rain', False],\n",
       " ['water', False],\n",
       " ['and', False],\n",
       " ['ground', False],\n",
       " ['water', False],\n",
       " ['.', False]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = []\n",
    "val_classes = []\n",
    "for sentence in val_data_list:\n",
    "    for word in sentence:\n",
    "        val_tokens.append(word[0])\n",
    "        val_classes.append(word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Institute has planned to increase the PRM batch size by another 60 , from the academic year 2013 -\n",
      "False False False False False False False False False False False False False False False False False False False False\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(val_tokens[:20]))\n",
    "print(' '.join([str(c) for c in val_classes[:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99    350435\n",
      "        True       0.00      0.00      0.00      9994\n",
      "\n",
      "    accuracy                           0.97    360429\n",
      "   macro avg       0.49      0.50      0.49    360429\n",
      "weighted avg       0.95      0.97      0.96    360429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_classes, [False] * len(val_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our baseline works the same on validation :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "for i, row in train_data_df.iterrows():\n",
    "    sentence = row[0].split()\n",
    "    sentence1 = row[1].split()\n",
    "    del sentence[-1]\n",
    "\n",
    "    result = make_val_list(sentence)\n",
    "    result.extend(make_val_list(sentence1))\n",
    "    train_data_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "train_classes = []\n",
    "for sentence in train_data_list:\n",
    "    for word in sentence:\n",
    "        train_tokens.append(word[0])\n",
    "        train_classes.append(word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts shape: (35653401, 533517)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_tokens)\n",
    "print('Train counts shape:', train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cross_val] F1: 0.4943167310504299\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1, solver='sag', multi_class=\"multinomial\", max_iter=100, n_jobs=-1)\n",
    "scores = cross_val_score(lr, train_counts, train_classes, cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "print('[cross_val] F1:', sum(scores)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "oh, this is not funny at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val counts shape: (360429, 533517)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99    350435\n",
      "        True       0.52      0.00      0.00      9994\n",
      "\n",
      "    accuracy                           0.97    360429\n",
      "   macro avg       0.75      0.50      0.49    360429\n",
      "weighted avg       0.96      0.97      0.96    360429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr.fit(train_counts, train_classes)\n",
    "\n",
    "val_counts = count_vect.transform(val_tokens)\n",
    "print('Val counts shape:', val_counts.shape)\n",
    "y_pred = lr.predict(val_counts)\n",
    "\n",
    "print(classification_report(val_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Мені здається я потроху доходжу до висновку, що мій датасет **надто** відрізняється від того, на котрому відбувається тест. Тому він не вгадує нічого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0].text\n",
    "    postag = sent[i][0].pos_\n",
    "    lemma = sent[i][0].lemma_\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'lemma': lemma,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper() and len(word) > 1,\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0].text\n",
    "        postag1 = sent[i-1][0].pos_\n",
    "        lemma1 = sent[i-1][0].lemma_\n",
    "        features.update({\n",
    "            '-1:word.lower()': lemma1,\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper() and len(word1) > 1,\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0].text\n",
    "        postag1 = sent[i+1][0].pos_\n",
    "        lemma1 = sent[i+1][0].lemma_\n",
    "        features.update({\n",
    "            '+1:word.lower()': lemma1,\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper() and len(word1) > 1,\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_list(sentence):\n",
    "    result = []\n",
    "    is_end_found = False\n",
    "    for token in sentence:\n",
    "        if not is_end_found and token.text in ('.', '?', '!'):\n",
    "            result[-1][1] = True\n",
    "            is_end_found = True\n",
    "        else:\n",
    "            result.append([token, False])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361579, 361579, 361579)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_temp_data = []\n",
    "for i, row in valid_data_df.iterrows():\n",
    "    doc = row[0] + ' ' + row[1]\n",
    "    val_temp_data.append(doc)\n",
    "\n",
    "for i, row in test_data_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    sentence2_split = row[1].split()\n",
    "    doc = sentence + ' ' + sentence2_split[0].lower() + ' ' + ' '.join(sentence2_split[1:])\n",
    "    val_temp_data.append(doc)\n",
    "\n",
    "val_data = []\n",
    "for doc in nlp.pipe(val_temp_data):\n",
    "    val_data.append(make_val_list(doc))\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(val_data)\n",
    "\n",
    "val_features = []\n",
    "val_tokens = []\n",
    "val_classes = []\n",
    "for sentence in val_data:\n",
    "    for i, word in enumerate(sentence):\n",
    "        val_features.append(word2features(sentence, i))\n",
    "        val_tokens.append(word[0])\n",
    "        val_classes.append(word[1])\n",
    "len(val_features), len(val_tokens), len(val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp_data = []\n",
    "for i, row in train_data_df.iterrows():\n",
    "    doc = row[0] + ' ' + row[1]\n",
    "    train_temp_data.append(doc)\n",
    "\n",
    "for i, row in test_data_df.iterrows():\n",
    "    sentence = row[0]\n",
    "    sentence2_split = row[1].split()\n",
    "    doc = sentence + ' ' + sentence2_split[0].lower() + ' ' + ' '.join(sentence2_split[1:])\n",
    "    train_temp_data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35951066, 35951066, 35951066)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "for doc in nlp.pipe(train_temp_data):\n",
    "    train_data.append(make_val_list(doc))\n",
    "\n",
    "train_features = []\n",
    "train_tokens = []\n",
    "train_classes = []\n",
    "for sentence in train_data:\n",
    "    for i, word in enumerate(sentence):\n",
    "        train_features.append(word2features(sentence, i))\n",
    "        train_tokens.append(word[0])\n",
    "        train_classes.append(word[1])\n",
    "len(train_features), len(train_tokens), len(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  1851555\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "vec = vectorizer.fit(train_features)\n",
    "print(\"Total number of features: \", len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features_vectorized = vec.transform(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_vectorized = vec.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cross_val] F1: 0.9423691593851049\n"
     ]
    }
   ],
   "source": [
    "lrf = LogisticRegression(random_state=1, solver='sag', multi_class=\"multinomial\", max_iter=100, n_jobs=-1)\n",
    "scores = cross_val_score(lrf, train_features_vectorized[:1000000], train_classes[:1000000], cv=3, scoring='f1_macro', n_jobs=-1)\n",
    "print('[cross_val] F1:', sum(scores)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmytro/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99    351581\n",
      "        True       0.93      0.45      0.61      9998\n",
      "\n",
      "    accuracy                           0.98    361579\n",
      "   macro avg       0.96      0.73      0.80    361579\n",
      "weighted avg       0.98      0.98      0.98    361579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrf.fit(train_features_vectorized, train_classes)\n",
    "\n",
    "y_pred = lrf.predict(val_features_vectorized)\n",
    "\n",
    "print(classification_report(val_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "Йой, на валідації це дуже непогано працює"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А тепер на тестових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4735, 4735, 4697, 4735, 4697)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word2test_features(sent, i):\n",
    "    word = sent[i].text\n",
    "    postag = sent[i].pos_\n",
    "    lemma = sent[i].lemma_\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'lemma': lemma,\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper() and len(word) > 1,\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1].text\n",
    "        postag1 = sent[i-1].pos_\n",
    "        lemma1 = sent[i-1].lemma_\n",
    "        features.update({\n",
    "            '-1:word.lower()': lemma1,\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper() and len(word1) > 1,\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1].text\n",
    "        postag1 = sent[i+1].pos_\n",
    "        lemma1 = sent[i+1].lemma_\n",
    "        features.update({\n",
    "            '+1:word.lower()': lemma1,\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper() and len(word1) > 1,\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "sentences = []\n",
    "sentences_classes = []\n",
    "\n",
    "prev_i = 0\n",
    "for i, token in enumerate(test_tokens):\n",
    "    if (test_classes[i]) or (token in ('.', '!', '?')):\n",
    "        sentences.append(' '.join(test_tokens[prev_i:i+1]))\n",
    "        prev_i = i + 1\n",
    "\n",
    "def make_test_list(sentence):\n",
    "    result = []\n",
    "    for i, token in enumerate(sentence):\n",
    "        class_ = False\n",
    "        if (i == len(sentence) - 1) and token.text not in ('.', '!', '?'):\n",
    "            class_ = True\n",
    "        result.append([token, class_])\n",
    "\n",
    "    return result\n",
    "\n",
    "new_test_features = []\n",
    "new_test_tokens = []\n",
    "new_test_classes = []\n",
    "for sentence in nlp.pipe(sentences):\n",
    "    for i, word in enumerate(make_test_list(sentence)):\n",
    "        new_test_features.append(word2test_features(sentence, i))\n",
    "        new_test_tokens.append(word[0])\n",
    "        new_test_classes.append(word[1])\n",
    "\n",
    "len(new_test_features), len(new_test_tokens), len(test_tokens), len(new_test_classes), len(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 155)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(new_test_classes), sum(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_vectorized = vec.transform(new_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      4580\n",
      "        True       0.00      0.00      0.00       155\n",
      "\n",
      "    accuracy                           0.96      4735\n",
      "   macro avg       0.48      0.50      0.49      4735\n",
      "weighted avg       0.94      0.96      0.95      4735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lrf.predict(test_features_vectorized)\n",
    "\n",
    "print(classification_report(new_test_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Висновки\n",
    "\n",
    "Щоб щось вийшло, треба щоб дані було максимально схожі на ті, котрі треба передбачати :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
