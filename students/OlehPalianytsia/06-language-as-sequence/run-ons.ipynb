{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog's (NOUN) - nsubj - are\n",
      "are (AUX) - ROOT - are\n",
      "funny (ADJ) - acomp - are\n",
      "animal's (PUNCT) - punct - are\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "dummy_tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def nlp_no_tokenize(text):\n",
    "    tokenizer = nlp.tokenizer\n",
    "    nlp.tokenizer = dummy_tokenizer\n",
    "    model = nlp(text)\n",
    "    nlp.tokenizer = tokenizer\n",
    "    return model\n",
    "\n",
    "for tok in nlp_no_tokenize(\"dog's are funny animal's\"):\n",
    "    print(\"{} ({}) - {} - {}\".format(tok.text, tok.pos_, tok.dep_, tok.head.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Готуємо тестувальний датасет. Дані будуть складатись з Spacy токенів. Це дозволить нам визначити межі речень без додаткового маркування, а також у них міститиметься інша корисна інформація (POS теги, залежності і т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../../tasks/06-language-as-sequence/run-on-test.json', 'r') as f:\n",
    "    run_on_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 4542, True: 155})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def prepare_test_data(run_on_test):\n",
    "    X = []\n",
    "    y = []\n",
    "    for sent in run_on_test:\n",
    "        words = [word for word, _ in sent]\n",
    "        labels = [label for _, label in sent]\n",
    "        \n",
    "        model = nlp_no_tokenize(' '.join(words))\n",
    "        if (not len(sent) == len(model) or \n",
    "                any(not tok.text == word for tok, word in zip(model, words))):\n",
    "            print('WARN: bad model: ', model)\n",
    "        X.extend(model)\n",
    "        y.extend(labels)\n",
    "    return X, y\n",
    "\n",
    "X_test, y_test = prepare_test_data(run_on_test)\n",
    "collections.Counter(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мінімалістичний бейзлайн на одному правилі :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_classify_token(tok):\n",
    "    next = tok.nbor() if tok.i < len(tok.doc) - 1 else None    \n",
    "    return (not next is None) and next.text.istitle() and not tok.text.istitle()\n",
    "\n",
    "def baseline_classify(X):\n",
    "    return [baseline_classify_token(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.96      0.97      4542\n",
      "        True       0.29      0.44      0.35       155\n",
      "\n",
      "    accuracy                           0.95      4697\n",
      "   macro avg       0.64      0.70      0.66      4697\n",
      "weighted avg       0.96      0.95      0.95      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, baseline_classify(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Підготовка тренувальних даних. Для цього використав корпус Брауна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_sents = list(brown.sents())\n",
    "brown_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 505, True: 495})\n",
      "Counter({2: 596, 3: 241, 4: 163})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "def num_sents_to_glue():\n",
    "    x = random.uniform(0, 1)\n",
    "    # we need:\n",
    "    #  - 60% of two glued sentences,\n",
    "    #  - 25% of three glued sentences,\n",
    "    #  - 15% of four glued sentences.\n",
    "    if x < 0.15:\n",
    "        return 4\n",
    "    elif x >= 0.15 and x < 0.4:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def need_lowercase():\n",
    "    return random.uniform(0, 1) > 0.5\n",
    "\n",
    "print(collections.Counter(map(lambda x: need_lowercase(), range(1000))))\n",
    "print(collections.Counter(map(lambda x: num_sents_to_glue(), range(1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue_and_mark(sents):\n",
    "    labels = []\n",
    "    \n",
    "    # while brown corpus is already tokenized,\n",
    "    # we still need to reparse it with Spacy, since it uses\n",
    "    # different tokenization scheme (e.g. in nltk \n",
    "    # \"don't\" is one token, but in Spacy it's two tokens)\n",
    "    joined_sents = [' '.join(s) for s in sents]\n",
    "    sent_models = [nlp(s) for s in joined_sents]\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    \n",
    "    def do_glue(model, strip_punct, do_lowercase):\n",
    "        model = model[0:-1] if strip_punct and model[-1].is_punct else model\n",
    "        \n",
    "        for tok in model:\n",
    "            lowercasing = (tok.i == 0 and do_lowercase and need_lowercase())\n",
    "            text = tok.lower_ if lowercasing else tok.text\n",
    "            is_break = (strip_punct and tok.i == (len(model) - 1))\n",
    "            words.append(text)\n",
    "            labels.append(is_break)\n",
    "    \n",
    "    do_glue(sent_models[0], True, False)\n",
    "    for s in sent_models[1:-1]:\n",
    "        do_glue(s, True, True)\n",
    "    do_glue(sent_models[-1], False, True)\n",
    "    \n",
    "    # join sentences with dropped punctuation and do parse again.\n",
    "    # also check whether everything is OK, after parse\n",
    "    glued_model = nlp_no_tokenize(' '.join(words))\n",
    "    if (not len(glued_model) == len(words) or \n",
    "            any([not tok.text == word for tok, word in zip(glued_model, words)])):\n",
    "        println('WARN: bad glue')\n",
    "        return ([], [])\n",
    "    \n",
    "    return glued_model, labels\n",
    "    \n",
    "    \n",
    "def prepare_train_data(sents):\n",
    "    quote_re = re.compile('[\\'`]')\n",
    "    \n",
    "    # drop sentences that start or end with quotes\n",
    "    sents = [s for s in sents if (not re.search(quote_re, s[0]) and \n",
    "                                  not re.search(quote_re, s[-1]))]\n",
    "    i = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    while i < len(sents):\n",
    "        n = num_sents_to_glue()\n",
    "        to_glue = sents[i:i+n]\n",
    "        if len(to_glue) > 1:\n",
    "            tokens, labels = glue_and_mark(to_glue)\n",
    "            X.extend(tokens)\n",
    "            y.extend(labels)\n",
    "        i += n\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 1049362, True: 31510})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = prepare_train_data(brown_sents)\n",
    "collections.Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - False\n",
      "Fulton - False\n",
      "County - False\n",
      "Grand - False\n",
      "Jury - False\n",
      "said - False\n",
      "Friday - False\n",
      "an - False\n",
      "investigation - False\n",
      "of - False\n",
      "Atlanta - False\n",
      "'s - False\n",
      "recent - False\n",
      "primary - False\n",
      "election - False\n",
      "produced - False\n",
      "` - False\n",
      "` - False\n",
      "no - False\n",
      "evidence - False\n",
      "'' - False\n",
      "that - False\n",
      "any - False\n",
      "irregularities - False\n",
      "took - False\n",
      "place - True\n",
      "the - False\n",
      "jury - False\n",
      "further - False\n",
      "said - False\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(X_train[:30], y_train[:30]):\n",
    "    print(\"{} - {}\".format(x.text, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утиліти для створення класифікатору та крос-валідації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, precision_score\n",
    "\n",
    "cv_scoring = {'recall_True': make_scorer(recall_score, average = None, labels = [True]), \n",
    "              'recall_False': make_scorer(recall_score, average = None, labels = [False]),\n",
    "              'precision_True': make_scorer(precision_score, average = None, labels = [True]),\n",
    "              'precision_False': make_scorer(precision_score, average = None, labels = [False]),\n",
    "              'f1_True': make_scorer(f1_score, average = None, labels = [True]),\n",
    "              'f1_False': make_scorer(f1_score, average = None, labels = [False])\n",
    "             }\n",
    "\n",
    "def cross_validation_report(clf, X, y):\n",
    "    results = cross_validate(clf, X, y, scoring=cv_scoring)\n",
    "    \n",
    "    def calc(arr):\n",
    "        mean = arr.mean()\n",
    "        dev= arr.std() * 2\n",
    "        \n",
    "        return \"%0.2f (+/- %0.2f)\" % (mean, dev)\n",
    "    \n",
    "    print(\"True\")\n",
    "    print(\"\\tprecision:\\t{}\".format(calc(results['test_precision_True'])))\n",
    "    print(\"\\trecall:\\t\\t{}\".format(calc(results['test_recall_True'])))\n",
    "    print(\"\\tf1:\\t\\t{}\".format(calc(results['test_f1_True'])))\n",
    "    print('')\n",
    "    print(\"False\")\n",
    "    print(\"\\tprecision:\\t{}\".format(calc(results['test_precision_False'])))\n",
    "    print(\"\\trecall:\\t\\t{}\".format(calc(results['test_recall_False'])))\n",
    "    print(\"\\tf1:\\t\\t{}\".format(calc(results['test_f1_False'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def ds_func(f):\n",
    "    return lambda X: [f(x) for x in X]\n",
    "\n",
    "def combine_extractors(funcs):\n",
    "    def combined(x):\n",
    "        feats = {}\n",
    "        for e in funcs:\n",
    "            feats.update(e(x))\n",
    "        return feats\n",
    "    return combined\n",
    "\n",
    "def make_classifier(*feature_extractors):\n",
    "    classifier = Pipeline([('extractor', FunctionTransformer()),\n",
    "                           ('dict_vect', DictVectorizer()),\n",
    "                           ('lrc', LogisticRegression())])\n",
    "        \n",
    "    params = {'lrc__random_state': 42,\n",
    "              'lrc__solver': 'sag',\n",
    "              'lrc__multi_class': 'multinomial',\n",
    "              'lrc__max_iter': 1000,\n",
    "              'extractor__func': ds_func(combine_extractors(feature_extractors))}\n",
    "    classifier.set_params(**params)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перша версія класифікатору на лог. регресії. Використовую лише слово та його контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tok):\n",
    "    feats = {}\n",
    "\n",
    "    feats['word'] = tok.text\n",
    "    feats['word-2'] = tok.nbor(-2).text if tok.i >= 2 else '<S>'\n",
    "    feats['word-1'] = tok.nbor(-1).text if tok.i >= 1 else '<S>'\n",
    "    feats['word+1'] = tok.nbor(1).text if tok.i <= len(tok.doc) - 2 else '</S>'\n",
    "    feats['word+2'] = tok.nbor(2).text if tok.i <= len(tok.doc) - 3 else '</S>'\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.87 (+/- 0.03)\n",
      "\trecall:\t\t0.50 (+/- 0.05)\n",
      "\tf1:\t\t0.64 (+/- 0.04)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Додаємо до фіч частину мови слова, а також частини мов слів в околі слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(tok):\n",
    "    feats = {}\n",
    "    \n",
    "    feats['pos'] = tok.tag_\n",
    "    feats['pos-2'] = tok.nbor(-2).tag_ if tok.i >= 2 else '<S>'\n",
    "    feats['pos-1'] = tok.nbor(-1).tag_ if tok.i >= 1 else '<S>'\n",
    "    feats['pos+1'] = tok.nbor(1).tag_ if tok.i <= len(tok.doc) - 2 else '</S>'\n",
    "    feats['pos+2'] = tok.nbor(2).tag_ if tok.i <= len(tok.doc) - 3 else '</S>'\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.85 (+/- 0.03)\n",
      "\trecall:\t\t0.54 (+/- 0.05)\n",
      "\tf1:\t\t0.66 (+/- 0.04)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, pos_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемма слова та лемми слів в околі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_features(tok):\n",
    "    feats = {}\n",
    "\n",
    "    feats['lemma'] = tok.lemma_\n",
    "    feats['lemma-2'] = tok.nbor(-2).lemma_ if tok.i >= 2 else '<S>'\n",
    "    feats['lemma-1'] = tok.nbor(-1).lemma_ if tok.i >= 1 else '<S>'    \n",
    "    feats['lemma+1'] = tok.nbor(1).lemma_ if tok.i <= len(tok.doc) - 2 else '</S>'\n",
    "    feats['lemma+2'] = tok.nbor(2).lemma_ if tok.i <= len(tok.doc) - 3 else '</S>'\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.83 (+/- 0.03)\n",
      "\trecall:\t\t0.55 (+/- 0.05)\n",
      "\tf1:\t\t0.66 (+/- 0.04)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, pos_features, lemma_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Залежності слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_depth(tok, depth=0):\n",
    "    if tok.dep_ == 'ROOT':\n",
    "        return depth\n",
    "    else:\n",
    "        depth = depth+1\n",
    "        if depth >= 500:\n",
    "            return depth\n",
    "        return token_depth(tok.head, depth) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_depth(nlp('I like cats very much')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_features(tok):\n",
    "    feats = {}\n",
    "    \n",
    "    feats['dep'] = tok.dep_\n",
    "    feats['dep-depth'] = token_depth(tok)\n",
    "    feats['head-word'] = tok.head.text\n",
    "    feats['head-lemma'] = tok.head.lemma_\n",
    "    feats['head-pos'] = tok.head.pos_\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.84 (+/- 0.02)\n",
      "\trecall:\t\t0.56 (+/- 0.05)\n",
      "\tf1:\t\t0.67 (+/- 0.04)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, pos_features, dep_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Інформація про \"дітей\" в дереві залежностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_children_features(tok):\n",
    "    feats = {}\n",
    "    children = list(tok.children)\n",
    "    feats['children-num'] = len(children)\n",
    "    feats['child-1-word'] = children[0].text if len(children) > 0 else '<<<none>>>'\n",
    "    feats['child-2-word'] = children[1].text if len(children) > 1 else '<<<none>>>'\n",
    "    feats['child-3-word'] = children[2].text if len(children) > 2 else '<<<none>>>'\n",
    "    feats['child-1-dep'] = children[0].dep_ if len(children) > 0 else '<<<none>>>'\n",
    "    feats['child-2-dep'] = children[1].dep_ if len(children) > 1 else '<<<none>>>'\n",
    "    feats['child-3-dep'] = children[2].dep_ if len(children) > 2 else '<<<none>>>'\n",
    "    feats['child-1-lemma'] = children[0].lemma_ if len(children) > 0 else '<<<none>>>'\n",
    "    feats['child-2-lemma'] = children[1].lemma_ if len(children) > 1 else '<<<none>>>'\n",
    "    feats['child-3-lemma'] = children[2].lemma_ if len(children) > 2 else '<<<none>>>'\n",
    "    feats['child-1-pos'] = children[0].pos_ if len(children) > 0 else '<<<none>>>'\n",
    "    feats['child-2-pos'] = children[1].pos_ if len(children) > 1 else '<<<none>>>'\n",
    "    feats['child-3-pos'] = children[2].pos_ if len(children) > 2 else '<<<none>>>'\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.83 (+/- 0.02)\n",
      "\trecall:\t\t0.57 (+/- 0.05)\n",
      "\tf1:\t\t0.68 (+/- 0.03)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, pos_features, dep_features, dep_children_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Збираю біграми через phrasefinder.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4654418, 47034, 14779, 14337, 2166, 269, 123]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phrasefinder import phrasefinder as pf\n",
    "\n",
    "def fetch_bigram(text):\n",
    "#     print('fetching...')\n",
    "    try:\n",
    "        query = pf.escape_query_term(text)\n",
    "        result = pf.search(pf.Corpus.AMERICAN_ENGLISH, query)\n",
    "        if result.error:\n",
    "            print('WARN: request failed: {}'.format(result.error['message']))\n",
    "            return None\n",
    "\n",
    "        return [phrase.match_count for phrase in result.phrases]\n",
    "    except Exception as error:\n",
    "        print('Fatal error: {}'.format(error))\n",
    "        return None\n",
    "\n",
    "    \n",
    "fetch_bigram(\"i like\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i like': [4654418, 47034, 14779, 14337, 2166, 269, 123],\n",
       " 'i .': [786752, 478533, 148],\n",
       " 'i ?': [1766871, 206615],\n",
       " 'i !': [67902, 24809]}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_bigram(bigram, res_dict):\n",
    "    word1, word2 = bigram\n",
    "    \n",
    "    def fetch_and_save(text):\n",
    "        if not text in res_dict:\n",
    "            freq = fetch_bigram(text)\n",
    "            if freq is not None:\n",
    "                res_dict[text] = freq\n",
    "    \n",
    "    formatted = '{} {}'.format(word1.lower(), word2.lower())\n",
    "    formatted_w_period = '{} .'.format(word1.lower())\n",
    "    formatted_w_qmark = '{} ?'.format(word1.lower())\n",
    "    formatted_w_excl_mark = '{} !'.format(word1.lower())\n",
    "\n",
    "    fetch_and_save(formatted)\n",
    "    fetch_and_save(formatted_w_period)\n",
    "    fetch_and_save(formatted_w_qmark)\n",
    "    fetch_and_save(formatted_w_excl_mark)\n",
    "            \n",
    "    return res_dict\n",
    "        \n",
    "process_bigram(['i', 'like'], {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_bigrams(words, res_dict):\n",
    "    print('starting...')\n",
    "    bigrams = [words[i:i+2] for i in range(len(words) - 1)]\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        process_bigram(bigram, res_dict)\n",
    "    \n",
    "    print('done!')\n",
    "    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "done!\n",
      "starting...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "bigrams_dict = {}\n",
    "collect_bigrams([tok.text for tok in X_train], bigrams_dict)\n",
    "collect_bigrams([tok.text for tok in X_test], bigrams_dict)\n",
    "\n",
    "\n",
    "with open('bigrams.json', 'w') as f:\n",
    "    json.dump(bigrams_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104819"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_freq(word1, word2):\n",
    "    arr = bigrams_dict[\"{} {}\".format(word1.lower(), word2.lower())]\n",
    "    if arr:\n",
    "        return arr[0]\n",
    "    else:\n",
    "        return 0\n",
    "bigram_freq('cats', 'are')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестуємо додавання у фічі різницю частот біграм '<слово> <наступне слово>' та '<слово> <крапка>'. Результат сильно погіршився :( Наскільки розумію, це через те, що ці числа дуже сильно різняться і їх важко за-fit-ити лінійною моделлю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_features(tok):\n",
    "    feats = {}\n",
    "    next_tok = tok.nbor(1) if tok.i <= len(tok.doc) - 2 else None\n",
    "    freq = bigram_freq(tok.text, next_tok.text) if next_tok else 0\n",
    "    period_freq = bigram_freq(tok.text, '.') if next_tok else 0\n",
    "    feats['period-noperiod'] = period_freq - freq\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.04 (+/- 0.00)\n",
      "\trecall:\t\t0.87 (+/- 0.02)\n",
      "\tf1:\t\t0.08 (+/- 0.00)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t0.44 (+/- 0.03)\n",
      "\tf1:\t\t0.61 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, bigrams_features)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заміняємо попередню фічу на індикатор, чи частота біграми '<слово> <крапка>' більша, ніж частота біграми '<слово> <наступне слово>'. Результат особливо не змінився :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_features_2(tok):\n",
    "    feats = {}\n",
    "    next_tok = tok.nbor(1) if tok.i <= len(tok.doc) - 2 else None\n",
    "    freq = bigram_freq(tok.text, next_tok.text) if next_tok else 0\n",
    "    period_freq = bigram_freq(tok.text, '.') if next_tok else 0\n",
    "    feats['period>no-period'] = int(period_freq > freq)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.86 (+/- 0.03)\n",
      "\trecall:\t\t0.51 (+/- 0.05)\n",
      "\tf1:\t\t0.64 (+/- 0.03)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, bigrams_features_2)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросвалідуємо та тестуємо фінальний класифікатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\tprecision:\t0.83 (+/- 0.03)\n",
      "\trecall:\t\t0.58 (+/- 0.05)\n",
      "\tf1:\t\t0.68 (+/- 0.03)\n",
      "\n",
      "False\n",
      "\tprecision:\t0.99 (+/- 0.00)\n",
      "\trecall:\t\t1.00 (+/- 0.00)\n",
      "\tf1:\t\t0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "clf = make_classifier(word_features, pos_features, dep_features, dep_children_features, bigrams_features_2)\n",
    "cross_validation_report(clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99      4542\n",
      "        True       0.75      0.59      0.66       155\n",
      "\n",
      "    accuracy                           0.98      4697\n",
      "   macro avg       0.87      0.79      0.82      4697\n",
      "weighted avg       0.98      0.98      0.98      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## спостереження і висновки:\n",
    "* довго мучився з біграмами, але покращення якості не вдалось досягти :(\n",
    "  * можливо треба було збирати ще триграми;\n",
    "  * також хотілося б зібрати н-грами частин мови, але в через phrasefinder.io це не можна зробити (принаймні я не знайшов), а з сирими гуглівськими н-грамами не було сил розбиратись;\n",
    "  * також не знайшов як у phrasefinder шукати н-грами типу \"<S\\> word\", а це могло б бути корисно;\n",
    "  * але загалом phrasefinder - крутий проект, особливо враховуючи, що це one-man's project.\n",
    "* перша ітерація класифікатору показала непоганий результат, але суттєво покращити його виявилось важкою задачею;\n",
    "* можливо потрібно було підібрати інший корпус, наприклад корпус повідомлень в інтернеті чи щось подібне, який був би більш актуальний для задачі;\n",
    "* також датасет та кількість фіч виявились досить великими, тому потрібно було відносно довго чекати поки класифікатор натренується."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
